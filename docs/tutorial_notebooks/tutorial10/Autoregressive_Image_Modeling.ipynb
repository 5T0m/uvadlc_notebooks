{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Image Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: implement PixelCNN(++), and understand the autoregressive convolutions and discrete logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_NOTEBOOK = False\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "if USE_NOTEBOOK:\n",
    "    %matplotlib inline \n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import seaborn as sns\n",
    "\n",
    "## Progress bar\n",
    "if USE_NOTEBOOK:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, mask, stride=1):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask[None,None])\n",
    "        kernel_size = (mask.shape[0], mask.shape[1])\n",
    "        padding = tuple([(kernel_size[i]-1)//2 for i in range(2)])\n",
    "        self.conv = nn.Conv2d(c_in, c_out, kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, kernel_size=3, stride=1, mask_center=False):\n",
    "        mask = torch.ones(kernel_size, kernel_size)\n",
    "        mask[kernel_size//2+1:,:] = 0\n",
    "        if mask_center:\n",
    "            mask[kernel_size//2,:] = 0\n",
    "        super().__init__(c_in, c_out, mask, stride)\n",
    "        \n",
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, kernel_size=3, stride=1, mask_center=False):\n",
    "        mask = torch.ones(1,kernel_size)\n",
    "        mask[0,kernel_size//2+1:] = 0\n",
    "        if mask_center:\n",
    "            mask[0,kernel_size//2] = 0\n",
    "        super().__init__(c_in, c_out, mask, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMaskedConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.conv_vert = VerticalStackConvolution(c_in, c_out=2*c_in)\n",
    "        self.conv_horiz = HorizontalStackConvolution(c_in, c_out=2*c_in)\n",
    "        self.conv_vert_to_horiz = nn.Conv2d(2*c_in, 2*c_in, kernel_size=1, padding=0)\n",
    "        self.conv_horiz_1x1 = nn.Conv2d(c_in, c_in, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, v_stack, h_stack):\n",
    "        v_stack_feat = self.conv_vert(v_stack)\n",
    "        v_val, v_gate = v_stack_feat.chunk(2, dim=1)\n",
    "        v_stack_out = torch.tanh(v_val) * torch.sigmoid(v_gate)\n",
    "        \n",
    "        h_stack_feat = self.conv_horiz(h_stack)\n",
    "        h_stack_feat = h_stack_feat + self.conv_vert_to_horiz(v_stack_feat)\n",
    "        h_val, h_gate = h_stack_feat.chunk(2, dim=1)\n",
    "        h_stack_feat = torch.tanh(h_val) * torch.sigmoid(h_gate)\n",
    "        h_stack_out = self.conv_horiz_1x1(h_stack_feat)\n",
    "        h_stack_out = h_stack_out + h_stack\n",
    "        \n",
    "        return v_stack_out, h_stack_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros(11, 11)\n",
    "img[img.shape[0]//2, img.shape[1]//2] = 1\n",
    "\n",
    "if USE_NOTEBOOK:\n",
    "    def show_recep_field(img):\n",
    "        img = img.squeeze().cpu().numpy()\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        pos = ax[0].imshow(img)\n",
    "        ax[1].imshow(img>0)\n",
    "        show_center = (img[img.shape[0]//2,img.shape[1]//2] == 0)\n",
    "        if show_center:\n",
    "            center_pixel = np.zeros(img.shape + (4,))\n",
    "            center_pixel[center_pixel.shape[0]//2,center_pixel.shape[1]//2,:] = np.array([1.0, 0.0, 0.0, 1.0]) \n",
    "        for i in range(2):\n",
    "            ax[i].axis('off')\n",
    "            if show_center:\n",
    "                ax[i].imshow(center_pixel)\n",
    "        ax[0].set_title(\"Feature map\")\n",
    "        ax[1].set_title(\"Binary feature map\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "else:\n",
    "    def show_recep_field(img):\n",
    "        pass\n",
    "\n",
    "show_recep_field(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "horiz_conv = HorizontalStackConvolution(1, 1, mask_center=True)\n",
    "horiz_conv.conv.weight.data.fill_(1)\n",
    "horiz_conv.conv.bias.data.fill_(0)\n",
    "with torch.no_grad():\n",
    "    horiz_img = horiz_conv(img[None,None])\n",
    "show_recep_field(horiz_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_conv = VerticalStackConvolution(1, 1, mask_center=True)\n",
    "vert_conv.conv.weight.data.fill_(1)\n",
    "vert_conv.conv.bias.data.fill_(0)\n",
    "with torch.no_grad():\n",
    "    vert_img = vert_conv(img[None,None])\n",
    "show_recep_field(vert_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "horiz_img = vert_img + horiz_img\n",
    "show_recep_field(horiz_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    horiz_conv = HorizontalStackConvolution(1, 1, mask_center=False)\n",
    "    horiz_conv.conv.weight.data.fill_(1)\n",
    "    horiz_conv.conv.bias.data.fill_(0)\n",
    "    vert_conv = VerticalStackConvolution(1, 1, mask_center=False)\n",
    "    vert_conv.conv.weight.data.fill_(1)\n",
    "    vert_conv.conv.bias.data.fill_(0)\n",
    "    with torch.no_grad():\n",
    "        vert_img = vert_conv(vert_img)\n",
    "        horiz_img = horiz_conv(horiz_img) + vert_img\n",
    "    show_recep_field(horiz_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recep_field(vert_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.conv_vstack = VerticalStackConvolution(c_in, c_hidden, mask_center=True)\n",
    "        self.conv_hstack = HorizontalStackConvolution(c_in, c_hidden, mask_center=True)\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList([GatedMaskedConv(c_hidden) for _ in range(num_layers)])\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(c_hidden, c_in * 256, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (x.float() / 255.0) * 2 - 1\n",
    "        v_stack = self.conv_vstack(x)\n",
    "        h_stack = self.conv_hstack(x)\n",
    "        \n",
    "        for layer in self.conv_layers:\n",
    "            v_stack, h_stack = layer(v_stack, h_stack)\n",
    "        \n",
    "        out = self.conv_out(F.elu(h_stack))\n",
    "        out = out.reshape(out.shape[0], 256, out.shape[1]//256, out.shape[2], out.shape[3])\n",
    "        return out\n",
    "    \n",
    "    def calc_likelihood(self, x):\n",
    "        pred = self.forward(x)\n",
    "        nll = F.cross_entropy(pred, x, reduction='none')\n",
    "        bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
    "        return bpd\n",
    "        \n",
    "    def sample(self, img_shape):\n",
    "        img = torch.zeros(img_shape, dtype=torch.long).to(device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for h in tqdm(range(img_shape[2])):\n",
    "                for w in range(img_shape[3]):\n",
    "                    for c in range(img_shape[1]):\n",
    "                        pred = self.forward(img)\n",
    "                        probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
    "                        img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.long)\n",
    "\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NOTEBOOK:\n",
    "\n",
    "    def show_imgs(imgs):\n",
    "        imgs = torchvision.utils.make_grid(imgs, nrow=4, pad_value=128)\n",
    "        np_imgs = imgs.numpy()\n",
    "        sns.set_style(\"white\")\n",
    "        plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    show_imgs([train_set[i][0] for i in range(8)])\n",
    "\n",
    "else:\n",
    "    def show_imgs(imgs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, max_epochs=80):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98) # Every epoch, we multiply the LR by 0.95\n",
    "    train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Start training %s...\" % model_name)\n",
    "    \n",
    "    avg_bpd = 8.0\n",
    "    \n",
    "    val_scores = []\n",
    "    for epoch in range(max_epochs):\n",
    "        pbar = tqdm(train_loader, leave=False) if USE_NOTEBOOK else train_loader\n",
    "        for imgs, _ in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            bpd = model.calc_likelihood(imgs).mean()\n",
    "            bpd.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_bpd = 0.9 * avg_bpd + 0.1 * bpd.item()\n",
    "            if USE_NOTEBOOK:\n",
    "                pbar.set_description(\"[Epoch %i] Bits per dimension: %5.3fbpd\" % (epoch+1, avg_bpd))\n",
    "        lr_scheduler.step()\n",
    "        if not USE_NOTEBOOK:\n",
    "            print(\"Training bpd: %5.3fbpd\" % avg_bpd)\n",
    "        val_bpd = test_model(model, val_loader)\n",
    "        print(\"[Epoch %2i] Validation bits per dimension: %5.3fbpd\" % (epoch+1, val_bpd))\n",
    "        \n",
    "        if len(val_scores) == 0 or val_bpd < min(val_scores):\n",
    "            print(\"\\t   (New best performance, saving model...)\")\n",
    "            save_model(model, CHECKPOINT_PATH, model_name)\n",
    "        val_scores.append(val_bpd)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            samples = model.sample(img_shape=(8,1,28,28))\n",
    "            show_imgs(samples.cpu())\n",
    "            \n",
    "    if USE_NOTEBOOK:\n",
    "        # Plot a curve of the validation accuracy\n",
    "        sns.set()\n",
    "        plt.plot([i for i in range(1,len(val_scores)+1)], val_scores)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Validation bits per dimension\")\n",
    "        plt.title(\"Validation performance of %s\" % model_name)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Val scores\", val_scores)\n",
    "    \n",
    "    test_bpd = test_model(model, test_loader)\n",
    "    print(\"Test bits per dimension: %5.3fbpd\" % (test_bpd))\n",
    "    \n",
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    test_bpd, counter = 0.0, 0\n",
    "    for imgs, _ in (tqdm(data_loader, leave=False, desc=\"Testing...\") if USE_NOTEBOOK else data_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_bpd = model.calc_likelihood(imgs)\n",
    "            test_bpd += img_bpd.sum().item()\n",
    "            counter += img_bpd.shape[0]\n",
    "    test_bpd = test_bpd / counter\n",
    "    return test_bpd \n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, model_name + \".tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training PixelCNN...\n",
      "Training bpd: 1.169bpd\n",
      "[Epoch  1] Validation bits per dimension: 1.169bpd\n",
      "\t   (New best performance, saving model...)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-de1debb520a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPixelCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PixelCNN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-41d698c479a7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, max_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mbpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mbpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PixelCNN(c_in=1, c_hidden=64, num_layers=4).to(device)\n",
    "train_model(model, model_name=\"PixelCNN\", max_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = model.sample(img_shape=(8,1,28,28))\n",
    "# show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = model.sample(img_shape=(8,1,64,64))\n",
    "# show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of predictive distribution (softmax)\n",
    "### Autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
