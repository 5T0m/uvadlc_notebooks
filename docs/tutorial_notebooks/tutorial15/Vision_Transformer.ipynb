{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "\"\"\"\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.3.4\n",
    "    import pytorch_lightning as pl\n",
    "\"\"\"\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial15\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "pl.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"vit.gif\" width=\"600px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, num_layers, num_classes, patch_size, num_channels, num_patches, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.input_layer = nn.Linear(num_channels*patch_size**2, embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def img_to_patch(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H//self.patch_size, self.patch_size, W//self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "        x = x.reshape(B, -1, C*self.patch_size**2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.img_to_patch(x)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]   \n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log('%s_loss' % mode, loss)\n",
    "        self.log('%s_acc' % mode, acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), \n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=180,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Global seed set to 42\n",
      "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/ipykernel_launcher.py:32: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "\n",
      "  | Name  | Type              | Params | In sizes         | Out sizes\n",
      "---------------------------------------------------------------------------\n",
      "0 | model | VisionTransformer | 3.2 M  | [128, 3, 32, 32] | [128, 10]\n",
      "---------------------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0604 17:27:50.211694 140002099119936 lightning.py:1346] \n",
      "  | Name  | Type              | Params | In sizes         | Out sizes\n",
      "---------------------------------------------------------------------------\n",
      "0 | model | VisionTransformer | 3.2 M  | [128, 3, 32, 32] | [128, 10]\n",
      "---------------------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca053bf2b93447d38c29ec84e4e2bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd463d26bc342b58a405bb6279d2b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c33262777394ac78fcc426160a2dafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ViT(\n",
       "   (model): VisionTransformer(\n",
       "     (input_layer): Linear(in_features=48, out_features=256, bias=True)\n",
       "     (transformer): Sequential(\n",
       "       (0): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): AttentionBlock(\n",
       "         (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): MultiheadAttention(\n",
       "           (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "         )\n",
       "         (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (linear): Sequential(\n",
       "           (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (1): GELU()\n",
       "           (2): Dropout(p=0.1, inplace=False)\n",
       "           (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (4): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (mlp_head): Sequential(\n",
       "       (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "       (1): Linear(in_features=256, out_features=10, bias=True)\n",
       "     )\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " {'test': 0.23739999532699585, 'val': 0.24140000343322754})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_kwargs={\n",
    "                'embed_dim': 256,\n",
    "                'hidden_dim': 512,\n",
    "                'num_heads': 8,\n",
    "                'num_layers': 6,\n",
    "                'patch_size': 4,\n",
    "                'num_channels': 3,\n",
    "                'num_patches': 64,\n",
    "                'num_classes': 10,\n",
    "                'dropout': 0.2\n",
    "            },\n",
    "            lr=3e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
