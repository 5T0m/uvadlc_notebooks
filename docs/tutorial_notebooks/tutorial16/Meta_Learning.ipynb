{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 16: Meta-Learning - Learning to Learn\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=In%20progress&color=red)\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/Meta_Learning.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/Meta_Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial16)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1GKCVDr5Mz3gLzyEaDLEikfOtaNZhZ6xh?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\"\"\"\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100, Omniglot\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.3.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial16\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_train_set = CIFAR100(root=DATASET_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR_test_set = CIFAR100(root=DATASET_PATH, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(8,8))\\nplt.title(\"Image examples of the CIFAR100 dataset\")\\nplt.imshow(img_grid)\\nplt.axis(\\'off\\')\\nplt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize some examples\n",
    "NUM_IMAGES = 12\n",
    "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
    "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, imgs, targets, img_transform=None):\n",
    "        super().__init__()\n",
    "        self.img_transform = img_transform\n",
    "        self.imgs = imgs\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.imgs[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "classes = torch.randperm(100)\n",
    "train_classes, val_classes, test_classes = classes[:80], classes[80:90], classes[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation classes: ['caterpillar', 'castle', 'skunk', 'ray', 'bus', 'motorcycle', 'keyboard', 'chimpanzee', 'possum', 'tiger']\n",
      "Test classes: ['kangaroo', 'crocodile', 'butterfly', 'shark', 'forest', 'pickup_truck', 'telephone', 'lion', 'worm', 'mushroom']\n"
     ]
    }
   ],
   "source": [
    "# Printing validation and test classes\n",
    "idx_to_class = {val: key for key, val in CIFAR_train_set.class_to_idx.items()}\n",
    "print(\"Validation classes:\", [idx_to_class[c.item()] for c in val_classes])\n",
    "print(\"Test classes:\", [idx_to_class[c.item()] for c in test_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_labels(imgs, targets, class_set, **kwargs):\n",
    "    class_mask = (targets[:,None] == class_set[None,:]).any(dim=-1)\n",
    "    return ImageDataset(imgs=imgs[class_mask],\n",
    "                        targets=targets[class_mask],\n",
    "                        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS = (CIFAR_train_set.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (CIFAR_train_set.data / 255.0).std(axis=(0,1,2))\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "\n",
    "train_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
    "val_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
    "test_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotBatchSampler(object):\n",
    "    \n",
    "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.dataset_targets = dataset_targets\n",
    "        self.N_way = N_way\n",
    "        self.K_shot = K_shot\n",
    "        self.shuffle = shuffle\n",
    "        self.include_query = include_query\n",
    "        if self.include_query:\n",
    "            self.K_shot *= 2\n",
    "            \n",
    "        self.batch_size = self.N_way * self.K_shot\n",
    "        \n",
    "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
    "        self.indices_per_class = {}\n",
    "        self.batches_per_class = {}\n",
    "        for c in self.classes:\n",
    "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
    "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
    "        \n",
    "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
    "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
    "        self.shuffle_data()\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        for c in self.classes:\n",
    "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
    "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
    "        random.shuffle(self.class_list)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        \n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        for it in range(self.iterations):\n",
    "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]\n",
    "            index_batch = []\n",
    "            for c in class_batch:\n",
    "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
    "                start_index[c] += self.K_shot\n",
    "            if self.include_query:\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield index_batch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5\n",
    "K_SHOT = 4\n",
    "train_data_loader = data.DataLoader(train_set, \n",
    "                                    batch_sampler=FewShotBatchSampler(train_set.targets, \n",
    "                                                                      include_query=True,\n",
    "                                                                      N_way=N_WAY,\n",
    "                                                                      K_shot=K_SHOT),\n",
    "                                    num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_set, \n",
    "                                  batch_sampler=FewShotBatchSampler(val_set.targets, \n",
    "                                                                    include_query=True,\n",
    "                                                                    N_way=N_WAY,\n",
    "                                                                    K_shot=K_SHOT,\n",
    "                                                                    shuffle=False),\n",
    "                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig, ax = plt.subplots(1, 2, figsize=(8,5))\\nax[0].imshow(support_grid)\\nax[0].set_title(\"Support set\")\\nax[0].axis(\\'off\\')\\nax[1].imshow(query_grid)\\nax[1].set_title(\"Query set\")\\nax[1].axis(\\'off\\')\\nplt.suptitle(\"Few Shot Batch\", weight=\\'bold\\')\\nplt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_data_loader))\n",
    "support_set, query_set = imgs.chunk(2, dim=0)\n",
    "support_grid = torchvision.utils.make_grid(support_set, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "support_grid = support_grid.permute(1, 2, 0)\n",
    "query_grid = torchvision.utils.make_grid(query_set, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "query_grid = query_grid.permute(1, 2, 0)\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,5))\n",
    "ax[0].imshow(support_grid)\n",
    "ax[0].set_title(\"Support set\")\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(query_grid)\n",
    "ax[1].set_title(\"Query set\")\n",
    "ax[1].axis('off')\n",
    "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(imgs, targets):\n",
    "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
    "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
    "    return support_imgs, query_imgs, support_targets, query_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convnet(output_size):\n",
    "    convnet = torchvision.models.DenseNet(growth_rate=32, \n",
    "                                          block_config=(6, 6, 6, 6), \n",
    "                                          bn_size=2,\n",
    "                                          num_init_features=64, \n",
    "                                          num_classes=output_size)\n",
    "    \"\"\"\n",
    "    convnet = torchvision.models.resnet18(num_classes=output_size)\n",
    "    \"\"\"\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962912"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_convnet(64)\n",
    "sum([np.prod(p.shape) for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, proto_dim, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
    "        \n",
    "    def calculate_prototypes(self, features, targets):\n",
    "        classes, _ = torch.unique(targets).sort()\n",
    "        prototypes = []\n",
    "        for c in classes:\n",
    "            p = features[torch.where(targets == c)[0]].mean(dim=0)\n",
    "            prototypes.append(p)\n",
    "        prototypes = torch.stack(prototypes, dim=0)\n",
    "        \n",
    "        return prototypes, classes\n",
    "    \n",
    "    def classify_feats(self, prototypes, classes, feats, targets):\n",
    "        dist = torch.pow(prototypes[None,:] - feats[:,None], 2).sum(dim=2)\n",
    "        preds = F.log_softmax(-dist, dim=1)\n",
    "        labels = (classes[None,:] == targets[:,None]).long().argmax(dim=-1)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        return preds, labels, acc\n",
    "        \n",
    "    def calculate_loss(self, batch, mode):\n",
    "        imgs, targets = batch\n",
    "        features = self.model(imgs)\n",
    "        support_feats, query_feats, support_targets, query_targets = split_batch(features, targets)\n",
    "        prototypes, classes = self.calculate_prototypes(support_feats, support_targets)\n",
    "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        \n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.calculate_loss(batch, mode=\"train\")\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self.calculate_loss(batch, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=200,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = model_class.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = model_class(**kwargs)\n",
    "        trainer.fit(model, train_data_loader, val_data_loader)\n",
    "        model = model_class.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at ../saved_models/tutorial16/ProtoNet.ckpt, loading...\n"
     ]
    }
   ],
   "source": [
    "model = train_model(ProtoNet, proto_dim=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_proto_net(model, dataset, classes, data_feats=None, k_shot=4, batch_size=128):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_classes = len(classes)\n",
    "    exmps_per_class = dataset.targets.shape[0]//len(classes)\n",
    "    \n",
    "    if data_feats is None:\n",
    "        # Dataset preparation\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False, drop_last=False)\n",
    "\n",
    "        img_features = []\n",
    "        img_targets = []\n",
    "        for imgs, targets in tqdm(dataloader, \"Extracting image features\"): \n",
    "            imgs = imgs.to(device)\n",
    "            feats = model.model(imgs)\n",
    "            img_features.append(feats.detach().cpu())\n",
    "            img_targets.append(targets)\n",
    "        img_features = torch.cat(img_features, dim=0)\n",
    "        img_targets = torch.cat(img_targets, dim=0)\n",
    "        img_targets, sort_idx = img_targets.sort()\n",
    "        img_targets = img_targets.reshape(num_classes, exmps_per_class).transpose(0, 1)\n",
    "        img_features = img_features[sort_idx].reshape(num_classes, exmps_per_class, -1).transpose(0, 1)\n",
    "    else:\n",
    "        img_features, img_targets = data_feats\n",
    "    \n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch. Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for k_idx in tqdm(range(0, img_features.shape[0], k_shot), \"Evaluating prototype classification\"):\n",
    "        k_img_feats, k_targets = img_features[k_idx:k_idx+k_shot].flatten(0,1), img_targets[k_idx:k_idx+k_shot].flatten(0,1)\n",
    "        prototypes, proto_classes = model.calculate_prototypes(k_img_feats, k_targets)\n",
    "        batch_acc = 0\n",
    "        for e_idx in range(0, img_features.shape[0], k_shot):\n",
    "            if k_idx == e_idx:\n",
    "                continue\n",
    "            e_img_feats, e_targets = img_features[e_idx:e_idx+k_shot].flatten(0,1), img_targets[e_idx:e_idx+k_shot].flatten(0,1)\n",
    "            _, _, acc = model.classify_feats(prototypes, proto_classes, e_img_feats, e_targets)\n",
    "            batch_acc += acc.item()\n",
    "        batch_acc /= img_features.shape[0]//k_shot-1\n",
    "        accuracies.append(batch_acc)\n",
    "    \n",
    "    return accuracies, mean(accuracies), (img_features, img_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96985b24064e4f0695b5d60069db0889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Extracting image features', max=47.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb311010234227bb9c672e740836ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating prototype classification', max=300.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for k=2: 46.10%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ef6da13c7d45e587563e13e84d6862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating prototype classification', max=150.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for k=4: 53.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc2c2fa69df4f92b1d0ac82c49713c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating prototype classification', max=75.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for k=8: 58.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1655880b528d4b3f9bb091106a353c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating prototype classification', max=38.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for k=16: 63.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806f9fa9ca4a4e74bdfb69ddef50327e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating prototype classification', max=19.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for k=32: 67.65%\n"
     ]
    }
   ],
   "source": [
    "data_feats = None\n",
    "for k in [2, 4, 8, 16, 32]:\n",
    "    _, mean_acc, data_feats = test_proto_net(model, test_set, test_classes, data_feats=data_feats, k_shot=k)\n",
    "    print(\"Accuracy for k=%i: %4.2f%%\" % (k, 100.0*mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML and ProtoMAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
