{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 16: Meta-Learning - Learning to Learn\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=In%20progress&color=red)\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/Meta_Learning.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/Meta_Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial16)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1GKCVDr5Mz3gLzyEaDLEikfOtaNZhZ6xh?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\"\"\"\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100, Omniglot\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.3.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial16\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_train_set = CIFAR100(root=DATASET_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR_test_set = CIFAR100(root=DATASET_PATH, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(8,8))\\nplt.title(\"Image examples of the CIFAR100 dataset\")\\nplt.imshow(img_grid)\\nplt.axis(\\'off\\')\\nplt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize some examples\n",
    "NUM_IMAGES = 12\n",
    "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
    "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, imgs, targets, img_transform=None):\n",
    "        super().__init__()\n",
    "        self.img_transform = img_transform\n",
    "        self.imgs = imgs\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.imgs[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "classes = torch.randperm(100)\n",
    "train_classes, val_classes, test_classes = classes[:80], classes[80:90], classes[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation classes: ['caterpillar', 'castle', 'skunk', 'ray', 'bus', 'motorcycle', 'keyboard', 'chimpanzee', 'possum', 'tiger']\n",
      "Test classes: ['kangaroo', 'crocodile', 'butterfly', 'shark', 'forest', 'pickup_truck', 'telephone', 'lion', 'worm', 'mushroom']\n"
     ]
    }
   ],
   "source": [
    "# Printing validation and test classes\n",
    "idx_to_class = {val: key for key, val in CIFAR_train_set.class_to_idx.items()}\n",
    "print(\"Validation classes:\", [idx_to_class[c.item()] for c in val_classes])\n",
    "print(\"Test classes:\", [idx_to_class[c.item()] for c in test_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_labels(imgs, targets, class_set, **kwargs):\n",
    "    class_mask = (targets[:,None] == class_set[None,:]).any(dim=-1)\n",
    "    return ImageDataset(imgs=imgs[class_mask],\n",
    "                        targets=targets[class_mask],\n",
    "                        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS = (CIFAR_train_set.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (CIFAR_train_set.data / 255.0).std(axis=(0,1,2))\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "\n",
    "train_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
    "val_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
    "test_set = dataset_from_labels(CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotBatchSampler(object):\n",
    "    \n",
    "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, order=False):\n",
    "        super().__init__()\n",
    "        self.dataset_targets = dataset_targets\n",
    "        self.N_way = N_way\n",
    "        self.K_shot = K_shot\n",
    "        self.shuffle = shuffle\n",
    "        self.include_query = include_query\n",
    "        if self.include_query:\n",
    "            self.K_shot *= 2\n",
    "            \n",
    "        self.batch_size = self.N_way * self.K_shot\n",
    "        \n",
    "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.indices_per_class = {}\n",
    "        self.batches_per_class = {}\n",
    "        for c in self.classes:\n",
    "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
    "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
    "        \n",
    "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
    "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
    "        if not order:\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            sort_idxs = [i+p*self.num_classes for i,c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
    "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        for c in self.classes:\n",
    "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
    "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
    "        random.shuffle(self.class_list)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        \n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        for it in range(self.iterations):\n",
    "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]\n",
    "            index_batch = []\n",
    "            for c in class_batch:\n",
    "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
    "                start_index[c] += self.K_shot\n",
    "            if self.include_query:\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield index_batch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5\n",
    "K_SHOT = 4\n",
    "train_data_loader = data.DataLoader(train_set, \n",
    "                                    batch_sampler=FewShotBatchSampler(train_set.targets, \n",
    "                                                                      include_query=True,\n",
    "                                                                      N_way=N_WAY,\n",
    "                                                                      K_shot=K_SHOT),\n",
    "                                    num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_set, \n",
    "                                  batch_sampler=FewShotBatchSampler(val_set.targets, \n",
    "                                                                    include_query=True,\n",
    "                                                                    N_way=N_WAY,\n",
    "                                                                    K_shot=K_SHOT,\n",
    "                                                                    shuffle=False),\n",
    "                                  num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig, ax = plt.subplots(1, 2, figsize=(8,5))\\nax[0].imshow(support_grid)\\nax[0].set_title(\"Support set\")\\nax[0].axis(\\'off\\')\\nax[1].imshow(query_grid)\\nax[1].set_title(\"Query set\")\\nax[1].axis(\\'off\\')\\nplt.suptitle(\"Few Shot Batch\", weight=\\'bold\\')\\nplt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_data_loader))\n",
    "support_set, query_set = imgs.chunk(2, dim=0)\n",
    "support_grid = torchvision.utils.make_grid(support_set, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "support_grid = support_grid.permute(1, 2, 0)\n",
    "query_grid = torchvision.utils.make_grid(query_set, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "query_grid = query_grid.permute(1, 2, 0)\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,5))\n",
    "ax[0].imshow(support_grid)\n",
    "ax[0].set_title(\"Support set\")\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(query_grid)\n",
    "ax[1].set_title(\"Query set\")\n",
    "ax[1].axis('off')\n",
    "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(imgs, targets):\n",
    "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
    "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
    "    return support_imgs, query_imgs, support_targets, query_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convnet(output_size):\n",
    "    convnet = torchvision.models.DenseNet(growth_rate=32, \n",
    "                                          block_config=(6, 6, 6, 6), \n",
    "                                          bn_size=2,\n",
    "                                          num_init_features=64, \n",
    "                                          num_classes=output_size)\n",
    "    \"\"\"\n",
    "    convnet = torchvision.models.resnet18(num_classes=output_size)\n",
    "    \"\"\"\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962912"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_convnet(64)\n",
    "sum([np.prod(p.shape) for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, proto_dim, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_prototypes(features, targets):\n",
    "        classes, _ = torch.unique(targets).sort()\n",
    "        prototypes = []\n",
    "        for c in classes:\n",
    "            p = features[torch.where(targets == c)[0]].mean(dim=0)\n",
    "            prototypes.append(p)\n",
    "        prototypes = torch.stack(prototypes, dim=0)\n",
    "        \n",
    "        return prototypes, classes\n",
    "    \n",
    "    def classify_feats(self, prototypes, classes, feats, targets):\n",
    "        dist = torch.pow(prototypes[None,:] - feats[:,None], 2).sum(dim=2)\n",
    "        preds = F.log_softmax(-dist, dim=1)\n",
    "        labels = (classes[None,:] == targets[:,None]).long().argmax(dim=-1)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        return preds, labels, acc\n",
    "        \n",
    "    def calculate_loss(self, batch, mode):\n",
    "        imgs, targets = batch\n",
    "        features = self.model(imgs)\n",
    "        support_feats, query_feats, support_targets, query_targets = split_batch(features, targets)\n",
    "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
    "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        \n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.calculate_loss(batch, mode=\"train\")\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self.calculate_loss(batch, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_class, train_loader, val_loader, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=200,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = model_class.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = model_class(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = model_class.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# protonet_model = train_model(ProtoNet, proto_dim=64, lr=1e-3, train_loader=train_data_loader, val_loader=val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_proto_net(model, dataset, classes, data_feats=None, k_shot=4, batch_size=128):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_classes = len(classes)\n",
    "    exmps_per_class = dataset.targets.shape[0]//len(classes)\n",
    "    \n",
    "    if data_feats is None:\n",
    "        # Dataset preparation\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False, drop_last=False)\n",
    "\n",
    "        img_features = []\n",
    "        img_targets = []\n",
    "        for imgs, targets in tqdm(dataloader, \"Extracting image features\"): \n",
    "            imgs = imgs.to(device)\n",
    "            feats = model.model(imgs)\n",
    "            img_features.append(feats.detach().cpu())\n",
    "            img_targets.append(targets)\n",
    "        img_features = torch.cat(img_features, dim=0)\n",
    "        img_targets = torch.cat(img_targets, dim=0)\n",
    "        img_targets, sort_idx = img_targets.sort()\n",
    "        img_targets = img_targets.reshape(num_classes, exmps_per_class).transpose(0, 1)\n",
    "        img_features = img_features[sort_idx].reshape(num_classes, exmps_per_class, -1).transpose(0, 1)\n",
    "    else:\n",
    "        img_features, img_targets = data_feats\n",
    "    \n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch. Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for k_idx in tqdm(range(0, img_features.shape[0], k_shot), \"Evaluating prototype classification\"):\n",
    "        k_img_feats, k_targets = img_features[k_idx:k_idx+k_shot].flatten(0,1), img_targets[k_idx:k_idx+k_shot].flatten(0,1)\n",
    "        prototypes, proto_classes = model.calculate_prototypes(k_img_feats, k_targets)\n",
    "        batch_acc = 0\n",
    "        for e_idx in range(0, img_features.shape[0], k_shot):\n",
    "            if k_idx == e_idx:\n",
    "                continue\n",
    "            e_img_feats, e_targets = img_features[e_idx:e_idx+k_shot].flatten(0,1), img_targets[e_idx:e_idx+k_shot].flatten(0,1)\n",
    "            _, _, acc = model.classify_feats(prototypes, proto_classes, e_img_feats, e_targets)\n",
    "            batch_acc += acc.item()\n",
    "        batch_acc /= img_features.shape[0]//k_shot-1\n",
    "        accuracies.append(batch_acc)\n",
    "    \n",
    "    return accuracies, mean(accuracies), (img_features, img_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_feats = None\\nfor k in [2, 4, 8, 16, 32]:\\n    _, mean_acc, data_feats = test_proto_net(protonet_model, test_set, test_classes, data_feats=data_feats, k_shot=k)\\n    print(\"Accuracy for k=%i: %4.2f%%\" % (k, 100.0*mean_acc))\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_feats = None\n",
    "for k in [2, 4, 8, 16, 32]:\n",
    "    _, mean_acc, data_feats = test_proto_net(protonet_model, test_set, test_classes, data_feats=data_feats, k_shot=k)\n",
    "    print(\"Accuracy for k=%i: %4.2f%%\" % (k, 100.0*mean_acc))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML and ProtoMAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoMAML(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, proto_dim, lr, lr_inner, lr_output, num_inner_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
    "        \n",
    "    def run_model(self, local_model, output_weight, output_bias, imgs, labels):\n",
    "        feats = local_model(imgs)\n",
    "        preds = F.linear(feats, output_weight, output_bias)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float()\n",
    "        return loss, preds, acc\n",
    "        \n",
    "    def adapt_few_shot(self, support_imgs, support_targets):\n",
    "        support_feats = self.model(support_imgs)\n",
    "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
    "        support_labels = (classes[None,:] == support_targets[:,None]).long().argmax(dim=-1)\n",
    "        \n",
    "        local_model = deepcopy(self.model)\n",
    "        local_model.train()\n",
    "        local_optim = optim.SGD(local_model.parameters(), lr=self.hparams.lr_inner)\n",
    "        local_optim.zero_grad()\n",
    "        \n",
    "        init_weight = 2 * prototypes\n",
    "        init_bias = -torch.norm(prototypes, dim=1)**2\n",
    "        output_weight = init_weight.detach().requires_grad_()\n",
    "        output_bias = init_bias.detach().requires_grad_()\n",
    "        \n",
    "        for _ in range(self.hparams.num_inner_steps):\n",
    "            loss, _, _ = self.run_model(local_model, output_weight, output_bias, support_imgs, support_labels)\n",
    "            # print(\"Inner loop loss\", loss.item())\n",
    "            loss.backward()\n",
    "            local_optim.step()\n",
    "            \n",
    "            output_weight.data -= self.hparams.lr_output * output_weight.grad\n",
    "            output_bias.data -= self.hparams.lr_output * output_bias.grad\n",
    "            \n",
    "            local_optim.zero_grad()\n",
    "            output_weight.grad.fill_(0)\n",
    "            output_bias.grad.fill_(0)\n",
    "            \n",
    "        output_weight = (output_weight - init_weight).detach() + init_weight\n",
    "        output_bias = (output_bias - init_bias).detach() + init_bias\n",
    "        \n",
    "        return local_model, output_weight, output_bias, classes\n",
    "        \n",
    "    def outer_loop(self, batch, mode=\"train\"):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # print(list(self.model.parameters())[0].flatten(0,-1)[0])\n",
    "        \n",
    "        for task_batch in batch:\n",
    "            imgs, targets = task_batch\n",
    "            support_imgs, query_imgs, support_targets, query_targets = split_batch(imgs, targets)\n",
    "            local_model, output_weight, output_bias, classes = self.adapt_few_shot(support_imgs, support_targets)\n",
    "            \n",
    "            query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "            loss, preds, acc = self.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                loss.backward()\n",
    "\n",
    "                for p_global, p_local in zip(self.model.parameters(), local_model.parameters()):\n",
    "                    p_global.grad += p_local.grad\n",
    "            \n",
    "            accuracies.append(acc.mean().detach())\n",
    "            losses.append(loss.detach())\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            opt = self.optimizers()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        # print(\"Loss\", avg_loss)\n",
    "        self.log(\"%s_loss\" % mode, avg_loss)\n",
    "        self.log(\"%s_acc\" % mode, sum(accuracies) / len(accuracies))\n",
    "        \n",
    "        return avg_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.outer_loop(batch, mode=\"train\")\n",
    "        return None # loss.detach().requires_grad_()\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.outer_loop(batch, mode=\"val\")\n",
    "        torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBatchSampler(object):\n",
    "    \n",
    "    def __init__(self, dataset_targets, batch_size, N_way, K_shot, include_query=False, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.batch_sampler = FewShotBatchSampler(dataset_targets, N_way, K_shot, include_query, shuffle)\n",
    "        self.task_batch_size = batch_size\n",
    "        self.local_batch_size = self.batch_sampler.batch_size\n",
    "        print(len(self.batch_sampler), len(self))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        batch_list = []\n",
    "        for batch_idx, batch in enumerate(self.batch_sampler):\n",
    "            batch_list.extend(batch)\n",
    "            if (batch_idx+1) % self.task_batch_size == 0:\n",
    "                yield batch_list\n",
    "                batch_list = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)//self.task_batch_size\n",
    "    \n",
    "    def get_collate_fn(self):\n",
    "        def collate_fn(item_list):\n",
    "            imgs = torch.stack([img for img, target in item_list], dim=0)\n",
    "            targets = torch.stack([target for img, target in item_list], dim=0)\n",
    "            imgs = imgs.chunk(self.task_batch_size, dim=0)\n",
    "            targets = targets.chunk(self.task_batch_size, dim=0)\n",
    "            return list(zip(imgs, targets))\n",
    "        return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 75\n",
      "150 150\n"
     ]
    }
   ],
   "source": [
    "N_WAY = 5\n",
    "K_SHOT = 4\n",
    "train_protomaml_sampler = TaskBatchSampler(train_set.targets, \n",
    "                                              include_query=True,\n",
    "                                              N_way=N_WAY,\n",
    "                                              K_shot=K_SHOT,\n",
    "                                          batch_size=16)\n",
    "train_protomaml_loader = data.DataLoader(train_set, \n",
    "                                    batch_sampler=train_protomaml_sampler,\n",
    "                                         collate_fn=train_protomaml_sampler.get_collate_fn(),\n",
    "                                    num_workers=2)\n",
    "\n",
    "val_protomaml_sampler = TaskBatchSampler(val_set.targets, \n",
    "                                              include_query=True,\n",
    "                                              N_way=N_WAY,\n",
    "                                              K_shot=K_SHOT,\n",
    "                                          batch_size=1,\n",
    "                                        shuffle=False)\n",
    "val_protomaml_loader = data.DataLoader(val_set, \n",
    "                                    batch_sampler=val_protomaml_sampler,\n",
    "                                         collate_fn=val_protomaml_sampler.get_collate_fn(),\n",
    "                                    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_inner_steps', type=int, default=5)\n",
    "parser.add_argument('--lr_inner', type=float, default=0.01)\n",
    "parser.add_argument('--lr_output', type=float, default=0.04)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at ../saved_models/tutorial16/ProtoMAML.ckpt, loading...\n"
     ]
    }
   ],
   "source": [
    "protomaml_model = train_model(ProtoMAML, proto_dim=64, lr=1e-3, \n",
    "                              lr_inner=args.lr_inner,\n",
    "                              lr_output=args.lr_output,\n",
    "                              num_inner_steps=args.num_inner_steps,\n",
    "                              train_loader=train_protomaml_loader, \n",
    "                              val_loader=val_protomaml_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_protomaml(model, dataset, classes, k_shot=4, batch_size=128):\n",
    "    pl.seed_everything(42)\n",
    "    model = model.to(device)\n",
    "    num_classes = len(classes)\n",
    "    exmps_per_class = dataset.targets.shape[0]//len(classes)\n",
    "    \n",
    "    full_dataloader = data.DataLoader(dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      num_workers=4, \n",
    "                                      shuffle=False, \n",
    "                                      drop_last=False)\n",
    "    sampler = FewShotBatchSampler(dataset.targets, \n",
    "                                  include_query=False,\n",
    "                                  N_way=10,\n",
    "                                  K_shot=k_shot,\n",
    "                                  shuffle=False,\n",
    "                                  order=True)\n",
    "    sample_dataloader = data.DataLoader(dataset, \n",
    "                                        batch_sampler=sampler,\n",
    "                                        num_workers=2)\n",
    "    \n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch. Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for (support_imgs, support_targets), support_indices in zip(sample_dataloader, sampler): # tqdm(, \"Performing few-shot finetuning\"):\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        support_targets = support_targets.to(device)\n",
    "        local_model, output_weight, output_bias, classes = model.adapt_few_shot(support_imgs, support_targets)\n",
    "        with torch.no_grad():\n",
    "            local_model.eval()\n",
    "            batch_acc = torch.zeros((0,), dtype=torch.float32, device=device)\n",
    "            for query_imgs, query_targets in full_dataloader:\n",
    "                query_imgs = query_imgs.to(device)\n",
    "                query_targets = query_targets.to(device)\n",
    "                query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
    "                _, _, acc = model.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
    "                batch_acc = torch.cat([batch_acc, acc.detach()], dim=0)\n",
    "            for s_idx in support_indices:\n",
    "                batch_acc[s_idx] = 0\n",
    "            batch_acc = batch_acc.sum().item() / (batch_acc.shape[0] - len(support_indices))\n",
    "            accuracies.append(batch_acc)\n",
    "    return accuracies, mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "protomaml_model.hparams.num_inner_steps = 100\n",
    "protomaml_model.hparams.lr_inner = 0.01\n",
    "protomaml_model.hparams.lr_output = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0145a6fc992e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_protomaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotomaml_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy for k=%i: %4.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmean_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-c07246af276c>\u001b[0m in \u001b[0;36mtest_protomaml\u001b[0;34m(model, dataset, classes, k_shot, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msupport_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msupport_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_few_shot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-0598cb625e23>\u001b[0m in \u001b[0;36madapt_few_shot\u001b[0;34m(self, support_imgs, support_targets)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# print(\"Inner loop loss\", loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mlocal_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting few-shot eval...\")\n",
    "for k in [2, 4, 8, 16, 32]:\n",
    "    _, mean_acc = test_protomaml(protomaml_model, test_set, test_classes, k_shot=k)\n",
    "    print(\"Accuracy for k=%i: %4.2f%%\" % (k, 100.0*mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((1,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
