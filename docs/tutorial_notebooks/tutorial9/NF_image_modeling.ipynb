{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9.2: Normalizing Flows on image modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: show application of NFs on image modeling, assumably on MNIST. Introduce Activation Normalization, 1x1 convolution, multi-scale architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial9\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.int32)\n",
    "\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs):\n",
    "    imgs = torchvision.utils.make_grid(imgs, nrow=4, pad_value=128)\n",
    "    np_imgs = imgs.numpy()\n",
    "    sns.set_style(\"white\")\n",
    "    plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "#show_imgs([train_set[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFlow(nn.Module):\n",
    "    \n",
    "    def __init__(self, flows):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        \n",
    "    def forward(self, imgs, return_nll=False):\n",
    "        z = imgs\n",
    "        ldj = torch.zeros(z.shape[0], device=device)\n",
    "        for flow in self.flows:\n",
    "            z, ldj = flow(z, ldj, reverse=False)\n",
    "        log_pz = self.prior.log_prob(z).sum(dim=[1,2,3])\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        if return_nll:\n",
    "            return nll\n",
    "        else:\n",
    "            # Calculating bits per dimension\n",
    "            bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "            return bpd\n",
    "        \n",
    "    def sample(self, img_shape):\n",
    "        z = self.prior.sample(sample_shape=img_shape).to(device)\n",
    "        ldj = torch.zeros(img_shape[0], device=device)\n",
    "        with torch.no_grad():\n",
    "            for flow in reversed(self.flows):\n",
    "                z, ldj = flow(z, ldj, reverse=True)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1e-5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, ldj = self.dequant(z, ldj)\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=True)\n",
    "        else:\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=False)\n",
    "            z = z * 256\n",
    "            z = torch.floor(z).clamp(min=0, max=255).to(torch.int32)\n",
    "        return z, ldj\n",
    "    \n",
    "    def sigmoid(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            ldj += (-z-2*F.softplus(-z)).sum(dim=[1,2,3])\n",
    "            z = torch.sigmoid(z)\n",
    "        else:\n",
    "            z = z * (1 - self.alpha) + 0.5 * self.alpha\n",
    "            ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            ldj += (-torch.log(z) - torch.log(1-z)).sum(dim=[1,2,3])\n",
    "            z = torch.log(z) - torch.log(1-z)\n",
    "        return z, ldj\n",
    "    \n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32) \n",
    "        z = z + torch.rand_like(z)\n",
    "        z = z / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2539])\n",
      "tensor([-2.2539])\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor([2])\n",
    "print(-z-2*F.softplus(-z))\n",
    "z = torch.sigmoid(z)\n",
    "print(torch.log(z)+torch.log(1-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig tensor(0, dtype=torch.int32)\n",
      "Reconst tensor(1, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "orig_img = train_set[0][0].unsqueeze(dim=0)\n",
    "ldj = torch.zeros(1,)\n",
    "dequant_module = Dequantization()\n",
    "deq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\n",
    "reconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n",
    "\n",
    "d1, d2 = torch.where(orig_img.squeeze() != reconst_img.squeeze())\n",
    "for i in range(d1.shape[0]):\n",
    "    print(\"Orig\", orig_img[0,0,d1[i], d2[i]])\n",
    "    print(\"Reconst\", reconst_img[0,0,d1[i], d2[i]])\n",
    "\n",
    "# Layer is not strictly invertible due to float precision constraints\n",
    "# assert (orig_img == reconst_img).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDequantization(Dequantization):\n",
    "    \n",
    "    def __init__(self, var_flows, alpha=1e-5):\n",
    "        super().__init__(alpha=alpha)\n",
    "        self.flows = nn.ModuleList(var_flows)\n",
    "        \n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32)\n",
    "        img = (z / 255.0) * 2 - 1\n",
    "        deq_noise = torch.rand_like(z)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=True)\n",
    "        for flow in self.flows:\n",
    "            deq_noise, ldj = flow(deq_noise, ldj, reverse=False, orig_img=img)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=False)\n",
    "        z = (z + deq_noise) / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, network, mask, c_in):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.network = network\n",
    "        self.scaling_factor = nn.Parameter(torch.zeros(c_in))\n",
    "        \n",
    "        #self.network[-1].weight.data.zero_()\n",
    "        #self.network[-1].bias.data.zero_()\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False, orig_img=None):\n",
    "        z_in = z * self.mask\n",
    "        if orig_img is None:\n",
    "            nn_out = self.network(z_in)\n",
    "        else:\n",
    "            nn_out = self.network(torch.cat([z_in, orig_img], dim=1))\n",
    "        s, t = nn_out.chunk(2, dim=1)\n",
    "        ## Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        if not reverse:\n",
    "            z = (z + t) * torch.exp(s)\n",
    "            ldj += s.sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z = (z * torch.exp(-s)) - t\n",
    "            ldj -= s.sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "    xx, yy = torch.meshgrid(x, y)\n",
    "    mask = torch.fmod(xx + yy, 2)\n",
    "    mask = mask.to(torch.float32).view(1, 1, h, w)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    mask = torch.cat([torch.ones(c_in//2, dtype=torch.float32), \n",
    "                      torch.zeros(c_in-c_in//2, dtype=torch.float32)])\n",
    "    mask = mask.view(1, c_in, 1, 1)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow(flow, max_epochs=1, sample_shape=(8,1,28,28), model_name=\"MNISTFlow\"):\n",
    "    optimizer = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95) # Every epoch, we multiply the LR by 0.95\n",
    "    train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True)\n",
    "    \n",
    "    avg_bpd = 8.0\n",
    "    \n",
    "    val_scores = []\n",
    "    for epoch in range(max_epochs):\n",
    "        pbar = tqdm(train_loader, leave=False)\n",
    "        for imgs, _ in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            bpd = flow(imgs).mean()\n",
    "            bpd.backward()\n",
    "            nn.utils.clip_grad_norm_(flow.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_bpd = 0.9 * avg_bpd + 0.1 * bpd.item()\n",
    "            pbar.set_description(\"[Epoch %i] Bits per dimension: %5.3fbpd\" % (epoch+1, avg_bpd))\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        val_bpd = test_flow(flow, val_loader)\n",
    "        print(\"[Epoch %2i] Validation bits per dimension: %5.3fbpd\" % (epoch+1, val_bpd))\n",
    "        \n",
    "        if len(val_scores) == 0 or val_bpd < min(val_scores):\n",
    "            print(\"\\t   (New best performance, saving model...)\")\n",
    "            save_model(flow, CHECKPOINT_PATH, model_name)\n",
    "        val_scores.append(val_bpd)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            samples = flow.sample(img_shape=sample_shape)\n",
    "            # show_imgs(samples.cpu())\n",
    "            \n",
    "    # Plot a curve of the validation accuracy\n",
    "    # sns.set()\n",
    "    # plt.plot([i for i in range(1,len(val_scores)+1)], val_scores)\n",
    "    # plt.xlabel(\"Epochs\")\n",
    "    # plt.ylabel(\"Validation bits per dimension\")\n",
    "    # plt.title(\"Validation performance of %s\" % model_name)\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    print(\"Val scores\", val_scores)\n",
    "    \n",
    "    test_bpd = test_flow(flow, test_loader)\n",
    "    print(\"Test bits per dimension: %5.3fbpd\" % (test_bpd))\n",
    "    \n",
    "def test_flow(flow, data_loader, import_samples=8):\n",
    "    flow.eval()\n",
    "    test_bpd, counter = 0.0, 0\n",
    "    for imgs, _ in tqdm(data_loader, leave=False, desc=\"Testing...\"):\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            samples = []\n",
    "            for _ in range(import_samples):\n",
    "                img_nll = flow(imgs, return_nll=True)\n",
    "                samples.append(img_nll)\n",
    "            img_nll = torch.stack(samples, dim=-1)\n",
    "            img_nll = torch.logsumexp(img_nll, dim=-1) - np.log(import_samples)\n",
    "            img_bpd = img_nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "            \n",
    "            test_bpd += img_bpd.sum().item()\n",
    "            counter += img_bpd.shape[0]\n",
    "    test_bpd = test_bpd / counter\n",
    "    return test_bpd \n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, model_name + \".tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ConvNet(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden=32, c_out=-1):\n",
    "        super().__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        self.nn = nn.Sequential(\n",
    "                        nn.Conv2d(c_in, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_out, 3, padding=1)\n",
    "                     )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_hidden=1024):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(28*28, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 28*28*2)\n",
    "        )\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "        out = self.nn(x.view(x.shape[0], -1))\n",
    "        return out.reshape(-1, 2, h, w)\n",
    "    \n",
    "class ConvEncDecNet(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden=32):\n",
    "        super().__init__()\n",
    "        self.enc0 = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_hidden, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden, c_hidden, 3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden, c_hidden, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden, c_hidden*2, 3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden*2, c_hidden*2, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_hidden*2, c_hidden, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden*2, c_hidden, 1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(c_hidden, c_hidden, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec0 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden*2, c_hidden, 1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden, 2*c_in, 3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_enc0 = self.enc0(x)\n",
    "        x_enc1 = self.enc1(x_enc0)\n",
    "        x_enc2 = self.enc2(x_enc1)\n",
    "        x_dec2 = self.dec2(x_enc2)\n",
    "        x_dec1 = self.dec1(torch.cat([x_dec2, x_enc1], dim=1))\n",
    "        x_dec0 = self.dec0(torch.cat([x_dec1, x_enc0], dim=1))\n",
    "        return x_dec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_simple_flow(use_vardeq=True, model_name=\"MNISTFlow_simple\"):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=ConvNet(c_in=2,c_out=2),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(1)]\n",
    "        flow_layers += [VariationalDequantization(var_flows=vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    for i in range(8):\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=1),\n",
    "                                      mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                      c_in=1)]\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    train_flow(flow_model, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeFlow(nn.Module):\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        B, C, H, W = z.shape\n",
    "        if not reverse:\n",
    "            z = z.reshape(B, C, H//2, 2, W//2, 2)\n",
    "            z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "            z = z.reshape(B, 4*C, H//2, W//2)\n",
    "        else:\n",
    "            z = z.reshape(B, C//4, 2, 2, H, W)\n",
    "            z = z.permute(0, 1, 4, 2, 5, 3)\n",
    "            z = z.reshape(B, C//4, H*2, W*2)\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (before) tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]]]])\n",
      "Image (forward) tensor([[[[ 0,  2],\n",
      "          [ 8, 10]],\n",
      "\n",
      "         [[ 1,  3],\n",
      "          [ 9, 11]],\n",
      "\n",
      "         [[ 4,  6],\n",
      "          [12, 14]],\n",
      "\n",
      "         [[ 5,  7],\n",
      "          [13, 15]]]])\n",
      "Image (reverse) tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]]]])\n"
     ]
    }
   ],
   "source": [
    "sq_flow = SqueezeFlow()\n",
    "rand_img = torch.arange(16).view(1, 1, 4, 4)\n",
    "print(\"Image (before)\", rand_img)\n",
    "forward_img, _ = sq_flow(rand_img, ldj=None, reverse=False)\n",
    "print(\"Image (forward)\", forward_img)\n",
    "reconst_img, _ = sq_flow(forward_img, ldj=None, reverse=True)\n",
    "print(\"Image (reverse)\", reconst_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFlow(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, z_split = z.chunk(2, dim=1)\n",
    "            ldj += self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z_split = self.prior.sample(sample_shape=z.shape).to(device)\n",
    "            z = torch.cat([z, z_split], dim=1)\n",
    "            ldj -= self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        w_init = np.random.randn(c_in, c_in)\n",
    "        w_init = np.linalg.qr(w_init)[0].astype(np.float32)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(w_init))\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        weight_ldj = torch.slogdet(self.weight)[1]\n",
    "        weight_ldj = weight_ldj * z.shape[2] * z.shape[3]\n",
    "        if not reverse:\n",
    "            w = self.weight\n",
    "            ldj += weight_ldj\n",
    "        else:\n",
    "            w = torch.inverse(self.weight.double()).float()\n",
    "            ldj -= weight_ldj\n",
    "        \n",
    "        z = F.conv2d(z, w[:,:,None,None])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1ConvLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        w_init = np.random.randn(c_in, c_in)\n",
    "        w_init = np.linalg.qr(w_init)[0].astype(np.float32)\n",
    "        \n",
    "        # LU decomposition can slightly speed up the inverse\n",
    "        np_p, np_l, np_u = scipy.linalg.lu(w_init)\n",
    "        np_s = np.diag(np_u)\n",
    "        np_sign_s = np.sign(np_s)\n",
    "        np_log_s = np.log(np.abs(np_s))\n",
    "        np_u = np.triu(np_u, k=1)\n",
    "        l_mask = np.tril(np.ones(w_init.shape, dtype=np.float32), -1)\n",
    "        eye = np.eye(*w_init.shape, dtype=np.float32)\n",
    "\n",
    "        self.register_buffer('p', torch.Tensor(np_p.astype(np.float32)))\n",
    "        self.register_buffer('sign_s', torch.Tensor(np_sign_s.astype(np.float32)))\n",
    "        self.l = nn.Parameter(torch.Tensor(np_l.astype(np.float32)), requires_grad=True)\n",
    "        self.log_s = nn.Parameter(torch.Tensor(np_log_s.astype(np.float32)), requires_grad=True)\n",
    "        self.u = nn.Parameter(torch.Tensor(np_u.astype(np.float32)), requires_grad=True)\n",
    "        self.register_buffer('l_mask', torch.Tensor(l_mask))\n",
    "        self.register_buffer('eye', torch.Tensor(eye))\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        l, log_s, u = self.l, self.log_s, self.u\n",
    "        l = l * self.l_mask + self.eye\n",
    "        u = u * self.l_mask.transpose(0, 1).contiguous() + torch.diag(self.sign_s * torch.exp(log_s))\n",
    "        w = torch.matmul(self.p, torch.matmul(l, u))\n",
    "        weight_ldj = log_s.sum()\n",
    "        weight_ldj = weight_ldj * z.shape[2] * z.shape[3]\n",
    "        if not reverse:\n",
    "            ldj += weight_ldj\n",
    "        else:\n",
    "            w = torch.inverse(w.double()).float()\n",
    "            ldj -= weight_ldj\n",
    "        \n",
    "        z = F.conv2d(z, w[:,:,None,None])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiscale_flow(use_vardeq=True, use_1x1_convs=True, model_name=\"MNISTFlow_multiscale\"):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=ConvNet(c_in=2,c_out=2),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    \n",
    "    flow_layers += [CouplingLayer(network=ConvNet(c_in=1),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(2):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=4, c_hidden=48),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    if use_1x1_convs:\n",
    "        flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "    flow_layers += [Invertible1x1ConvLU(c_in=4),\n",
    "                    SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=8)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=8, c_hidden=64),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    train_flow(flow_model, sample_shape=[8,8,7,7], model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Validation bits per dimension: 1.573bpd\n",
      "\t   (New best performance, saving model...)\n",
      "Val scores [1.5731880645751952]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bits per dimension: 1.572bpd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Validation bits per dimension: 1.501bpd\n",
      "\t   (New best performance, saving model...)\n",
      "Val scores [1.5013453960418701]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bits per dimension: 1.502bpd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Validation bits per dimension: 1.335bpd\n",
      "\t   (New best performance, saving model...)\n",
      "Val scores [1.3345446292877197]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89851d4300b14f83a094de0a91fa092d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_simple_flow(use_vardeq=False, model_name=\"MNISTFlow_simple\")\n",
    "train_simple_flow(use_vardeq=True, model_name=\"MNISTFlow_vardeq\")\n",
    "train_multiscale_flow(use_1x1_convs=False, model_name=\"MNISTFlow_multiscale\")\n",
    "train_multiscale_flow(use_1x1_convs=True, model_name=\"MNISTFlow_multiscale_1x1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
