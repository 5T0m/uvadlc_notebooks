{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide 3: Debugging in PyTorch\n",
    "\n",
    "When you start learning PyTorch, it is expected that you hit bugs and errors. To help you debug your code, we will summarize the most common mistakes in this guide, explain why they happen, and how you can solve them.\n",
    "\n",
    "## My model is underperforming\n",
    "\n",
    "These errors are the most \"evil\" and hard to debug. Your model is not reaching the performance that it should, but PyTorch is not telling you why that happens. There are couple of things you can check. If none of these solve the problem for you, one of us TAs will help you debug your code more in detail.\n",
    "\n",
    "### Softmax, CrossEntropyLoss and NLLLoss\n",
    "\n",
    "The most common mistake is the mismatch between loss function and output activation function. The loss module `nn.CrossEntropyLoss` in PyTorch performs two operations: `nn.LogSoftmax` and `nn.NLLLoss`. Hence, the input to this loss module should be the output of your last linear layer. **Do not apply a softmax before the Cross-Entropy loss**. Otherwise, PyTorch will apply a log-softmax on your softmax outputs, which will significantly worsens the performance, and gives you headaches. \n",
    "\n",
    "If you use the loss module `nn.NLLLoss`, you need to apply the log-softmax yourself. **NLLLoss requires log-probabilities, not plain probabilities.** Hence, make sure to apply `nn.LogSoftmax` or `nn.functional.log_softmax`, and **not** `nn.Softmax`.\n",
    "\n",
    "### Categorical data and Embedding\n",
    "\n",
    "Categorical data, as for example language characters or the datasets you are given in assignment 2, require special care. Data like language characters 'a', 'b', 'c' etc. are usually represented as integers 0, 1, 2, etc. **Do not use integers as input for categorical data.** If you would enter those integers as inputs to the model, two problems arise. \n",
    "\n",
    "1. You bias the model to see relations where there are none. In the language example above, the model would think that 'a' is closer to 'b' than to 'o', although 'a' and 'o' are both vocals, and the closeness of 'a' and 'b' does not necessarily say anything about there usage.\n",
    "2. If you have many categories, you will have input values between 0 and >50. The model will have a hard time separating all those >50 categories without blending over some. Hence, the model loses a lot of information although this is not necessary.\n",
    "\n",
    "The much better option in the case of categorical data is to use one-hot vectors, or embeddings. A one-hot vector represents each category by a vector of 0s, with one index being 1. This makes the model's life much easier as it can distinguish between the categories in a very simple manner (if feature !=0, it is a specific category). Alternatively, you can learn an embedding with the help of `nn.Embedding`. The inputs to this module are: \n",
    "\n",
    "* `num_embeddings` which is the number of different categories you have in your input data (in case of language characters, something like 26 as you have 'a' to 'z')\n",
    "* `embedding_dim` which is the number of features you want to represent each category with. If you use the embedding directly as input to an LSTM or RNN, a good rule of thumb is to use 1/4 - 1/2 of your hidden size inside the LSTM.\n",
    "* `padding_idx` which would allow you to assign a specific index for the padding symbol. Can be skipped if you do not use \"-1\" as padding index.\n",
    "\n",
    "The embedding feature vectors are initialized from $\\mathcal{N}(0,1)$. **Do not overwrite this init by Kaiming, Xavier or similar.** The used standard deviation is 1 because the initialization, activation functions etc. have been designed to have a input standard deviation of 1. Example usage of the embedding module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "Output shape: torch.Size([3, 2, 32])\n",
      "Example features:\n",
      " tensor([[[ 0.3128, -0.6407],\n",
      "         [ 1.4841,  0.6402]],\n",
      "\n",
      "        [[-0.1510,  0.9268],\n",
      "         [-1.2901, -0.0687]],\n",
      "\n",
      "        [[ 0.3128, -0.6407],\n",
      "         [ 0.1267, -1.0100]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Create 5 embedding vectors each with 32 features\n",
    "embedding = nn.Embedding(num_embeddings=5,\n",
    "                         embedding_dim=32)\n",
    "\n",
    "# Example integer input\n",
    "input_tensor = torch.LongTensor([[0, 4], [2, 3], [0, 1]])\n",
    "\n",
    "# Get embeddings\n",
    "embed_vectors = embedding(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", embed_vectors.shape)\n",
    "print(\"Example features:\\n\", embed_vectors[:,:,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden shape mismatch\n",
    "\n",
    "If you perform matrix multiplications and have a shape mismatch between two matrices, PyTorch will complain and throw an error. However, there are also situations where PyTorch does not throw an error because the misaligned dimensions have (unluckily) the same size. For instance, imagine you have a weight matrix of size $d_{in}\\times d_{out}$. If you take the input $x$ of size $B \\times d_{in}$ ($B$ being the batch dimension), and in your hyperparameter setting, $B=d_{in}$, you can end up performing the matrix multiplication over the wrong dimension while PyTorch is not detecting it. **Test your code with multiple, different batch sizes to prevent shape misalignments with the batch dimension.**\n",
    "\n",
    "### Training and Evaluation switch\n",
    "\n",
    "In PyTorch, a module and/or neural network has two modes: training and evaluation. You switch between them using `model.eval()` and `model.train()`. The modes decide for instance whether to apply dropout or not, and how to handle the forward of Batch Normalization. However, a common mistake is to forget to set your model back into training mode after evaluation. **Make sure to set your model back to train mode after validation.** In case your model does not contain dropout, BatchNorm or similar modules, this might not effect your performance.\n",
    "\n",
    "### Parameter handling\n",
    "\n",
    "As you might know from the PyTorch Tutorial, PyTorch supports hierarchical usage of `nn.Modules`. One module can contain another module, which can again contain a module, and so on. When you call `.parameters()` on a module, PyTorch looks for all modules inside the module to also add their parameters to the highest-level module's parameter. However, **PyTorch does not detect parameters of modules in lists, dicts or similar structures.** If you have a list of modules, make sure to put them into a `nn.ModuleList` or `nn.Sequential` object. Parameters of modules inside those containers are detected. Similarly, for dictionaries, you can use `nn.ModuleDict`.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Initializing the parameters of your model correctly is very important (see [Tutorial 4](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html) for details on this). Initializing parameters with a standard normal distribution is not a good practice and often fails. It can occassionally work for very shallow networks, but don't risk it! **Think about your initialization, and use proper methods like Kaiming or Xavier.**\n",
    "\n",
    "### Zero-grad in optimizers\n",
    "\n",
    "**Remember to call optimizer.zero_grad() before doing loss.backward().** If you do not reset the gradients for all parameters before performing backpropagation, your gradients will be added to those from the previous batch. Hence, your gradients end up to be not the ones you intended them to be.\n",
    "\n",
    "### Weight decay and Adam\n",
    "\n",
    "Adam is known to have a different implementation of weight decay in many frameworks than you would expect. Specifically, the weight decay is usually added as gradients **before** determining the adaptive learning rate, and hence scaling up the weight decay for parameters with low gradient norms. Details on this problem, which is actually shared across most common DL frameworks, can be found [here](https://arxiv.org/pdf/1711.05101.pdf). In PyTorch, you can use the desired version of weight decay in Adam using `torch.optim.AdamW` (identical to `torch.optim.Adam` besides the weight decay implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch throws an error\n",
    "\n",
    "These errors are the \"happy\" errors because PyTorch actually talks to you about what is wrong. Until these are not solved, you probably cannot train your model.\n",
    "\n",
    "### Trying to backward through the graph a second time, specify retain_graph=True\n",
    "\n",
    "This error occurs if you re-use a tensor from the computation graph of the previous batch. This should usually not happen. Make sure to not keep tensors across batches if not strictly necessary. Example where this issue can occur: when implementing your own LSTM, make sure that the initial hidden state is a constant zero tensor, and not the last hidden state of the previuos batch.\n",
    "\n",
    "### Size mismatch\n",
    "\n",
    "This usually occurs if your dimension of the input to a module does not match the specified input dimension of the weight tensor, like in a Linear layer. Make sure to have specified the correct dimensions. Usually, a good way to debug this is to print the shape of the input tensor before every layer you call. \n",
    "\n",
    "If this happens for a matrix multiplication you have implemented, print the shapes of both matrices, and try to figure out over which dimension the matrix multiplication should actually have been performed, and over which PyTorch currently does it.\n",
    "\n",
    "### Device mismatch\n",
    "\n",
    "You might sometimes see a mistake such as: `Runtime Error: Input type (torch.FloatTensor) dand weigh type (torch.cuda.FloatTensor) should be on the same device`. This error indicates that the input data is on CPU, while your weights are on the GPU. Make sure that all data is on the same device. This is usually the GPU as it support acceleration for both training and testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
