{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Graph Neural Networks\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Under%20development&color=red)\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)  \n",
    "**Empty notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)  \n",
    "**Pre-trained models:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.0.3\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.cuda(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph representation\n",
    "\n",
    "* Adjacency matrix\n",
    "* List of edges\n",
    "\n",
    "While expressing a graph as a list of edges is more efficient, using an adjacency matrix is more intuitive and simpler to implement. We will use the list of edges to define a sparse adjacency matrix with which we can work as if it was a dense matrix, but allows more efficient operations.\n",
    "\n",
    "* Introduction to `torch.sparse` and its concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "        \n",
    "    \n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(2, 2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "node_feats = torch.arange(6, dtype=torch.float32).view(1, 3, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 0, 1]]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "        \n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "        \n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * batch_size + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * batch_size + edges[:,2]\n",
    "        \n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "        \n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "        \n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "        \n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        \n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs\n",
      " tensor([[[[0.3543, 0.6457, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 1.0000]],\n",
      "\n",
      "         [[0.5200, 0.4800, 0.0000],\n",
      "          [0.0000, 1.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 1.0000]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]])\n",
      "Output features tensor([[[1.2913, 1.9600],\n",
      "         [2.0000, 3.0000],\n",
      "         [4.0000, 5.0000]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.15, -0.1]])\n",
    "node_feats = torch.arange(6, dtype=torch.float32).view(1, 3, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 0, 1]]])\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [22:40:59] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# torch geometric\n",
    "try: \n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    !pip install torch_geometric\n",
    "    import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-level tasks: Semi-supervised node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "        \n",
    "        layers = []\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=c_in if l_idx == 0 else c_hidden, \n",
    "                          out_channels=c_hidden,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "        layers += [gnn_layer(in_channels=c_hidden, \n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        for l in self.layers:\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, \"Unknown forward mode: %s\" % mode\n",
    "        \n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         checkpoint_callback=ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                         gpus=1 if torch.cuda.is_available() else 0,\n",
    "                         max_epochs=200,\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"NodeLevel%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = NodeLevelGNN(c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "    # Test best model on validation and test set\n",
    "    test_result = trainer.test(model, test_dataloaders=node_data_loader, verbose=False)\n",
    "    result = {\"test\": test_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I1029 22:45:49.348504 140238683461440 distributed.py:49] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I1029 22:45:49.350577 140238683461440 distributed.py:49] TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I1029 22:45:49.352211 140238683461440 accelerator_connector.py:385] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | GNNModel         | 23 K  \n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "I1029 22:45:49.362196 140238683461440 lightning.py:1290] \n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | GNNModel         | 23 K  \n",
      "1 | loss_module | CrossEntropyLoss | 0     \n"
     ]
    }
   ],
   "source": [
    "model, result = train_node_classifier(model_name=\"GCN\", dataset=cora_dataset, c_hidden=16, layer_name=\"GCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': [{'train_loss': 0.031641341745853424, 'train_acc': 1.0, 'val_acc': 0.7839999794960022, 'test_acc': 0.8240000009536743}]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge-level tasks: Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-level tasks: Graph classification\n",
    "\n",
    "https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in, \n",
    "                            c_hidden=c_hidden, \n",
    "                            c_out=c_hidden*2, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden*2, c_out)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, batch_idxs):\n",
    "        x = self.super.forward(x, edge_index)\n",
    "        x = geom_nn.global_max_pool(x, batch_idx)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset = torch_geometric.datasets.QM9(root=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==========================================**\n",
    "\n",
    "**==========================================**\n",
    "## Old code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse version\n",
    "\n",
    "As there doesn't exist a batch-wise matrix multiplication function in the sparse package yet, we have to do it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bspmm(mat1, mat2):\n",
    "    return torch.stack([torch.spmm(mat1[i], mat2[i]) for i in range(mat1.shape[0])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNSparseLayer(GCNLayer):\n",
    "    \n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Sparse Tensor with entries of 1 where there is an edge between nodes (including self-connection). \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Determine number of neighbours for averaging (torch.sparse.sum is same as .sum on sparse matrices)\n",
    "        num_neighbours = torch.sparse.sum(adj_matrix, dim=-1)\n",
    "        num_neighbours = num_neighbours.to_dense() # Convert sparse tensor to dense one\n",
    "        \n",
    "        node_feats = self.projection(node_feats)\n",
    "        \n",
    "        # torch.bmm on sparse matrices\n",
    "        node_feats = bspmm(adj_matrix, node_feats)\n",
    "        \n",
    "        node_feats = node_feats / num_neighbours.unsqueeze(dim=-1)\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor(indices=tensor([[0, 0, 0, 0],\n",
      "                       [0, 0, 1, 2],\n",
      "                       [0, 1, 1, 2]]),\n",
      "       values=tensor([1., 1., 1., 1.]),\n",
      "       size=(1, 3, 3), nnz=4, layout=torch.sparse_coo)\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = GCNSparseLayer(2, 2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "node_feats = torch.arange(6, dtype=torch.float32).view(1, 3, 2)\n",
    "\n",
    "adj_matrix = torch.sparse.FloatTensor(torch.LongTensor([[0,0,0], [0,0,1], [0,1,1], [0,2,2]]).t(),\n",
    "                                      torch.FloatTensor([1, 1, 1, 1]),\n",
    "                                      torch.Size([1,3,3]))\n",
    "\n",
    "out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing efficiency\n",
    "\n",
    "We can test the efficiency/gain we have gotten when using the efficient version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_efficiency(layer_class, layer_sparse_class, c_in, batch_size=4, graph_size=5000, link_prob=0.01, **kwargs):\n",
    "    normal_layer = layer_class(c_in=c_in, **kwargs)\n",
    "    sparse_layer = layer_sparse_class(c_in=c_in, **kwargs)\n",
    "    sparse_layer.load_state_dict(normal_layer.state_dict())\n",
    "    \n",
    "    node_feats = torch.randn(batch_size, graph_size, c_in)\n",
    "    adj_matrix = (torch.rand(batch_size, graph_size, graph_size) < link_prob).float()\n",
    "    edges = adj_matrix.nonzero().t()\n",
    "    print(edges.shape)\n",
    "    sparse_adj_matrix = torch.sparse.FloatTensor(edges,\n",
    "                                                 torch.ones(edges.shape[1],),\n",
    "                                                 adj_matrix.shape)\n",
    "    \n",
    "    test_layer(normal_layer, node_feats, adj_matrix)\n",
    "    test_layer(sparse_layer, node_feats, sparse_adj_matrix)\n",
    "    \n",
    "    \n",
    "def test_layer(layer_module, node_feats, adj_matrix):\n",
    "    torch.cuda.empty_cache() \n",
    "    layer_module.to(device)\n",
    "    node_feats, adj_matrix = node_feats.to(device), adj_matrix.to(device)\n",
    "    start_time = time.time()\n",
    "    out = layer_module(node_feats, adj_matrix)\n",
    "    out.sum().backward()\n",
    "    end_time = time.time()\n",
    "    print(\"Time: %6.5fs\" % (end_time - start_time))\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA memory: %4.2fGB\" % (torch.cuda.memory_allocated(device)/1e9))\n",
    "    \n",
    "    layer_module.to(torch.device(\"cpu\"))\n",
    "    del node_feats\n",
    "    del adj_matrix\n",
    "    del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16000893])\n",
      "Time: 0.00446s\n",
      "CUDA memory: 8.13GB\n",
      "Time: 1.20744s\n",
      "CUDA memory: 2.17GB\n"
     ]
    }
   ],
   "source": [
    "test_efficiency(GCNLayer, GCNSparseLayer, c_in=512, c_out=512, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATSparseLayer(GATLayer):\n",
    "    \n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "        edges = adj_matrix._indices().t()\n",
    "        \n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * batch_size + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * batch_size + edges[:,2]\n",
    "        \n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) # Shape: [batch, num_nodes, num_nodes, num_heads]\n",
    "        attn_logits = self.leakyrelu(attn_logits) \n",
    "        attn_unprobs = attn_logits.exp()\n",
    "        \n",
    "        node_feats_per_head = []\n",
    "        for h in range(self.num_heads): # Split over heads\n",
    "            attn_matrix = torch.sparse.FloatTensor(adj_matrix._indices(),\n",
    "                                                   attn_unprobs[:,h],\n",
    "                                                   size=adj_matrix.shape)\n",
    "            node_head_feats = bspmm(attn_matrix, node_feats[:,:,h])\n",
    "            attn_row_sum = torch.sparse.sum(attn_matrix, dim=-1).to_dense()\n",
    "            node_head_feats = node_head_feats / attn_row_sum.unsqueeze(dim=-1)\n",
    "            node_feats_per_head.append(node_head_feats)\n",
    "        \n",
    "        if self.concat_heads:\n",
    "            node_feats = torch.cat(node_feats_per_head, dim=-1)\n",
    "        else:\n",
    "            node_feats = sum(node_feats_per_head) / self.num_heads\n",
    "        \n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor(indices=tensor([[0, 0, 0, 0],\n",
      "                       [0, 0, 1, 2],\n",
      "                       [0, 1, 1, 2]]),\n",
      "       values=tensor([1., 1., 1., 1.]),\n",
      "       size=(1, 3, 3), nnz=4, layout=torch.sparse_coo)\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.]]])\n",
      "Output features tensor([[[1.2913, 1.9600],\n",
      "         [2.0000, 3.0000],\n",
      "         [4.0000, 5.0000]]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "layer = GATSparseLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.15, -0.1]])\n",
    "node_feats = torch.arange(6, dtype=torch.float32).view(1, 3, 2)\n",
    "\n",
    "adj_matrix = torch.sparse.FloatTensor(torch.LongTensor([[0,0,0], [0,0,1], [0,1,1], [0,2,2]]).t(),\n",
    "                                      torch.FloatTensor([1, 1, 1, 1]),\n",
    "                                      torch.Size([1,3,3]))\n",
    "\n",
    "out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10080])\n",
      "tensor([[  0,   0,  80],\n",
      "        [  0,   0, 409],\n",
      "        [  0,   1, 136],\n",
      "        ...,\n",
      "        [  3, 499, 237],\n",
      "        [  3, 499, 241],\n",
      "        [  3, 499, 282]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-724ca5654715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_efficiency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGATLayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATSparseLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-f0ec22bfba64>\u001b[0m in \u001b[0;36mtest_efficiency\u001b[0;34m(layer_class, layer_sparse_class, c_in, batch_size, graph_size, link_prob, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                  adj_matrix.shape)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtest_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_adj_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-f0ec22bfba64>\u001b[0m in \u001b[0;36mtest_layer\u001b[0;34m(layer_module, node_feats, adj_matrix)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnode_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-05ecf4f6d439>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_feats, adj_matrix, print_attn_probs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_heads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mnode_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mnode_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "test_efficiency(GATLayer2, GATSparseLayer, c_in=128, c_out=128, num_heads=4, batch_size=4, graph_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
