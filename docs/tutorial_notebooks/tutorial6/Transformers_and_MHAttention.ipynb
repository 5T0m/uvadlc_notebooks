{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Transformers and Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.0.3\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot Product Attention:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[[ 0.3367,  0.1288],\n",
      "         [ 0.2345,  0.2303],\n",
      "         [-1.1229, -0.1863]]])\n",
      "K\n",
      " tensor([[[ 2.2082, -0.6380],\n",
      "         [ 0.4617,  0.2674],\n",
      "         [ 0.5349,  0.8094]]])\n",
      "V\n",
      " tensor([[[ 1.1103, -1.6898],\n",
      "         [-0.9890,  0.9580],\n",
      "         [ 1.3221,  0.8172]]])\n",
      "Values\n",
      " tensor([[[ 0.5698, -0.1520],\n",
      "         [ 0.5379, -0.0265],\n",
      "         [ 0.2246,  0.5556]]])\n",
      "Attention\n",
      " tensor([[[0.4028, 0.2886, 0.3086],\n",
      "         [0.3538, 0.3069, 0.3393],\n",
      "         [0.1303, 0.4630, 0.4067]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, d_k = 1, 3, 2\n",
    "q = torch.randn(batch_size, seq_len, d_k)\n",
    "k = torch.randn(batch_size, seq_len, d_k)\n",
    "v = torch.randn(batch_size, seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5698)\n",
      "tensor([0.4028, 0.2886, 0.3086])\n",
      "tensor([ 1.1103, -0.9890,  1.3221])\n"
     ]
    }
   ],
   "source": [
    "print((attention[0,0] * v[0,:,0]).sum())\n",
    "print(attention[0,0])\n",
    "print(v[0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where} \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-block: combining multi-head attention with linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "        \n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block is copied from [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "    \n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"197.039844pt\" version=\"1.1\" viewBox=\"0 0 485.59 197.039844\" width=\"485.59pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2020-11-03T16:19:26.328009</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 197.039844 \n",
       "L 485.59 197.039844 \n",
       "L 485.59 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 31.99 170.28 \n",
       "L 478.39 170.28 \n",
       "L 478.39 7.2 \n",
       "L 31.99 7.2 \n",
       "z\n",
       "\" style=\"fill:#eaeaf2;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 52.280909 170.28 \n",
       "L 52.280909 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(49.222393 187.653594)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 4.15625 35.296875 \n",
       "Q 4.15625 48 6.765625 55.734375 \n",
       "Q 9.375 63.484375 14.515625 67.671875 \n",
       "Q 19.671875 71.875 27.484375 71.875 \n",
       "Q 33.25 71.875 37.59375 69.546875 \n",
       "Q 41.9375 67.234375 44.765625 62.859375 \n",
       "Q 47.609375 58.5 49.21875 52.21875 \n",
       "Q 50.828125 45.953125 50.828125 35.296875 \n",
       "Q 50.828125 22.703125 48.234375 14.96875 \n",
       "Q 45.65625 7.234375 40.5 3 \n",
       "Q 35.359375 -1.21875 27.484375 -1.21875 \n",
       "Q 17.140625 -1.21875 11.234375 6.203125 \n",
       "Q 4.15625 15.140625 4.15625 35.296875 \n",
       "z\n",
       "M 13.1875 35.296875 \n",
       "Q 13.1875 17.671875 17.3125 11.828125 \n",
       "Q 21.4375 6 27.484375 6 \n",
       "Q 33.546875 6 37.671875 11.859375 \n",
       "Q 41.796875 17.71875 41.796875 35.296875 \n",
       "Q 41.796875 52.984375 37.671875 58.78125 \n",
       "Q 33.546875 64.59375 27.390625 64.59375 \n",
       "Q 21.34375 64.59375 17.71875 59.46875 \n",
       "Q 13.1875 52.9375 13.1875 35.296875 \n",
       "z\n",
       "\" id=\"ArialMT-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 103.033558 170.28 \n",
       "L 103.033558 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 250 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(93.858011 187.653594)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 50.34375 8.453125 \n",
       "L 50.34375 0 \n",
       "L 3.03125 0 \n",
       "Q 2.9375 3.171875 4.046875 6.109375 \n",
       "Q 5.859375 10.9375 9.828125 15.625 \n",
       "Q 13.8125 20.3125 21.34375 26.46875 \n",
       "Q 33.015625 36.03125 37.109375 41.625 \n",
       "Q 41.21875 47.21875 41.21875 52.203125 \n",
       "Q 41.21875 57.421875 37.46875 61 \n",
       "Q 33.734375 64.59375 27.734375 64.59375 \n",
       "Q 21.390625 64.59375 17.578125 60.78125 \n",
       "Q 13.765625 56.984375 13.71875 50.25 \n",
       "L 4.6875 51.171875 \n",
       "Q 5.609375 61.28125 11.65625 66.578125 \n",
       "Q 17.71875 71.875 27.9375 71.875 \n",
       "Q 38.234375 71.875 44.234375 66.15625 \n",
       "Q 50.25 60.453125 50.25 52 \n",
       "Q 50.25 47.703125 48.484375 43.546875 \n",
       "Q 46.734375 39.40625 42.65625 34.8125 \n",
       "Q 38.578125 30.21875 29.109375 22.21875 \n",
       "Q 21.1875 15.578125 18.9375 13.203125 \n",
       "Q 16.703125 10.84375 15.234375 8.453125 \n",
       "z\n",
       "\" id=\"ArialMT-50\"/>\n",
       "        <path d=\"M 4.15625 18.75 \n",
       "L 13.375 19.53125 \n",
       "Q 14.40625 12.796875 18.140625 9.390625 \n",
       "Q 21.875 6 27.15625 6 \n",
       "Q 33.5 6 37.890625 10.78125 \n",
       "Q 42.28125 15.578125 42.28125 23.484375 \n",
       "Q 42.28125 31 38.0625 35.34375 \n",
       "Q 33.84375 39.703125 27 39.703125 \n",
       "Q 22.75 39.703125 19.328125 37.765625 \n",
       "Q 15.921875 35.84375 13.96875 32.765625 \n",
       "L 5.71875 33.84375 \n",
       "L 12.640625 70.609375 \n",
       "L 48.25 70.609375 \n",
       "L 48.25 62.203125 \n",
       "L 19.671875 62.203125 \n",
       "L 15.828125 42.96875 \n",
       "Q 22.265625 47.46875 29.34375 47.46875 \n",
       "Q 38.71875 47.46875 45.15625 40.96875 \n",
       "Q 51.609375 34.46875 51.609375 24.265625 \n",
       "Q 51.609375 14.546875 45.953125 7.46875 \n",
       "Q 39.0625 -1.21875 27.15625 -1.21875 \n",
       "Q 17.390625 -1.21875 11.203125 4.25 \n",
       "Q 5.03125 9.71875 4.15625 18.75 \n",
       "z\n",
       "\" id=\"ArialMT-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 153.786207 170.28 \n",
       "L 153.786207 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 500 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(144.61066 187.653594)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 204.538856 170.28 \n",
       "L 204.538856 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 750 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(195.363309 187.653594)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 4.734375 62.203125 \n",
       "L 4.734375 70.65625 \n",
       "L 51.078125 70.65625 \n",
       "L 51.078125 63.8125 \n",
       "Q 44.234375 56.546875 37.515625 44.484375 \n",
       "Q 30.8125 32.421875 27.15625 19.671875 \n",
       "Q 24.515625 10.6875 23.78125 0 \n",
       "L 14.75 0 \n",
       "Q 14.890625 8.453125 18.0625 20.40625 \n",
       "Q 21.234375 32.375 27.171875 43.484375 \n",
       "Q 33.109375 54.59375 39.796875 62.203125 \n",
       "z\n",
       "\" id=\"ArialMT-55\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-55\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 255.291505 170.28 \n",
       "L 255.291505 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1000 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(243.057443 187.653594)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 37.25 0 \n",
       "L 28.46875 0 \n",
       "L 28.46875 56 \n",
       "Q 25.296875 52.984375 20.140625 49.953125 \n",
       "Q 14.984375 46.921875 10.890625 45.40625 \n",
       "L 10.890625 53.90625 \n",
       "Q 18.265625 57.375 23.78125 62.296875 \n",
       "Q 29.296875 67.234375 31.59375 71.875 \n",
       "L 37.25 71.875 \n",
       "z\n",
       "\" id=\"ArialMT-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 306.044154 170.28 \n",
       "L 306.044154 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1250 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(293.810092 187.653594)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 356.796803 170.28 \n",
       "L 356.796803 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1500 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(344.562741 187.653594)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 407.549452 170.28 \n",
       "L 407.549452 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1750 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(395.31539 187.653594)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-55\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 458.302102 170.28 \n",
       "L 458.302102 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2000 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(446.068039 187.653594)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 162.867273 \n",
       "L 478.39 162.867273 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.0 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 166.80407)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 9.078125 0 \n",
       "L 9.078125 10.015625 \n",
       "L 19.09375 10.015625 \n",
       "L 19.09375 0 \n",
       "z\n",
       "\" id=\"ArialMT-46\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 133.032707 \n",
       "L 478.39 133.032707 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.2 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 136.969504)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 103.198142 \n",
       "L 478.39 103.198142 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.4 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 107.134938)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 32.328125 0 \n",
       "L 32.328125 17.140625 \n",
       "L 1.265625 17.140625 \n",
       "L 1.265625 25.203125 \n",
       "L 33.9375 71.578125 \n",
       "L 41.109375 71.578125 \n",
       "L 41.109375 25.203125 \n",
       "L 50.78125 25.203125 \n",
       "L 50.78125 17.140625 \n",
       "L 41.109375 17.140625 \n",
       "L 41.109375 0 \n",
       "z\n",
       "M 32.328125 25.203125 \n",
       "L 32.328125 57.46875 \n",
       "L 9.90625 25.203125 \n",
       "z\n",
       "\" id=\"ArialMT-52\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 73.363576 \n",
       "L 478.39 73.363576 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.6 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 77.300373)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 49.75 54.046875 \n",
       "L 41.015625 53.375 \n",
       "Q 39.84375 58.546875 37.703125 60.890625 \n",
       "Q 34.125 64.65625 28.90625 64.65625 \n",
       "Q 24.703125 64.65625 21.53125 62.3125 \n",
       "Q 17.390625 59.28125 14.984375 53.46875 \n",
       "Q 12.59375 47.65625 12.5 36.921875 \n",
       "Q 15.671875 41.75 20.265625 44.09375 \n",
       "Q 24.859375 46.4375 29.890625 46.4375 \n",
       "Q 38.671875 46.4375 44.84375 39.96875 \n",
       "Q 51.03125 33.5 51.03125 23.25 \n",
       "Q 51.03125 16.5 48.125 10.71875 \n",
       "Q 45.21875 4.9375 40.140625 1.859375 \n",
       "Q 35.0625 -1.21875 28.609375 -1.21875 \n",
       "Q 17.625 -1.21875 10.6875 6.859375 \n",
       "Q 3.765625 14.9375 3.765625 33.5 \n",
       "Q 3.765625 54.25 11.421875 63.671875 \n",
       "Q 18.109375 71.875 29.4375 71.875 \n",
       "Q 37.890625 71.875 43.28125 67.140625 \n",
       "Q 48.6875 62.40625 49.75 54.046875 \n",
       "z\n",
       "M 13.875 23.1875 \n",
       "Q 13.875 18.65625 15.796875 14.5 \n",
       "Q 17.71875 10.359375 21.1875 8.171875 \n",
       "Q 24.65625 6 28.46875 6 \n",
       "Q 34.03125 6 38.03125 10.484375 \n",
       "Q 42.046875 14.984375 42.046875 22.703125 \n",
       "Q 42.046875 30.125 38.078125 34.390625 \n",
       "Q 34.125 38.671875 28.125 38.671875 \n",
       "Q 22.171875 38.671875 18.015625 34.390625 \n",
       "Q 13.875 30.125 13.875 23.1875 \n",
       "z\n",
       "\" id=\"ArialMT-54\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 43.52901 \n",
       "L 478.39 43.52901 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.8 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 47.465807)scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path d=\"M 17.671875 38.8125 \n",
       "Q 12.203125 40.828125 9.5625 44.53125 \n",
       "Q 6.9375 48.25 6.9375 53.421875 \n",
       "Q 6.9375 61.234375 12.546875 66.546875 \n",
       "Q 18.171875 71.875 27.484375 71.875 \n",
       "Q 36.859375 71.875 42.578125 66.421875 \n",
       "Q 48.296875 60.984375 48.296875 53.171875 \n",
       "Q 48.296875 48.1875 45.671875 44.5 \n",
       "Q 43.0625 40.828125 37.75 38.8125 \n",
       "Q 44.34375 36.671875 47.78125 31.875 \n",
       "Q 51.21875 27.09375 51.21875 20.453125 \n",
       "Q 51.21875 11.28125 44.71875 5.03125 \n",
       "Q 38.234375 -1.21875 27.640625 -1.21875 \n",
       "Q 17.046875 -1.21875 10.546875 5.046875 \n",
       "Q 4.046875 11.328125 4.046875 20.703125 \n",
       "Q 4.046875 27.6875 7.59375 32.390625 \n",
       "Q 11.140625 37.109375 17.671875 38.8125 \n",
       "z\n",
       "M 15.921875 53.71875 \n",
       "Q 15.921875 48.640625 19.1875 45.40625 \n",
       "Q 22.46875 42.1875 27.6875 42.1875 \n",
       "Q 32.765625 42.1875 36.015625 45.375 \n",
       "Q 39.265625 48.578125 39.265625 53.21875 \n",
       "Q 39.265625 58.0625 35.90625 61.359375 \n",
       "Q 32.5625 64.65625 27.59375 64.65625 \n",
       "Q 22.5625 64.65625 19.234375 61.421875 \n",
       "Q 15.921875 58.203125 15.921875 53.71875 \n",
       "z\n",
       "M 13.09375 20.65625 \n",
       "Q 13.09375 16.890625 14.875 13.375 \n",
       "Q 16.65625 9.859375 20.171875 7.921875 \n",
       "Q 23.6875 6 27.734375 6 \n",
       "Q 34.03125 6 38.125 10.046875 \n",
       "Q 42.234375 14.109375 42.234375 20.359375 \n",
       "Q 42.234375 26.703125 38.015625 30.859375 \n",
       "Q 33.796875 35.015625 27.4375 35.015625 \n",
       "Q 21.234375 35.015625 17.15625 30.90625 \n",
       "Q 13.09375 26.8125 13.09375 20.65625 \n",
       "z\n",
       "\" id=\"ArialMT-56\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path clip-path=\"url(#p4f5c776dab)\" d=\"M 31.99 13.694445 \n",
       "L 478.39 13.694445 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 1.0 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 17.631242)scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
       "       <use x=\"83.398438\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path clip-path=\"url(#p4f5c776dab)\" d=\"M 52.280909 162.867273 \n",
       "L 71.769926 20.473927 \n",
       "L 72.581969 14.612727 \n",
       "L 79.68734 15.365185 \n",
       "L 86.79271 16.33798 \n",
       "L 94.101092 17.565338 \n",
       "L 101.409473 19.018766 \n",
       "L 108.920865 20.743253 \n",
       "L 116.432257 22.695808 \n",
       "L 124.14666 24.931604 \n",
       "L 132.064073 27.460785 \n",
       "L 140.184497 30.291759 \n",
       "L 148.507932 33.430915 \n",
       "L 157.034377 36.882331 \n",
       "L 165.966843 40.737689 \n",
       "L 175.30533 45.01071 \n",
       "L 185.25285 49.810238 \n",
       "L 195.809401 55.1521 \n",
       "L 207.381005 61.260038 \n",
       "L 220.373683 68.373596 \n",
       "L 236.208509 77.307787 \n",
       "L 263.61494 93.081101 \n",
       "L 285.337074 105.46478 \n",
       "L 299.953836 113.546105 \n",
       "L 312.337483 120.14418 \n",
       "L 323.503066 125.844307 \n",
       "L 333.856606 130.8804 \n",
       "L 343.601115 135.3712 \n",
       "L 352.736591 139.338773 \n",
       "L 361.466047 142.892015 \n",
       "L 369.992492 146.122305 \n",
       "L 378.112916 148.964995 \n",
       "L 386.030329 151.506116 \n",
       "L 393.744732 153.753993 \n",
       "L 401.459135 155.768664 \n",
       "L 408.970527 157.499395 \n",
       "L 416.278908 158.95904 \n",
       "L 423.58729 160.192734 \n",
       "L 430.895671 161.196532 \n",
       "L 438.001042 161.94899 \n",
       "L 445.106413 162.478837 \n",
       "L 452.211784 162.784472 \n",
       "L 458.099091 162.867181 \n",
       "L 458.099091 162.867181 \n",
       "\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 31.99 170.28 \n",
       "L 31.99 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 478.39 170.28 \n",
       "L 478.39 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 31.99 170.28 \n",
       "L 478.39 170.28 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 31.99 7.2 \n",
       "L 478.39 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4f5c776dab\">\n",
       "   <rect height=\"163.08\" width=\"446.4\" x=\"31.99\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = nn.Parameter(torch.empty(4,4))\n",
    "optimizer = optim.Adam([p], lr=1e-3)\n",
    "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
    "\n",
    "epochs = list(range(2000))\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
    "plt.show()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "        \n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.input_net = nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim,\n",
    "                                                      dropout=self.hparams.dropout)\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads)\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        ) \n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                                  warmup=self.hparams.warmup, \n",
    "                                                  max_iters=self.hparams.max_iters)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError    \n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### Sequence to Sequence\n",
    "\n",
    "* Toy dataset: reverse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        \n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9]),\n",
       " tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
    "                         checkpoint_callback=ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                         gpus=1, \n",
    "                         max_epochs=5)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 176   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 2 K   \n",
      "3 | output_net          | Sequential         | 474   \n",
      "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b79b10edeb40ce8aff7c98b7f433fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
    "                                              model_dim=16,\n",
    "                                              num_heads=1,\n",
    "                                              num_classes=train_loader.dataset.num_categories,\n",
    "                                              num_layers=1,\n",
    "                                              dropout=0.0,\n",
    "                                              lr=5e-4,\n",
    "                                              warmup=50)\n",
    "print(reverse_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = reverse_model.get_attention_maps(inp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "    \n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set anomaly detection\n",
    "\n",
    "* Given set of images, which ones doesn't fit in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
    "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
    "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\n",
    "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
    "pretrained_model = torchvision.models.resnet34(pretrained=True)\n",
    "# Remove classification layer\n",
    "pretrained_model.fc = nn.Sequential()\n",
    "# To GPU\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Only eval\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(dataset):\n",
    "    data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "    extracted_features = []\n",
    "    for imgs, _ in tqdm(data_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = pretrained_model(imgs)\n",
    "        extracted_features.append(feats)\n",
    "    extracted_features = torch.cat(extracted_features, dim=0)\n",
    "    extracted_features = extracted_features.detach().cpu()\n",
    "    \n",
    "    # extracted_features = F.one_hot(torch.LongTensor(dataset.targets), num_classes=100).float()\n",
    "    return extracted_features\n",
    "\n",
    "train_set_feats = extract_features(train_set)\n",
    "test_feats = extract_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train+val\n",
    "labels = train_set.targets\n",
    "\n",
    "labels = torch.LongTensor(labels)\n",
    "num_labels = labels.max()+1\n",
    "sorted_indices = torch.argsort(labels).reshape(num_labels, -1)\n",
    "\n",
    "num_val_exmps = sorted_indices.shape[1] // 10\n",
    "\n",
    "val_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\n",
    "train_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n",
    "\n",
    "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
    "val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAnomalyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_feats, labels, set_size=5, train=True):\n",
    "        super().__init__()\n",
    "        self.img_feats = img_feats\n",
    "        self.labels = labels\n",
    "        self.set_size = set_size-1 # The set size is here the size of correct images\n",
    "        self.train = train\n",
    "        \n",
    "        self.num_labels = labels.max()+1\n",
    "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
    "        \n",
    "        if not train:\n",
    "            self.test_sets = self._create_test_sets()\n",
    "            \n",
    "            \n",
    "    def _create_test_sets(self):\n",
    "        test_sets = []\n",
    "        num_imgs = self.img_feats.shape[0]\n",
    "        np.random.seed(42)\n",
    "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
    "        test_sets = torch.stack(test_sets, dim=0)\n",
    "        return test_sets\n",
    "            \n",
    "        \n",
    "    def sample_img_set(self, anomaly_label):\n",
    "        set_label = np.random.randint(self.num_labels-1)\n",
    "        if set_label >= anomaly_label:\n",
    "            set_label += 1\n",
    "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
    "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
    "        return img_indices\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_feats.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anomaly = self.img_feats[idx]\n",
    "        if self.train:\n",
    "            img_indices = self.sample_img_set(self.labels[idx])\n",
    "        else:\n",
    "            img_indices = self.test_sets[idx]\n",
    "            \n",
    "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
    "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
    "        label = img_set.shape[0]-1\n",
    "        return img_set, indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_SIZE = 6\n",
    "test_labels = torch.LongTensor(test_set.targets)\n",
    "\n",
    "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
    "val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
    "test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
    "\n",
    "train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=4, pin_memory=True)\n",
    "val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_exmp(indices, orig_dataset):\n",
    "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
    "    \n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Anomaly examples on CIFAR100\")\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "_, indices, _ = next(iter(test_anom_loader))\n",
    "visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyPredictor(TransformerPredictor):\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        img_sets, _, labels = batch\n",
    "        preds = self.forward(img_sets, add_positional_encoding=False)\n",
    "        preds = preds.squeeze(dim=-1)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
    "                         checkpoint_callback=ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                         gpus=1, \n",
    "                         max_epochs=80,\n",
    "                        callbacks=[LearningRateMonitor(\"step\")])\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = AnomalyPredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
    "        trainer.fit(model, train_anom_loader, val_anom_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, test_dataloaders=val_anom_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_anom_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_model, anomaly_result = train_anomaly(input_dim=train_anom_dataset.img_feats.shape[-1],\n",
    "                                              model_dim=256,\n",
    "                                              num_heads=4,\n",
    "                                              num_classes=1,\n",
    "                                              num_layers=6,\n",
    "                                              dropout=0.1,\n",
    "                                              lr=4e-4,\n",
    "                                              warmup=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, indices, labels = next(iter(test_anom_loader))\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
