{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 17: Self-Supervised Contrastive Learning with SimCLR\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=In%20progress&color=red)\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb)  \n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial17)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1BmisSPs5BXQKpolyHp5X4klSAhpDx479?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will take a closer look at self-supervised contrastive learning. Self-supervised learning, or also sometimes called unsupervised learning, describes the scenario where we have given input data, but no accompanying labels to train in a classical supervised way. However, this data still contains a lot of information from which we can learn: how are the images different from each other? What patterns are descriptive for certain images? Can we cluster the images? And so on. Methods for self-supervised learning try to learn as much as possible from the data alone, so it can quickly be finetuned for a specific classification task. \n",
    "The benefit of self-supervised learning is that large dataset can often easily be obtained. For instance, if we want to train a vision model on semantic segmentation for autonomous driving, we can collect large amounts of data by simply installing a camera in a car, and driving through a city for an hour. In contrast, if we would want to do supervised learning, we would have to manually label all those images before training a model. This is extremely expensive, and would likely take a couple of months to manually label the same amount of data. Futher, self-supervised learning can provide an alternative to transfer learning from ImageNet pretrainings since we could pretrain a model on a specific dataset/situation, e.g. traffic scenarios for autonomous driving.\n",
    "\n",
    "Within the last two years, a lot of new approaches have been proposed for self-supervised learning, in particular for images, that have resulted in great improvements over supervised models when few labels are available. The subfield that we will focus on in this tutorial are contrastive learning methods. Contrastive learning is motivated by the question mentioned above: how are images different from each other? Specifically, contrastive learning methods train a model to cluster an image and its slightly augmented version in latent space, while the distance to other images should be maximized. A very recent and simple method for this is [SimCLR](https://arxiv.org/abs/2006.10029), which is visualized below (figure credit - [Ting Chen et al.](https://simclr.github.io/)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"simclr_contrastive_learning.png\" width=\"500px\"></center>\n",
    "\n",
    "The general setup is that we are given a dataset of images without any labels, and want to train a model on this data such that it can quickly adapt to any image recognition task afterward. During each training iteration, we sample a batch of images as usual. For each image, we create two version by applying data augmentation techniques like cropping, gaussian noise, blurring, etc. An example of such is shown on the left with the image of the dog. We will go into the details and effect of the chosen augmentation techniques later. On those images, we apply a CNN like ResNet, and obtain as output a 1D feature vector on which we apply a small MLP. The output features of the two augmented images are then trained to be close to each other, while all other images in that batch should be as different as possible. This way, the model has to learn to recognize the content of the image that remains unchanged under the data augmentations, such as objects which we usually care about in supervised tasks.\n",
    "\n",
    "We will now implement this framework ourselves and discuss further details along the way. Let's first start with importing our standard libraries below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of workers: 16\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install pytorch-lightning==1.3.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial17\"\n",
    "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
    "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in many tutorials before, we provide pre-trained models. Note that those models are slightly larger as normal (~100MB overall) since we use the default ResNet-50 architecture. If you are running this notebook locally, make sure to have sufficient disk space available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"SimCLR.ckpt\", \"LogisticRegression.ckpt\", \"ResNet.ckpt\",\n",
    "                    \"tensorboards/SimCLR/events.out.tfevents.SimCLR\",\n",
    "                    \"tensorboards/classification/LogisticRegression/events.out.tfevents.LogisticRegression\",\n",
    "                    \"tensorboards/classification/ResNet/events.out.tfevents.ResNet\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations(object):\n",
    "    \n",
    "    def __init__(self, base_transforms, n_views=2):\n",
    "        self.base_transforms = base_transforms\n",
    "        self.n_views = n_views\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html))\n",
    "\n",
    "<center width=\"100%\"><img src=\"simclr_data_augmentations.png\" width=\"800px\"></center>\n",
    "\n",
    "(figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html))\n",
    "\n",
    "<center width=\"100%\"><img src=\"crop_views.svg\" width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.RandomResizedCrop(size=96),\n",
    "                                          transforms.RandomApply([\n",
    "                                              transforms.ColorJitter(brightness=0.5, \n",
    "                                                                     contrast=0.5, \n",
    "                                                                     saturation=0.5, \n",
    "                                                                     hue=0.1)\n",
    "                                          ], p=0.8),\n",
    "                                          transforms.RandomGrayscale(p=0.2),\n",
    "                                          transforms.GaussianBlur(kernel_size=9),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5,), (0.5,))\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = STL10(root=DATASET_PATH, split='unlabeled', download=True, \n",
    "                       transform=ContrastiveTransformations(contrast_transforms, n_views=2))\n",
    "train_data_contrast = STL10(root=DATASET_PATH, split='train', download=True, \n",
    "                            transform=ContrastiveTransformations(contrast_transforms, n_views=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "pl.seed_everything(42)\n",
    "NUM_IMAGES = 8\n",
    "imgs = torch.stack([img for idx in range(NUM_IMAGES) for img in unlabeled_data[idx][0]], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(imgs, nrow=NUM_IMAGES//2, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title('Augmented image examples of the STL10 dataset')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimCLR implementation\n",
    "\n",
    "<center width=\"100%\"><img src=\"simclr_network_setup.svg\" width=\"350px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False, num_classes=hidden_dim)\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc,  # Linear(ResNet output, hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        \n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        imgs, _ = batch\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        num_imgs = imgs.shape[0]\n",
    "        \n",
    "        feats = self.convnet(imgs)\n",
    "        \n",
    "        # TODO: SIMPLIFY TO SET SELF TO -9e15, and extract pos afterwards\n",
    "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
    "        pos_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        pos_mask = pos_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        neg_mask = ~pos_mask\n",
    "        neg_mask.fill_diagonal_(False)\n",
    "\n",
    "        pos_sim = cos_sim[pos_mask].view(num_imgs, 1)\n",
    "        neg_sim = cos_sim[neg_mask].view(num_imgs, -1)\n",
    "        comb_sim = torch.cat([pos_sim, neg_sim], dim=-1)\n",
    "        nll = -F.log_softmax(comb_sim / self.hparams.temperature, dim=-1)[:,0]\n",
    "        nll = nll.mean()\n",
    "        \n",
    "        self.log(mode+'_loss', nll)\n",
    "\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "        \n",
    "        return nll\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
    "                         gpus=1 if str(device)=='cuda:0' else 0,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch')],\n",
    "                         progress_bar_refresh_rate=1)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = SimCLR.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        train_loader = data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True, \n",
    "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        val_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False, \n",
    "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model = train_simclr(batch_size=256, \n",
    "                            hidden_dim=128, \n",
    "                            lr=5e-4, \n",
    "                            temperature=0.07, \n",
    "                            weight_decay=1e-4, \n",
    "                            max_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboards/SimCLR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                      milestones=[60, 80], \n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        \n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples: 5000\n"
     ]
    }
   ],
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_img_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
    "                       transform=img_transforms)\n",
    "val_img_data = STL10(root=DATASET_PATH, split='test', download=True,\n",
    "                     transform=img_transforms)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataset, reset_fc=True):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "    \n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    return data.TensorDataset(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(batch_size, train_feats_data, val_feats_data, model_name, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression_\"+model_name),\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=0,\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True, \n",
    "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = data.DataLoader(val_feats_data, batch_size=batch_size, shuffle=False, \n",
    "                                 drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"LogisticRegression_%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducable\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.test(model, test_dataloaders=train_loader, verbose=False)\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "        \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_simclr = prepare_data_features(simclr_model, train_img_data)\n",
    "val_feats_simclr = prepare_data_features(simclr_model, val_img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_simclr_model, logreg_simclr_result = train_logreg(batch_size=64,\n",
    "                                                         train_feats_data=train_feats_simclr,\n",
    "                                                         val_feats_data=val_feats_simclr,\n",
    "                                                         model_name='SimCLR',\n",
    "                                                         feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
    "                                                         num_classes=10,\n",
    "                                                         lr=1e-3,\n",
    "                                                         weight_decay=1e-3)\n",
    "print('SimCLR logistic regression:', logreg_simclr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Standard ResNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = torchvision.models.resnet18(\n",
    "            pretrained=False, num_classes=num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                      milestones=[70, 90],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomResizedCrop(size=96, scale=(0.8,1.0)),\n",
    "                                       transforms.RandomGrayscale(p=0.2),\n",
    "                                       transforms.GaussianBlur(kernel_size=9, sigma=(0.1,0.5)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ])\n",
    "\n",
    "train_img_aug_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
    "                           transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(batch_size, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ResNet\"),\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         check_val_every_n_epoch=2)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = data.DataLoader(train_img_aug_data, batch_size=batch_size, shuffle=True, \n",
    "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = data.DataLoader(val_img_data, batch_size=batch_size, shuffle=False, \n",
    "                                 drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ResNet.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = ResNet.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ResNet(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ResNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation set\n",
    "    train_result = trainer.test(model, test_dataloaders=train_loader, verbose=False)\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "        \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model, resnet_result = train_resnet(batch_size=64,\n",
    "                                           num_classes=10,\n",
    "                                           lr=1e-3,\n",
    "                                           weight_decay=2e-4,\n",
    "                                           max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboards/classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., & Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. ICML 2020 ([link](https://arxiv.org/abs/2006.10029))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap your own latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hidden_dim, beta, lr, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False, num_classes=hidden_dim)\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc,\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2*hidden_dim),\n",
    "            nn.BatchNorm1d(2*hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.target_encoder = None\n",
    "        \n",
    "    def _init_target_network(self):\n",
    "        self.target_encoder = deepcopy(self.convnet)\n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.target_encoder.eval()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _update_target_network(self):\n",
    "        for target_param, online_param in zip(self.target_encoder.parameters(), self.convnet.parameters()):\n",
    "            target_param.data.mul_(self.hparams.beta).add_((1-self.hparams.beta)*online_param.data)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def feats_loss(self, feat1, feat2):\n",
    "        feat1 = F.normalize(feat1, dim=-1, p=2)\n",
    "        feat2 = F.normalize(feat2, dim=-1, p=2)\n",
    "        loss = 2 - 2 * (feat1 * feat2).sum(dim=-1)\n",
    "        return loss\n",
    "        \n",
    "    def byol_loss(self, batch, mode='train'):\n",
    "        assert self.target_encoder, 'The target encoder has to be initialized before training.'\n",
    "        imgs, _ = batch\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        num_imgs = imgs.shape[0]\n",
    "        \n",
    "        online_feats = self.convnet(imgs)\n",
    "        online_feats = self.predictor(online_feats)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_feats = self.target_encoder(imgs).detach()\n",
    "            \n",
    "        online_feats1, online_feats2 = online_feats.chunk(2, dim=0)\n",
    "        target_feats1, target_feats2 = target_feats.chunk(2, dim=0)\n",
    "        \n",
    "        loss = self.feats_loss(online_feats1, target_feats2) + \\\n",
    "               self.feats_loss(online_feats2, target_feats1)\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        self.log(mode+'_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.byol_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.byol_loss(batch, mode='val')\n",
    "        \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self._update_target_network()\n",
    "        \n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        state_dict = super().state_dict(*args, **kwargs)\n",
    "        state_dict = {k: state_dict[k] for k in state_dict if not k.startswith('target_encoder')}\n",
    "        return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_byol(batch_size, max_epochs=500, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"BYOL\"),\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"BYOL.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = BYOL.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        train_loader = data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True, \n",
    "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        val_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False, \n",
    "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = BYOL(max_epochs=max_epochs, **kwargs)\n",
    "        model._init_target_network()\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = BYOL.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at ../saved_models/tutorial17/BYOL.ckpt, loading...\n"
     ]
    }
   ],
   "source": [
    "byol_model = train_byol(batch_size=256,\n",
    "                        hidden_dim=128,\n",
    "                        lr=5e-4,\n",
    "                        beta=0.99,\n",
    "                        weight_decay=1e-4,\n",
    "                        max_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog post about BatchNorm - BYOL relation: https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46130317a21a41a084e2990b97b9982d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418e6186496d403397ebc4d2e60fff7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_feats_byol = prepare_data_features(byol_model, train_img_data)\n",
    "val_feats_byol = prepare_data_features(byol_model, val_img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Linear | 5.1 K \n",
      "---------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYOL logitistc regression: {'train': 0.7249599099159241, 'val': 0.6965000033378601}\n"
     ]
    }
   ],
   "source": [
    "logreg_byol_model, logreg_byol_result = train_logreg(batch_size=64,\n",
    "                                                     train_feats_data=train_feats_byol,\n",
    "                                                     val_feats_data=val_feats_byol,\n",
    "                                                     model_name='BYOL',\n",
    "                                                     feature_dim=train_feats_byol.tensors[0].shape[1],\n",
    "                                                     num_classes=10,\n",
    "                                                     lr=1e-3,\n",
    "                                                     weight_decay=1e-3)\n",
    "print('BYOL logitistc regression:', logreg_byol_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
