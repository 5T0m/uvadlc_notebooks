{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 18 (JAX): Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random as jrandom\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial18_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gym\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet gym[all]\n",
    "    import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# env with preprocessing\n",
    "env = gym.make('PongNoFrameskip-v4', \n",
    "               # 'ALE/SpaceInvaders-v5', \n",
    "               new_step_api=True, \n",
    "               full_action_space=False,\n",
    "               frameskip=1\n",
    "              )\n",
    "env = gym.wrappers.AtariPreprocessing(env, \n",
    "                                      new_step_api=True,\n",
    "                                      grayscale_obs=False,\n",
    "                                      )\n",
    "env = gym.wrappers.FrameStack(env, 2, new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity=15000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = list()\n",
    "        \n",
    "    def add(self, s, s_next, action, reward):\n",
    "        while len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((s, s_next, action, reward))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        transitions = tuple(np.stack(t, axis=0) for t in zip(*transitions))\n",
    "        return transitions\n",
    "    \n",
    "    def avg_reward(self):\n",
    "        rewards = [b[3] for b in self.buffer]\n",
    "        return sum(rewards) / len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ExperienceReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(apply_fn, params, imgs):\n",
    "    return apply_fn(params, imgs)\n",
    "\n",
    "@jax.jit\n",
    "def sample_action(state, imgs, rng, eps=0.0):\n",
    "    q_vals = run_model(state.apply_fn, state.params, imgs)\n",
    "    q_argmax = q_vals.argmax(axis=-1)\n",
    "#     if eps == 0.0:\n",
    "#         return q_argmax\n",
    "#     else:\n",
    "    probs = jax.nn.one_hot(q_argmax, num_classes=q_vals.shape[-1]) * (1 - eps) + eps / q_vals.shape[-1]\n",
    "    actions = jrandom.categorical(rng, jnp.log(jnp.maximum(probs, 1e-10)), axis=-1)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(state, buffer, eps=0.0):\n",
    "    s = env.reset()\n",
    "    rng = state.rng\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        rng, step_rng = jrandom.split(rng)\n",
    "        a = sample_action(state, np.array(s)[None], rng, eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, terminated, truncated, info = env.step(a)\n",
    "        buffer.add(s, s_next, a, r)  #  / 5.\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "        s = s_next\n",
    "    state = state.replace(rng=rng)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    num_actions : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32) / 255. * 2. - 1.\n",
    "        x = jnp.concatenate([x[:,1] - x[:,0], x[:,1]], axis=-1)\n",
    "        \n",
    "        x = nn.Conv(16, kernel_size=(7, 7), strides=(4, 4))(x)  # 84 => 21\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(32, kernel_size=(5, 5), strides=(2, 2))(x)  # 21 => 10\n",
    "        x = nn.relu(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_actions,\n",
    "                     kernel_init=nn.initializers.zeros)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_actions=env.action_space.n)\n",
    "s = env.reset()\n",
    "params = model.init(jrandom.PRNGKey(0), np.array(s)[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # You can further extend the TrainState by any additional part here\n",
    "    # For example, rng to keep for init, dropout, etc.\n",
    "    rng : Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainState.create(apply_fn=model.apply,\n",
    "                          params=params,\n",
    "                          rng=jrandom.PRNGKey(42),\n",
    "                          tx=optax.adam(3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = run_episode(state, buffer, eps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_mse(y_true, y_pred):\n",
    "    diff = jnp.abs(y_true - y_pred)\n",
    "    return jnp.where(diff > 1.0, diff, diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def train_step(state, state_target, batch):\n",
    "    s, s_next, action, reward = batch\n",
    "    def loss_fn(params):\n",
    "        q_current = run_model(state.apply_fn, params, s)[np.arange(action.shape[0]), \n",
    "                                                         action.astype(np.int32)]\n",
    "        q_next = run_model(state_target.apply_fn, state_target.params, s_next)\n",
    "        q_target = reward + q_next.max(axis=-1)\n",
    "        loss = clipped_mse(q_target, q_current)\n",
    "        return loss.mean()\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0234375\n"
     ]
    }
   ],
   "source": [
    "_, loss = train_step(state, state, buffer.sample(128))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.sample(128)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1984dab5ec64d9dbd1df7d17e6b7038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Loss: 0.031, Avg reward: -0.02\n",
      "[Episode 10] Loss: 0.015, Avg reward: -0.02\n",
      "[Episode 20] Loss: 0.020, Avg reward: -0.03\n",
      "[Episode 30] Loss: 0.004, Avg reward: -0.03\n",
      "[Episode 40] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 50] Loss: 0.025, Avg reward: -0.02\n",
      "[Episode 60] Loss: 0.006, Avg reward: -0.03\n",
      "[Episode 70] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 80] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 90] Loss: 0.000, Avg reward: -0.02\n",
      "[Episode 100] Loss: 0.042, Avg reward: -0.02\n",
      "[Episode 110] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 120] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 130] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 140] Loss: 0.000, Avg reward: -0.03\n",
      "[Episode 150] Loss: 0.015, Avg reward: -0.03\n",
      "[Episode 160] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 170] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 180] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 190] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 200] Loss: 0.031, Avg reward: -0.02\n",
      "[Episode 210] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 220] Loss: 0.009, Avg reward: -0.02\n",
      "[Episode 230] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 240] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 250] Loss: 0.022, Avg reward: -0.02\n",
      "[Episode 260] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 270] Loss: 0.002, Avg reward: -0.03\n",
      "[Episode 280] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 290] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 300] Loss: 0.013, Avg reward: -0.02\n",
      "[Episode 310] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 320] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 330] Loss: 0.000, Avg reward: -0.03\n",
      "[Episode 340] Loss: 0.002, Avg reward: -0.03\n",
      "[Episode 350] Loss: 0.013, Avg reward: -0.03\n",
      "[Episode 360] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 370] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 380] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 390] Loss: 0.000, Avg reward: -0.03\n",
      "[Episode 400] Loss: 0.021, Avg reward: -0.03\n",
      "[Episode 410] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 420] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 430] Loss: 0.002, Avg reward: -0.03\n",
      "[Episode 440] Loss: 0.001, Avg reward: -0.03\n",
      "[Episode 450] Loss: 0.010, Avg reward: -0.03\n",
      "[Episode 460] Loss: 0.011, Avg reward: -0.02\n",
      "[Episode 470] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 480] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 490] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 500] Loss: 0.016, Avg reward: -0.02\n",
      "[Episode 510] Loss: 0.010, Avg reward: -0.03\n",
      "[Episode 520] Loss: 0.002, Avg reward: -0.03\n",
      "[Episode 530] Loss: 0.002, Avg reward: -0.03\n",
      "[Episode 540] Loss: 0.004, Avg reward: -0.03\n",
      "[Episode 550] Loss: 0.018, Avg reward: -0.02\n",
      "[Episode 560] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 570] Loss: 0.001, Avg reward: -0.03\n",
      "[Episode 580] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 590] Loss: 0.005, Avg reward: -0.02\n",
      "[Episode 600] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 610] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 620] Loss: 0.001, Avg reward: -0.02\n",
      "[Episode 630] Loss: 0.001, Avg reward: -0.03\n",
      "[Episode 640] Loss: 0.003, Avg reward: -0.03\n",
      "[Episode 650] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 660] Loss: 0.011, Avg reward: -0.03\n",
      "[Episode 670] Loss: 0.005, Avg reward: -0.02\n",
      "[Episode 680] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 690] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 700] Loss: 0.009, Avg reward: -0.02\n",
      "[Episode 710] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 720] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 730] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 740] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 750] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 760] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 770] Loss: 0.008, Avg reward: -0.02\n",
      "[Episode 780] Loss: 0.002, Avg reward: -0.02\n",
      "[Episode 790] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 800] Loss: 0.017, Avg reward: -0.02\n",
      "[Episode 810] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 820] Loss: 0.003, Avg reward: -0.02\n",
      "[Episode 830] Loss: 0.005, Avg reward: -0.02\n",
      "[Episode 840] Loss: 0.005, Avg reward: -0.02\n",
      "[Episode 850] Loss: 0.016, Avg reward: -0.02\n",
      "[Episode 860] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 870] Loss: 0.008, Avg reward: -0.02\n",
      "[Episode 880] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 890] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 900] Loss: 0.013, Avg reward: -0.02\n",
      "[Episode 910] Loss: 0.005, Avg reward: -0.02\n",
      "[Episode 920] Loss: 0.004, Avg reward: -0.02\n",
      "[Episode 930] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 940] Loss: 0.006, Avg reward: -0.02\n",
      "[Episode 950] Loss: 0.010, Avg reward: -0.02\n",
      "[Episode 960] Loss: 0.011, Avg reward: -0.02\n",
      "[Episode 970] Loss: 0.022, Avg reward: -0.02\n",
      "[Episode 980] Loss: 0.019, Avg reward: -0.02\n",
      "[Episode 990] Loss: 0.017, Avg reward: -0.02\n",
      "[Episode 1000] Loss: 0.018, Avg reward: -0.02\n",
      "[Episode 1010] Loss: 0.017, Avg reward: -0.02\n",
      "[Episode 1020] Loss: 0.013, Avg reward: -0.02\n",
      "[Episode 1030] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 1040] Loss: 0.012, Avg reward: -0.02\n",
      "[Episode 1050] Loss: 0.009, Avg reward: -0.02\n",
      "[Episode 1060] Loss: 0.019, Avg reward: -0.02\n",
      "[Episode 1070] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1080] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1090] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1100] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1110] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1120] Loss: 0.012, Avg reward: -0.02\n",
      "[Episode 1130] Loss: 0.013, Avg reward: -0.02\n",
      "[Episode 1140] Loss: 0.015, Avg reward: -0.02\n",
      "[Episode 1150] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 1160] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1170] Loss: 0.015, Avg reward: -0.01\n",
      "[Episode 1180] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 1190] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 1200] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1210] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1220] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 1230] Loss: 0.006, Avg reward: -0.01\n",
      "[Episode 1240] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 1250] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1260] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1270] Loss: 0.007, Avg reward: -0.02\n",
      "[Episode 1280] Loss: 0.006, Avg reward: -0.01\n",
      "[Episode 1290] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1300] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1310] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1320] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 1330] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1340] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 1350] Loss: 0.015, Avg reward: -0.01\n",
      "[Episode 1360] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1370] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 1380] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 1390] Loss: 0.006, Avg reward: -0.01\n",
      "[Episode 1400] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1410] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1420] Loss: 0.013, Avg reward: -0.02\n",
      "[Episode 1430] Loss: 0.006, Avg reward: -0.01\n",
      "[Episode 1440] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1450] Loss: 0.017, Avg reward: -0.01\n",
      "[Episode 1460] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1470] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 1480] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 1490] Loss: 0.005, Avg reward: -0.01\n",
      "[Episode 1500] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1510] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1520] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1530] Loss: 0.005, Avg reward: -0.01\n",
      "[Episode 1540] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1550] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1560] Loss: 0.021, Avg reward: -0.01\n",
      "[Episode 1570] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1580] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1590] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1600] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1610] Loss: 0.026, Avg reward: -0.01\n",
      "[Episode 1620] Loss: 0.020, Avg reward: -0.01\n",
      "[Episode 1630] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 1640] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1650] Loss: 0.023, Avg reward: -0.01\n",
      "[Episode 1660] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1670] Loss: 0.017, Avg reward: -0.01\n",
      "[Episode 1680] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1690] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 1700] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 1710] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 1720] Loss: 0.019, Avg reward: -0.01\n",
      "[Episode 1730] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1740] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1750] Loss: 0.031, Avg reward: -0.01\n",
      "[Episode 1760] Loss: 0.020, Avg reward: -0.01\n",
      "[Episode 1770] Loss: 0.019, Avg reward: -0.01\n",
      "[Episode 1780] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1790] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1800] Loss: 0.020, Avg reward: -0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 1810] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1820] Loss: 0.019, Avg reward: -0.01\n",
      "[Episode 1830] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1840] Loss: 0.015, Avg reward: -0.01\n",
      "[Episode 1850] Loss: 0.021, Avg reward: -0.01\n",
      "[Episode 1860] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 1870] Loss: 0.015, Avg reward: -0.01\n",
      "[Episode 1880] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1890] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1900] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 1910] Loss: 0.032, Avg reward: -0.01\n",
      "[Episode 1920] Loss: 0.024, Avg reward: -0.01\n",
      "[Episode 1930] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 1940] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 1950] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 1960] Loss: 0.023, Avg reward: -0.01\n",
      "[Episode 1970] Loss: 0.016, Avg reward: -0.00\n",
      "[Episode 1980] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 1990] Loss: 0.022, Avg reward: -0.01\n",
      "[Episode 2000] Loss: 0.029, Avg reward: -0.01\n",
      "[Episode 2010] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 2020] Loss: 0.019, Avg reward: -0.01\n",
      "[Episode 2030] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 2040] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 2050] Loss: 0.017, Avg reward: -0.01\n",
      "[Episode 2060] Loss: 0.012, Avg reward: -0.00\n",
      "[Episode 2070] Loss: 0.008, Avg reward: -0.01\n",
      "[Episode 2080] Loss: 0.018, Avg reward: -0.01\n",
      "[Episode 2090] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 2100] Loss: 0.036, Avg reward: -0.01\n",
      "[Episode 2110] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 2120] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 2130] Loss: 0.011, Avg reward: -0.01\n",
      "[Episode 2140] Loss: 0.011, Avg reward: -0.00\n",
      "[Episode 2150] Loss: 0.027, Avg reward: -0.01\n",
      "[Episode 2160] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 2170] Loss: 0.008, Avg reward: -0.01\n",
      "[Episode 2180] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 2190] Loss: 0.020, Avg reward: -0.01\n",
      "[Episode 2200] Loss: 0.023, Avg reward: -0.01\n",
      "[Episode 2210] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 2220] Loss: 0.007, Avg reward: -0.01\n",
      "[Episode 2230] Loss: 0.016, Avg reward: -0.01\n",
      "[Episode 2240] Loss: 0.008, Avg reward: -0.00\n",
      "[Episode 2250] Loss: 0.021, Avg reward: -0.01\n",
      "[Episode 2260] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 2270] Loss: 0.014, Avg reward: -0.00\n",
      "[Episode 2280] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 2290] Loss: 0.020, Avg reward: -0.00\n",
      "[Episode 2300] Loss: 0.016, Avg reward: -0.00\n",
      "[Episode 2310] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 2320] Loss: 0.013, Avg reward: -0.01\n",
      "[Episode 2330] Loss: 0.023, Avg reward: -0.00\n",
      "[Episode 2340] Loss: 0.026, Avg reward: -0.01\n",
      "[Episode 2350] Loss: 0.018, Avg reward: -0.00\n",
      "[Episode 2360] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 2370] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 2380] Loss: 0.008, Avg reward: -0.01\n",
      "[Episode 2390] Loss: 0.010, Avg reward: -0.00\n",
      "[Episode 2400] Loss: 0.015, Avg reward: -0.01\n",
      "[Episode 2410] Loss: 0.010, Avg reward: -0.01\n",
      "[Episode 2420] Loss: 0.012, Avg reward: -0.01\n",
      "[Episode 2430] Loss: 0.020, Avg reward: -0.01\n",
      "[Episode 2440] Loss: 0.014, Avg reward: -0.01\n",
      "[Episode 2450] Loss: 0.019, Avg reward: -0.01\n",
      "[Episode 2460] Loss: 0.108, Avg reward: -0.00\n",
      "[Episode 2470] Loss: 0.020, Avg reward: -0.01\n",
      "[Episode 2480] Loss: 0.009, Avg reward: -0.01\n",
      "[Episode 2490] Loss: 0.012, Avg reward: -0.00\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2500\n",
    "for episode_idx in tqdm(range(num_episodes)):\n",
    "    if episode_idx % 50 == 0:\n",
    "        state_target = state\n",
    "    state = run_episode(state, buffer, eps=0.5 * (1.0 - episode_idx / num_episodes))\n",
    "    for _ in range(5):\n",
    "        state, loss = train_step(state, state_target, buffer.sample(128))\n",
    "    if episode_idx % 10 == 0:\n",
    "        print(f'[Episode {episode_idx}] Loss: {loss.item():5.3f}, Avg reward: {buffer.avg_reward():3.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
