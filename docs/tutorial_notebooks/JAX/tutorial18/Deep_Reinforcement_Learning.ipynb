{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 18 (JAX): Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n",
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable\n",
    "from types import SimpleNamespace\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial18_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gym\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet gym[all]\n",
    "    import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_to_np_rng(rng):\n",
    "    return random.randint(rng, shape=(1,), minval=0, maxval=int(1e6), dtype=np.int64).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, grid_size = 5):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.spec = SimpleNamespace(max_episode_steps=100)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        if isinstance(seed, jnp.ndarray):\n",
    "            seed = jax_to_np_rng(seed)\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self.pos = self.rng.integers(0, high=self.grid_size, size=(2,))\n",
    "        self.goal = None\n",
    "        while self.goal is None or self._is_on_goal():\n",
    "            self.goal = self.rng.integers(0, high=self.grid_size, size=(2,))\n",
    "        self.grid = np.ones((self.grid_size, self.grid_size, 3), dtype=np.uint8) * 255\n",
    "        return self._render_img()\n",
    "    \n",
    "    def _render_img(self):\n",
    "        grid = np.copy(self.grid)\n",
    "        grid[self.pos[0], self.pos[1], 1:] = 0\n",
    "        grid[self.goal[0], self.goal[1], ::2] = 0\n",
    "        return grid\n",
    "    \n",
    "    def _is_on_goal(self):\n",
    "        return all([self.pos[i] == self.goal[i] for i in range(2)])\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0: # Up\n",
    "            self.pos[0] = max(0, self.pos[0] - 1)\n",
    "        elif action == 1: # Right\n",
    "            self.pos[1] = min(self.grid_size - 1, self.pos[1] + 1)\n",
    "        elif action == 2: # Down\n",
    "            self.pos[0] = min(self.grid_size - 1, self.pos[0] + 1)\n",
    "        elif action == 3: # Left\n",
    "            self.pos[1] = max(0, self.pos[1] - 1)\n",
    "        \n",
    "        done = self._is_on_goal()\n",
    "        reward = 1 if done else -0.1\n",
    "        state = self._render_img()\n",
    "        \n",
    "        return state, reward, done, False, {}\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412753"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_to_np_rng(random.PRNGKey(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationsToNumpy(gym.ObservationWrapper):\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return np.array(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    # # env with preprocessing\n",
    "    env = gym.make('PongNoFrameskip-v4', \n",
    "                   # 'ALE/SpaceInvaders-v5', \n",
    "                   # new_step_api=True, \n",
    "                   full_action_space=False,\n",
    "                   frameskip=1\n",
    "                  )\n",
    "    env = gym.wrappers.AtariPreprocessing(env, \n",
    "                                          # new_step_api=True,\n",
    "                                          grayscale_newaxis=True)\n",
    "    env = gym.wrappers.FrameStack(env, 3, # new_step_api=True\n",
    "                                 )\n",
    "    env = ObservationsToNumpy(env)\n",
    "    env.spec.max_episode_steps = 5000\n",
    "    # env = gym.wrappers.TransformReward(env, lambda r: r / 5.)\n",
    "    return env\n",
    "# env = MazeEnv(grid_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()\n",
    "eval_env = create_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32) / 255. * 2. - 1.\n",
    "        if len(x.shape) >= 5:\n",
    "            x = jnp.concatenate([x[:,2] - 2 * x[:,1] + x[:,0],\n",
    "                                 x[:,2] - x[:,1], \n",
    "                                 x[:,2]], \n",
    "                                axis=-1)\n",
    "        \n",
    "        if True:\n",
    "            x = nn.Conv(32, kernel_size=(8, 8), strides=(4, 4))(x)  # 84 => 21\n",
    "            x = nn.relu(x)\n",
    "            x = nn.Conv(64, kernel_size=(4, 4), strides=(2, 2))(x)  # 21 => 10\n",
    "            x = nn.relu(x)\n",
    "            x = nn.Conv(64, kernel_size=(3, 3), strides=(2, 2))(x)  # 21 => 10\n",
    "            x = nn.relu(x)\n",
    "        else:\n",
    "            x = nn.Conv(16, kernel_size=(3, 3), strides=(1, 1))(x) \n",
    "            x = nn.relu(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    num_actions : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = Backbone()(x)\n",
    "        x = nn.Dense(1024)(x)\n",
    "        x = nn.relu(x)\n",
    "        q = nn.Dense(self.num_actions,\n",
    "                     kernel_init=nn.initializers.zeros)(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity=50000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = list()\n",
    "        self.probs = list()\n",
    "        \n",
    "    def add(self, s, s_next, action, reward, done, prob=1.0):\n",
    "        while len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "            self.probs.pop(0)\n",
    "        self.buffer.append((s, s_next, action, reward, done))\n",
    "        self.probs.append(prob)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        probs = np.array(self.probs)\n",
    "        probs = probs / probs.sum()\n",
    "        trans_idxs = np.random.choice(len(self.buffer), \n",
    "                                      batch_size, \n",
    "                                      replace=len(self.buffer) <= batch_size,\n",
    "                                      p=probs)\n",
    "        transitions = [self.buffer[idx] for idx in trans_idxs]\n",
    "        transitions = tuple(np.stack(t, axis=0) for t in zip(*transitions))\n",
    "        return transitions\n",
    "    \n",
    "    def avg_reward(self):\n",
    "        rewards = [b[3] for b in self.buffer]\n",
    "        return mean(rewards)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.probs.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sample_action(state, inps, rng, eps=0.0):\n",
    "    if len(inps.shape) == 2:\n",
    "        q_vals = inps\n",
    "    else:\n",
    "        q_vals = state.apply_fn(state.params, inps)\n",
    "    # Epsilon-greedy policy\n",
    "    q_argmax = q_vals.argmax(axis=-1)\n",
    "    probs = jax.nn.one_hot(q_argmax, num_classes=q_vals.shape[-1]) * (1 - eps) + eps / q_vals.shape[-1]\n",
    "    actions = random.categorical(rng, jnp.log(jnp.maximum(probs, 1e-10)), axis=-1)\n",
    "    # q-value of selected action\n",
    "    act_q_vals = q_vals[jnp.arange(actions.shape[0]), actions]\n",
    "    return actions, act_q_vals\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, params, imgs):\n",
    "    return state.apply_fn(params, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_error_func(reward, q_vals, target_q, done, gamma):\n",
    "    target = reward + (1 - done) * gamma * target_q.max(axis=-1)\n",
    "    error = target - q_vals\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_mse(error):\n",
    "    diff = jnp.abs(error)\n",
    "    return jnp.where(diff > 1.0, diff, diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_episode(state, env, seed, eps=0.0):\n",
    "    env.seed(seed)\n",
    "    s, _ = env.reset() # seed)\n",
    "    rng = random.PRNGKey(seed)\n",
    "    rew_return = 0.0\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        rng, step_rng = random.split(rng)\n",
    "        a, _ = sample_action(state, s[None], step_rng, eps=eps)\n",
    "        a = a.item()\n",
    "        s, r, terminated, truncated, _ = env.step(a)\n",
    "        rew_return += r\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            return (True, t, rew_return)\n",
    "    return (False, t, rew_return)\n",
    "\n",
    "def eval_policy(state, env, num_seeds=100):\n",
    "    start_time = time.time()\n",
    "    wins, steps, returns = zip(*[eval_episode(state, env, seed=i, eps=0.0) for i in range(num_seeds)])\n",
    "    win_prop = mean(wins)\n",
    "    avg_steps = mean(steps)\n",
    "    avg_return = mean(returns)\n",
    "    unsolved = [i for i in range(len(wins)) if not wins[i]]\n",
    "    end_time = time.time()\n",
    "    return {'wins': win_prop, \n",
    "            'steps': avg_steps, \n",
    "            'returns': avg_return, \n",
    "            'unsolved': unsolved, \n",
    "            'eval_time': end_time - start_time,\n",
    "            'all_returns': returns,\n",
    "            'num_seeds': num_seeds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEpsilonScheduler:\n",
    "    \n",
    "    def __init__(self, max_iters, steps=None):\n",
    "        super().__init__()\n",
    "        self.max_iters = max_iters\n",
    "        self.steps = steps\n",
    "        if self.steps is None:\n",
    "            self.steps = [(0, 1.0), (1./3, 0.1), (2./3, 0.01)]\n",
    "        self.iter = 0\n",
    "        \n",
    "    def get_eps(self, take_iter=True):\n",
    "        if take_iter:\n",
    "            self.iter += 1\n",
    "        \n",
    "        val = self.iter / self.max_iters\n",
    "        if val < self.steps[0][0]:\n",
    "            return self.steps[0][1]\n",
    "        for i in range(1, len(self.steps)):\n",
    "            if self.steps[i][0] > val:\n",
    "                x_diff = self.steps[i][0] - self.steps[i-1][0]\n",
    "                y_diff = self.steps[i][1] - self.steps[i-1][1]\n",
    "                v_diff = val - self.steps[i-1][0]\n",
    "                return self.steps[i-1][1] + v_diff * y_diff / x_diff\n",
    "        return self.steps[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearEpsilonScheduler(max_iters=1500)\n",
    "eps_vals = [scheduler.get_eps(take_iter=True) for _ in range(scheduler.max_iters)]\n",
    "# plt.plot(eps_vals)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env : gym.Env, \n",
    "                 eval_env : gym.Env = None,\n",
    "                 seed : int = 42,\n",
    "                 lr : float = 3e-4,\n",
    "                 gamma : float = 0.99,\n",
    "                 train_freq : int = 4,\n",
    "                 eval_freq : int = 10000,\n",
    "                 target_freq : int = 2000,\n",
    "                 buffer_warmup : int = 10000,\n",
    "                 batch_size : int = 64,\n",
    "                 scheduler_fn : Any = LinearEpsilonScheduler,\n",
    "                 model_name : str = \"DQN\"):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.eval_env = eval_env\n",
    "        if self.eval_env is None:\n",
    "            self.eval_env = deepcopy(env)\n",
    "        self.train_freq = train_freq\n",
    "        self.eval_freq = eval_freq\n",
    "        self.target_freq = target_freq\n",
    "        self.buffer_warmup = buffer_warmup\n",
    "        self.batch_size = batch_size\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.create_model()\n",
    "        self.init(seed)\n",
    "        self.create_train_step()\n",
    "        self.buffer = ExperienceReplayBuffer()\n",
    "        env_name = env.unwrapped.spec.id.replace('/', '_')\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH,\n",
    "                                    f'{model_name}/{env_name}_seed_{seed}/')\n",
    "        self.logger = SummaryWriter(self.log_dir)\n",
    "        self.all_evals = {}\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = DQN(num_actions=self.env.action_space.n)\n",
    "        \n",
    "    def init(self, seed):\n",
    "        rng = random.PRNGKey(seed)\n",
    "        rng, init_rng, env_rng = random.split(rng, 3)\n",
    "        self.env.seed(jax_to_np_rng(env_rng))\n",
    "        s, _ = self.env.reset()\n",
    "        params = self.model.init(init_rng, s[None])\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(10),\n",
    "            optax.adam(self.lr)\n",
    "        )\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=params,\n",
    "                                       tx=optimizer)\n",
    "        self.rng = rng\n",
    "        \n",
    "    def create_train_step(self):\n",
    "        self.create_loss_fn()\n",
    "        def train_step(state, target_params, batch):\n",
    "            loss, grads = jax.value_and_grad(self.loss_fn)(state.params,\n",
    "                                                           state,\n",
    "                                                           target_params,\n",
    "                                                           batch)\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, loss\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        \n",
    "    def create_loss_fn(self):\n",
    "        def loss_fn(params, state, target_params, batch):\n",
    "            s, s_next, action, reward, done = batch\n",
    "            q_current = state.apply_fn(params, s)\n",
    "            q_next = state.apply_fn(target_params, s_next)\n",
    "            q_current = q_current[np.arange(action.shape[0]), \n",
    "                                  action.astype(np.int32)]\n",
    "            error = td_error_func(reward, q_current, q_next, done, self.gamma)\n",
    "            loss = clipped_mse(error)\n",
    "            return loss.mean()\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def init_environment_state(self):\n",
    "        self.rng, env_rng = random.split(self.rng, 2)\n",
    "        self.env.seed(jax_to_np_rng(env_rng))\n",
    "        s, _ = self.env.reset()\n",
    "        self.env_state = {\n",
    "            'last_state': s\n",
    "        }\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, _ = sample_action(self.state, \n",
    "                             s[None], \n",
    "                             step_rng, \n",
    "                             eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        self.buffer.add(s, s_next, a, r, done)\n",
    "        if done:\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            env.seed(jax_to_np_rng(env_rng))\n",
    "            s_next, _ = self.env.reset()\n",
    "        self.env_state['last_state'] = s_next\n",
    "    \n",
    "    def train_model(self, num_steps=500000):\n",
    "        self.target_params = self.state.params\n",
    "        eps_scheduler = self.scheduler_fn(num_steps)\n",
    "        eval_env = self.eval_env\n",
    "        self.init_environment_state()\n",
    "        losses = []\n",
    "        for step_idx in tqdm(range(1, num_steps + 1)):\n",
    "            train_eps = eps_scheduler.get_eps()\n",
    "            self.take_environment_step(eps=train_eps)\n",
    "            if len(self.buffer) < self.buffer_warmup:\n",
    "                continue\n",
    "            \n",
    "            if step_idx % self.train_freq == 0:\n",
    "                batch = self.buffer.sample(self.batch_size)\n",
    "                self.state, loss = self.train_step(self.state, self.target_params, batch)\n",
    "                losses.append(loss.item())\n",
    "                if len(losses) >= 50:\n",
    "                    self.logger.add_scalar('train/loss', mean(losses), global_step=step_idx)\n",
    "                    self.logger.add_scalar('train/avg_reward', self.buffer.avg_reward(), global_step=step_idx)\n",
    "                    losses.clear()\n",
    "            \n",
    "            if step_idx % self.eval_freq == 0:\n",
    "                eval_dict = eval_policy(self.state, eval_env, num_seeds=10)\n",
    "                self.logger.add_scalar('val/wins', \n",
    "                                       eval_dict['wins'], \n",
    "                                       global_step=step_idx)\n",
    "                self.logger.add_scalar('val/steps', \n",
    "                                       eval_dict['steps'], \n",
    "                                       global_step=step_idx)\n",
    "                self.logger.add_scalar('val/returns', \n",
    "                                       eval_dict['returns'], \n",
    "                                       global_step=step_idx)\n",
    "                self.save_eval(eval_dict, step_idx)\n",
    "                self.save_model(step_idx)\n",
    "                \n",
    "            if step_idx % self.target_freq == 0:\n",
    "                self.target_params = self.state.params\n",
    "                \n",
    "            if step_idx % 1000 == 0:\n",
    "                self.logger.add_scalar('train/eps', train_eps, global_step=step_idx)\n",
    "                \n",
    "        test_dict = eval_policy(self.state, eval_env, num_seeds=100)\n",
    "        self.save_eval(test_dict, 'test')\n",
    "                \n",
    "    def save_eval(self, eval_dict, key):\n",
    "        self.all_evals[key] = eval_dict\n",
    "        with open(os.path.join(self.log_dir, 'evals.pik'), 'wb') as f:\n",
    "            pickle.dump(self.all_evals, f)\n",
    "            \n",
    "    def load_eval(self):\n",
    "        with open(os.path.join(self.log_dir, 'evals.pik'), 'rb') as f:\n",
    "            self.all_evals = pickle.load(f)\n",
    "        \n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target=self.state.params,\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "        \n",
    "    def load_model(self):\n",
    "        params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        self.state = self.state.replace(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(trainer_class, env, num_seeds, eval_env=None, seed_offset=42):\n",
    "    trainers = []\n",
    "    for seed in range(num_seeds):\n",
    "        trainer = trainer_class(env=env, \n",
    "                                eval_env=eval_env, \n",
    "                                seed=seed_offset + seed, \n",
    "                                eval_freq=25000, \n",
    "                                target_freq=10000)\n",
    "        trainer.train_model(num_steps=30000)\n",
    "        trainers.append(trainer)\n",
    "    return trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc66e2353e74e6f98e885d1a739c443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06a1221687ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_trainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_seeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-95e1c777bad0>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(trainer_class, env, num_seeds, eval_env, seed_offset)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 target_freq=10000)\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-c5a82b3ccb14>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/flax/core/frozen_dict.py\u001b[0m in \u001b[0;36mtree_unflatten\u001b[0;34m(cls, _, data)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# data is already deep copied due to tree map mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_trainers = train_models(QTrainer, env, eval_env=eval_env, num_seeds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing learned Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_func(state, seed):\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "    print(env.pos)\n",
    "    env_states = []\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            env.pos = np.array([i, j])\n",
    "            env_states.append(env._render_img())\n",
    "    env_states = np.stack(env_states, axis=0)        \n",
    "    q_vals = state.apply_fn(state.params, env_states)\n",
    "    v_vals = q_vals.max(axis=-1)\n",
    "    v_vals = v_vals.reshape(env.grid_size, env.grid_size)\n",
    "    v_vals = jax.device_get(v_vals)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
    "    ax[0].imshow(v_vals)\n",
    "    ax[1].imshow(env_states[0])\n",
    "    plt.show()\n",
    "    print(v_vals)\n",
    "    \n",
    "# visualize_value_func(state, seed=eval_dict['unsolved'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPrioTrainer(QTrainer):\n",
    "    \n",
    "    def __init__(self, *args, \n",
    "                 prio_alpha : float = 0.6,\n",
    "                 prio_eps : float = 1e-5,\n",
    "                 model_name : str = 'DQN_Prio',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         model_name=model_name, \n",
    "                         **kwargs)\n",
    "        self.prio_alpha = prio_alpha\n",
    "        self.prio_eps = prio_eps\n",
    "        self.create_prio_fn()\n",
    "        \n",
    "    def create_prio_fn(self):\n",
    "        def get_prio_td_error(state, target_params, r, q_vals, s_next, done):\n",
    "            target_q = apply_model(state, target_params, s_next[None])\n",
    "            td_error = td_error_func(r, q_vals, target_q, done, self.gamma)\n",
    "            return td_error\n",
    "        self.get_prio_td_error = jax.jit(get_prio_td_error)\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, q_vals = sample_action(self.state, \n",
    "                                  s[None], \n",
    "                                  step_rng, \n",
    "                                  eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        td_error = self.get_prio_td_error(self.state, self.target_params,\n",
    "                                          r, q_vals, s_next, done)\n",
    "        sample_prob = (abs(td_error.item()) + self.prio_eps) ** self.prio_alpha\n",
    "        self.buffer.add(s, s_next, a, r, done, prob=sample_prob)\n",
    "        if done:\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            self.env.seed(jax_to_np_rng(env_rng))\n",
    "            s_next, _ = self.env.reset()\n",
    "        self.env_state['last_state'] = s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qprio_trainers = train_models(QPrioTrainer, env, eval_env=eval_env, num_seeds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qprio_trainers[0].all_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just description on Double Q-Learning, we can combine it with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def doubleQ_error_func(reward, q_vals, q_next, target_q, done, gamma):\n",
    "    target_q = target_q[jnp.arange(target_q.shape[0]),\n",
    "                        q_next.argmax(axis=-1)]\n",
    "    target = reward + (1 - done) * gamma * target_q\n",
    "    error = target - q_vals\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQTrainer(QPrioTrainer):\n",
    "    \n",
    "    def __init__(self, *args,\n",
    "                 model_name : str = 'DoubleDQN',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, model_name=model_name, **kwargs)\n",
    "        self.create_prio_fn()\n",
    "    \n",
    "    def create_loss_fn(self):\n",
    "        def loss_fn(params, state, target_params, batch):\n",
    "            s, s_next, action, reward, done = batch\n",
    "            q = state.apply_fn(params, jnp.concatenate([s, s_next], axis=0))\n",
    "            q_current, q_next = q.split(2, axis=0)\n",
    "            q_target = state.apply_fn(target_params, s_next)\n",
    "            q_current = q_current[np.arange(action.shape[0]), \n",
    "                                  action.astype(np.int32)]\n",
    "            error = doubleQ_error_func(reward, q_current, q_next, q_target, done, self.gamma)\n",
    "            loss = clipped_mse(error)\n",
    "            return loss.mean()\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def create_prio_fn(self):\n",
    "        def get_prio_td_error(state, target_params, r, q_vals, s_next, done):\n",
    "            target_q = apply_model(state, target_params, s_next[None])\n",
    "            orig_next_q = apply_model(state, state.params, s_next[None])\n",
    "            td_error = doubleQ_error_func(r, q_vals, orig_next_q, target_q, done, self.gamma)\n",
    "            return td_error, orig_next_q\n",
    "        self.get_prio_td_error = jax.jit(get_prio_td_error)\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, q_vals = sample_action(self.state, \n",
    "                                  self.env_state.get('last_qvals', s[None]), \n",
    "                                  step_rng, \n",
    "                                  eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        td_error, next_q = self.get_prio_td_error(self.state, \n",
    "                                                  self.target_params, \n",
    "                                                  r, \n",
    "                                                  q_vals, \n",
    "                                                  s_next, \n",
    "                                                  done)\n",
    "        self.env_state['last_qvals'] = next_q\n",
    "        sample_prob = (abs(td_error.item()) + self.prio_eps) ** self.prio_alpha\n",
    "        self.buffer.add(s, s_next, a, r, done, prob=sample_prob)\n",
    "        if done:\n",
    "            self.env_state.pop('last_qvals')\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            self.env.seed(jax_to_np_rng(env_rng))\n",
    "            s_next, _ = self.env.reset() # env_rng)\n",
    "        self.env_state['last_state'] = s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleq_trainers = train_models(DoubleQTrainer, env, eval_env=eval_env, num_seeds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    num_actions : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, return_separate=False):\n",
    "        x = Backbone()(x)\n",
    "        v = nn.Dense(512)(x)\n",
    "        v = nn.relu(v)\n",
    "        v = nn.Dense(1,\n",
    "                     kernel_init=nn.initializers.zeros)(v)\n",
    "        a = nn.Dense(512)(x)\n",
    "        a = nn.relu(a)\n",
    "        a = nn.Dense(self.num_actions,\n",
    "                     kernel_init=nn.initializers.zeros)(a)\n",
    "        a = a - a.mean(axis=-1, keepdims=True)\n",
    "        q = v + a\n",
    "        \n",
    "        if not return_separate:\n",
    "            return q\n",
    "        else:\n",
    "            return q, {'v': v, 'a': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQTrainer(DoubleQTrainer):\n",
    "    \n",
    "    def __init__(self, *args,\n",
    "                 model_name : str = 'DuelingDQN',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, model_name=model_name, **kwargs)\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.model = DuelingDQN(num_actions=self.env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duelingq_trainers = train_models(DuelingQTrainer, env, eval_env=eval_env, num_seeds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
