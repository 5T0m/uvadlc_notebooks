{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 18 (JAX): Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n",
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable\n",
    "from types import SimpleNamespace\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "import pickle\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial18_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gym\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet gym[all]\n",
    "    import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def jax_to_np_rng(rng):\n",
    "    return random.randint(rng, shape=(1,), minval=0, maxval=int(1e6), dtype=np.int64).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, grid_size = 5):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.spec = SimpleNamespace(max_episode_steps=100)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        if isinstance(seed, jnp.ndarray):\n",
    "            seed = jax_to_np_rng(seed)\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "        self.pos = self.rng.integers(0, high=self.grid_size, size=(2,))\n",
    "        self.goal = None\n",
    "        while self.goal is None or self._is_on_goal():\n",
    "            self.goal = self.rng.integers(0, high=self.grid_size, size=(2,))\n",
    "        self.grid = np.ones((self.grid_size, self.grid_size, 3), dtype=np.uint8) * 255\n",
    "        return self._render_img()\n",
    "    \n",
    "    def _render_img(self):\n",
    "        grid = np.copy(self.grid)\n",
    "        grid[self.pos[0], self.pos[1], 1:] = 0\n",
    "        grid[self.goal[0], self.goal[1], ::2] = 0\n",
    "        return grid\n",
    "    \n",
    "    def _is_on_goal(self):\n",
    "        return all([self.pos[i] == self.goal[i] for i in range(2)])\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0: # Up\n",
    "            self.pos[0] = max(0, self.pos[0] - 1)\n",
    "        elif action == 1: # Right\n",
    "            self.pos[1] = min(self.grid_size - 1, self.pos[1] + 1)\n",
    "        elif action == 2: # Down\n",
    "            self.pos[0] = min(self.grid_size - 1, self.pos[0] + 1)\n",
    "        elif action == 3: # Left\n",
    "            self.pos[1] = max(0, self.pos[1] - 1)\n",
    "        \n",
    "        done = self._is_on_goal()\n",
    "        reward = 1 if done else -0.1\n",
    "        state = self._render_img()\n",
    "        \n",
    "        return state, reward, done, False, {}\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_to_np_rng(random.PRNGKey(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # env with preprocessing\n",
    "# env = gym.make('PongNoFrameskip-v4', \n",
    "#                # 'ALE/SpaceInvaders-v5', \n",
    "#                new_step_api=True, \n",
    "#                full_action_space=False,\n",
    "#                frameskip=1\n",
    "#               )\n",
    "# env = gym.wrappers.AtariPreprocessing(env, \n",
    "#                                       new_step_api=True,\n",
    "#                                       grayscale_obs=False,\n",
    "#                                       )\n",
    "# env = gym.wrappers.FrameStack(env, 2, new_step_api=True)\n",
    "env = MazeEnv(grid_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32) / 255. * 2. - 1.\n",
    "        if len(x.shape) >= 5:\n",
    "            x = jnp.concatenate([x[:,1] - x[:,0], x[:,1]], axis=-1)\n",
    "        \n",
    "        if False:\n",
    "            x = nn.Conv(16, kernel_size=(7, 7), strides=(4, 4))(x)  # 84 => 21\n",
    "            x = nn.relu(x)\n",
    "            x = nn.Conv(32, kernel_size=(5, 5), strides=(2, 2))(x)  # 21 => 10\n",
    "            x = nn.relu(x)\n",
    "        else:\n",
    "            x = nn.Conv(16, kernel_size=(3, 3), strides=(1, 1))(x) \n",
    "            x = nn.relu(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    num_actions : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = Backbone()(x)\n",
    "        q = nn.Dense(self.num_actions,\n",
    "                     kernel_init=nn.initializers.zeros)(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity=25000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = list()\n",
    "        self.probs = list()\n",
    "        \n",
    "    def add(self, s, s_next, action, reward, done, prob=1.0):\n",
    "        while len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "            self.probs.pop(0)\n",
    "        self.buffer.append((s, s_next, action, reward, done))\n",
    "        self.probs.append(prob)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        probs = np.array(self.probs)\n",
    "        probs = probs / probs.sum()\n",
    "        trans_idxs = np.random.choice(len(self.buffer), \n",
    "                                      batch_size, \n",
    "                                      replace=len(self.buffer) <= batch_size,\n",
    "                                      p=probs)\n",
    "        transitions = [self.buffer[idx] for idx in trans_idxs]\n",
    "        transitions = tuple(np.stack(t, axis=0) for t in zip(*transitions))\n",
    "        return transitions\n",
    "    \n",
    "    def avg_reward(self):\n",
    "        rewards = [b[3] for b in self.buffer]\n",
    "        return sum(rewards) / len(rewards)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.probs.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sample_action(state, inps, rng, eps=0.0):\n",
    "    if len(inps.shape) == 2:\n",
    "        q_vals = inps\n",
    "    else:\n",
    "        q_vals = state.apply_fn(state.params, inps)\n",
    "    # Epsilon-greedy policy\n",
    "    q_argmax = q_vals.argmax(axis=-1)\n",
    "    probs = jax.nn.one_hot(q_argmax, num_classes=q_vals.shape[-1]) * (1 - eps) + eps / q_vals.shape[-1]\n",
    "    actions = random.categorical(rng, jnp.log(jnp.maximum(probs, 1e-10)), axis=-1)\n",
    "    # q-value of selected action\n",
    "    act_q_vals = q_vals[jnp.arange(actions.shape[0]), actions]\n",
    "    return actions, act_q_vals\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, params, imgs):\n",
    "    return state.apply_fn(params, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_error_func(reward, q_vals, target_q, done, gamma):\n",
    "    target = reward + (1 - done) * gamma * target_q.max(axis=-1)\n",
    "    error = target - q_vals\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_mse(error):\n",
    "    diff = jnp.abs(error)\n",
    "    return jnp.where(diff > 1.0, diff, diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_episode(state, env, seed, eps=0.0):\n",
    "    s = env.reset(seed)\n",
    "    rng = random.PRNGKey(seed)\n",
    "    rew_return = 0.0\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        rng, step_rng = random.split(rng)\n",
    "        a, _ = sample_action(state, np.array(s)[None], step_rng, eps=eps)\n",
    "        a = a.item()\n",
    "        s, r, terminated, truncated, _ = env.step(a)\n",
    "        rew_return += r\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            return (True, t, rew_return)\n",
    "    return (False, t, rew_return)\n",
    "\n",
    "def eval_policy(state, env):\n",
    "    wins, steps, returns = zip(*[eval_episode(state, env, seed=i, eps=0.0) for i in range(100)])\n",
    "    win_prop = mean(wins)\n",
    "    avg_steps = mean(steps)\n",
    "    avg_return = mean(returns)\n",
    "    unsolved = [i for i in range(len(wins)) if not wins[i]]\n",
    "    return {'wins': win_prop, 'steps': avg_steps, 'returns': avg_return, 'unsolved': unsolved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env : gym.Env, \n",
    "                 seed : int = 42,\n",
    "                 lr : float = 3e-4,\n",
    "                 gamma : float = 0.99,\n",
    "                 train_freq : int = 4,\n",
    "                 eval_freq : int = 10000,\n",
    "                 target_freq : int = 2000,\n",
    "                 batch_size : int = 64,\n",
    "                 model_name : str = \"DQN\"):\n",
    "        super().__init__()\n",
    "        self.env = deepcopy(env)\n",
    "        self.train_freq = train_freq\n",
    "        self.eval_freq = eval_freq\n",
    "        self.target_freq = target_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.create_model()\n",
    "        self.init(seed)\n",
    "        self.create_train_step()\n",
    "        self.buffer = ExperienceReplayBuffer()\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH,\n",
    "                                    f'{model_name}/seed_{seed}/')\n",
    "        self.logger = SummaryWriter(self.log_dir)\n",
    "        self.all_evals = {}\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = DQN(num_actions=self.env.action_space.n)\n",
    "        \n",
    "    def init(self, seed):\n",
    "        rng = random.PRNGKey(seed)\n",
    "        rng, init_rng, env_rng = random.split(rng, 3)\n",
    "        s = self.env.reset(env_rng)\n",
    "        params = self.model.init(init_rng, s[None])\n",
    "        optimizer = optax.adam(self.lr)\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=params,\n",
    "                                       tx=optimizer)\n",
    "        self.rng = rng\n",
    "        \n",
    "    def create_train_step(self):\n",
    "        self.create_loss_fn()\n",
    "        def train_step(state, target_params, batch):\n",
    "            loss, grads = jax.value_and_grad(self.loss_fn)(state.params,\n",
    "                                                           state,\n",
    "                                                           target_params,\n",
    "                                                           batch)\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, loss\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        \n",
    "    def create_loss_fn(self):\n",
    "        def loss_fn(params, state, target_params, batch):\n",
    "            s, s_next, action, reward, done = batch\n",
    "            q_current = state.apply_fn(params, s)\n",
    "            q_next = state.apply_fn(target_params, s_next)\n",
    "            q_current = q_current[np.arange(action.shape[0]), \n",
    "                                  action.astype(np.int32)]\n",
    "            error = td_error_func(reward, q_current, q_next, done, self.gamma)\n",
    "            loss = clipped_mse(error)\n",
    "            return loss.mean()\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def init_environment_state(self):\n",
    "        self.rng, env_rng = random.split(self.rng, 2)\n",
    "        s = self.env.reset(env_rng)\n",
    "        self.env_state = {\n",
    "            'last_state': s\n",
    "        }\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, _ = sample_action(self.state, \n",
    "                             s[None], \n",
    "                             step_rng, \n",
    "                             eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        self.buffer.add(s, s_next, a, r, done)\n",
    "        if done:\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            s_next = self.env.reset(env_rng)\n",
    "        self.env_state['last_state'] = s_next\n",
    "    \n",
    "    def train_model(self, num_steps=500000):\n",
    "        self.target_params = self.state.params\n",
    "        best_return = -9e15\n",
    "        eval_env = deepcopy(self.env)\n",
    "        self.init_environment_state()\n",
    "        losses = []\n",
    "        for step_idx in tqdm(range(1, num_steps + 1)):\n",
    "            self.take_environment_step(eps=1.0 - step_idx / num_steps)\n",
    "            if len(self.buffer) < 1000:\n",
    "                continue\n",
    "            \n",
    "            if step_idx % self.train_freq == 0:\n",
    "                batch = self.buffer.sample(self.batch_size)\n",
    "                self.state, loss = self.train_step(self.state, self.target_params, batch)\n",
    "                losses.append(loss.item())\n",
    "                if len(losses) >= 50:\n",
    "                    self.logger.add_scalar('train/loss', mean(losses), global_step=step_idx)\n",
    "                    losses.clear()\n",
    "            \n",
    "            if step_idx % self.eval_freq == 0:\n",
    "                eval_dict = eval_policy(self.state, eval_env)\n",
    "                self.logger.add_scalar('val/wins', \n",
    "                                       eval_dict['wins'], \n",
    "                                       global_step=step_idx)\n",
    "                self.logger.add_scalar('val/steps', \n",
    "                                       eval_dict['steps'], \n",
    "                                       global_step=step_idx)\n",
    "                self.logger.add_scalar('val/returns', \n",
    "                                       eval_dict['returns'], \n",
    "                                       global_step=step_idx)\n",
    "                self.save_eval(eval_dict, step_idx)\n",
    "                if eval_dict['returns'] > best_return:\n",
    "                    best_return = eval_dict['returns']\n",
    "                    self.save_model(step_idx)\n",
    "                \n",
    "            if step_idx % self.target_freq == 0:\n",
    "                self.target_params = self.state.params\n",
    "                \n",
    "    def save_eval(self, eval_dict, step):\n",
    "        self.all_evals[step] = eval_dict\n",
    "        with open(os.path.join(self.log_dir, 'evals.pik'), 'wb') as f:\n",
    "            pickle.dump(self.all_evals, f)\n",
    "            \n",
    "    def load_eval(self):\n",
    "        with open(os.path.join(self.log_dir, 'evals.pik'), 'rb') as f:\n",
    "            self.all_evals = pickle.load(f)\n",
    "        \n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target=self.state.params,\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "        \n",
    "    def load_model(self):\n",
    "        params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        self.state = self.state.replace(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(trainer_class, env, num_seeds):\n",
    "    trainers = []\n",
    "    for seed in range(num_seeds):\n",
    "        trainer = trainer_class(env=env, seed=42 + seed)\n",
    "        trainer.train_model(num_steps=250000)\n",
    "        trainers.append(trainer)\n",
    "    return trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trainers = train_models(QTrainer, env, num_seeds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing learned Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_func(state, seed):\n",
    "    env.reset(seed)\n",
    "    print(env.pos)\n",
    "    env_states = []\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            env.pos = np.array([i, j])\n",
    "            env_states.append(env._render_img())\n",
    "    env_states = np.stack(env_states, axis=0)        \n",
    "    q_vals = state.apply_fn(state.params, env_states)\n",
    "    v_vals = q_vals.max(axis=-1)\n",
    "    v_vals = v_vals.reshape(env.grid_size, env.grid_size)\n",
    "    v_vals = jax.device_get(v_vals)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
    "    ax[0].imshow(v_vals)\n",
    "    ax[1].imshow(env_states[0])\n",
    "    plt.show()\n",
    "    print(v_vals)\n",
    "    \n",
    "# visualize_value_func(state, seed=eval_dict['unsolved'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPrioTrainer(QTrainer):\n",
    "    \n",
    "    def __init__(self, *args, \n",
    "                 prio_alpha : float = 0.6,\n",
    "                 prio_eps : float = 1e-5,\n",
    "                 model_name : str = 'DQN_Prio',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         model_name=model_name, \n",
    "                         **kwargs)\n",
    "        self.prio_alpha = prio_alpha\n",
    "        self.prio_eps = prio_eps\n",
    "        \n",
    "    def get_prio_td_error(self, r, q_vals, s_next, done):\n",
    "        target_q = apply_model(self.state, self.target_params, s_next[None])\n",
    "        td_error = td_error_func(r, q_vals, target_q, done, self.gamma)\n",
    "        return td_error\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, q_vals = sample_action(self.state, \n",
    "                                  s[None], \n",
    "                                  step_rng, \n",
    "                                  eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        td_error = self.get_prio_td_error(r, q_vals, s_next, done)\n",
    "        sample_prob = (abs(td_error.item()) + self.prio_eps) ** self.prio_alpha\n",
    "        self.buffer.add(s, s_next, a, r, done, prob=sample_prob)\n",
    "        if done:\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            s_next = self.env.reset(env_rng)\n",
    "        self.env_state['last_state'] = s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qprio_trainers = train_models(QPrioTrainer, env, num_seeds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just description on Double Q-Learning, we can combine it with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleQ_error_func(reward, q_vals, q_next, target_q, done, gamma):\n",
    "    target_q = target_q[jnp.arange(target_q.shape[0]),\n",
    "                        q_next.argmax(axis=-1)]\n",
    "    target = reward + (1 - done) * gamma * target_q\n",
    "    error = target - q_vals\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQTrainer(QPrioTrainer):\n",
    "    \n",
    "    def __init__(self, *args,\n",
    "                 model_name : str = 'DoubleDQN',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, model_name=model_name, **kwargs)\n",
    "    \n",
    "    def create_loss_fn(self):\n",
    "        def loss_fn(params, state, target_params, batch):\n",
    "            s, s_next, action, reward, done = batch\n",
    "            q = state.apply_fn(params, jnp.concatenate([s, s_next], axis=0))\n",
    "            q_current, q_next = q.split(2, axis=0)\n",
    "            q_target = state.apply_fn(target_params, s_next)\n",
    "            q_current = q_current[np.arange(action.shape[0]), \n",
    "                                  action.astype(np.int32)]\n",
    "            error = doubleQ_error_func(reward, q_current, q_next, q_target, done, self.gamma)\n",
    "            loss = clipped_mse(error)\n",
    "            return loss.mean()\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def get_prio_td_error(self, r, q_vals, s_next, done):\n",
    "        target_q = apply_model(self.state, self.target_params, s_next[None])\n",
    "        orig_next_q = apply_model(self.state, self.state.params, s_next[None])\n",
    "        self.env_state['last_qvals'] = orig_next_q\n",
    "        td_error = doubleQ_error_func(r, q_vals, orig_next_q, target_q, done, self.gamma)\n",
    "        return td_error\n",
    "        \n",
    "    def init_environment_state(self):\n",
    "        super().init_environment_state()\n",
    "        \n",
    "    def take_environment_step(self, eps):\n",
    "        self.rng, step_rng = random.split(self.rng)\n",
    "        s = self.env_state['last_state']\n",
    "        a, q_vals = sample_action(self.state, \n",
    "                                  self.env_state.get('last_qvals', s[None]), \n",
    "                                  step_rng, \n",
    "                                  eps=eps)\n",
    "        a = a.item()\n",
    "        s_next, r, done, _, _ = self.env.step(a)\n",
    "        td_error = self.get_prio_td_error(r, q_vals, s_next, done)\n",
    "        sample_prob = (abs(td_error.item()) + self.prio_eps) ** self.prio_alpha\n",
    "        self.buffer.add(s, s_next, a, r, done, prob=sample_prob)\n",
    "        if done:\n",
    "            self.env_state.pop('last_qvals')\n",
    "            self.rng, env_rng = random.split(self.rng)\n",
    "            s_next = self.env.reset(env_rng)\n",
    "        self.env_state['last_state'] = s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleq_trainers = train_models(DoubleQTrainer, env, num_seeds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    num_actions : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, return_separate=False):\n",
    "        x = Backbone()(x)\n",
    "        v = nn.Dense(1,\n",
    "                     kernel_init=nn.initializers.zeros)(x)\n",
    "        a = nn.Dense(self.num_actions,\n",
    "                     kernel_init=nn.initializers.zeros)(x)\n",
    "        a = a - a.mean(axis=-1, keepdims=True)\n",
    "        q = v + a\n",
    "        \n",
    "        if not return_separate:\n",
    "            return q\n",
    "        else:\n",
    "            return q, {'v': v, 'a': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQTrainer(DoubleQTrainer):\n",
    "    \n",
    "    def __init__(self, *args,\n",
    "                 model_name : str = 'DuelingDQN',\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, model_name=model_name, **kwargs)\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.model = DuelingDQN(num_actions=self.env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duelingq_trainers = train_models(DuelingQTrainer, env, num_seeds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
