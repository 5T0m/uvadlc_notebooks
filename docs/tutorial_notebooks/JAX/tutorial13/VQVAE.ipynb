{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13 (JAX): Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Sequence, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch Data Loading\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (i.e. STL10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial13_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial13/\"\n",
    "# Files to download\n",
    "pretrained_files = []\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255. - 0.5) * 2.0\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 image_to_numpy\n",
    "                                ])\n",
    "\n",
    "train_data = STL10(root=DATASET_PATH, split='unlabeled', download=True,\n",
    "                   transform=data_transforms)\n",
    "test_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
    "                  transform=image_to_numpy)\n",
    "test_data, val_data = data.random_split(test_data, [3000, 2000], \n",
    "                                        generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = data.DataLoader(train_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )\n",
    "val_loader   = data.DataLoader(val_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )\n",
    "test_loader  = data.DataLoader(test_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv initialized with kaiming int, but uses fan-out instead of fan-in mode\n",
    "# Fan-out focuses on the gradient distribution, and is commonly used in ResNets\n",
    "resnet_kernel_init = nn.initializers.variance_scaling(2.0, mode='fan_out', distribution='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "    subsample : bool = False  # If True, we apply a stride inside F\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = nn.BatchNorm()(x, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    strides=(1, 1) if not self.subsample else (2, 2),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "\n",
    "        if self.subsample:\n",
    "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
    "            x = self.act_fn(x)\n",
    "            x = nn.Conv(self.c_out,\n",
    "                        kernel_size=(1, 1),\n",
    "                        strides=(2, 2),\n",
    "                        kernel_init=resnet_kernel_init,\n",
    "                        use_bias=False)(x)\n",
    "\n",
    "        x_out = z + x\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = nn.BatchNorm()(x, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = nn.BatchNorm()(z, use_running_average=not train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        \n",
    "        if x.shape[-1] != z.shape[-1]:\n",
    "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
    "            x = self.act_fn(x)\n",
    "            x = nn.Conv(self.c_out, kernel_size=(1, 1),\n",
    "                        kernel_init=resnet_kernel_init,\n",
    "                        use_bias=False)(z)\n",
    "        \n",
    "        return x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    num_blocks : Sequence[int]\n",
    "    c_hidden : Sequence[int]\n",
    "    vocab_dim : int\n",
    "    act_fn : callable = nn.swish\n",
    "        \n",
    "    def setup(self):\n",
    "        self.in_conv = nn.Conv(self.c_hidden[0], kernel_size=(3, 3),\n",
    "                                 strides=(2, 2),\n",
    "                                 kernel_init=resnet_kernel_init,\n",
    "                                 use_bias=False)\n",
    "        block_modules = []\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                subsample = (bc == 0 and block_idx > 0)  # Subsample the first block of each group, except the very first one.\n",
    "                block_modules.append(\n",
    "                    EncoderResNetBlock(act_fn=self.act_fn,\n",
    "                                       subsample=subsample,\n",
    "                                       c_out=self.c_hidden[block_idx])\n",
    "                )\n",
    "        self.block_modules = block_modules\n",
    "                \n",
    "        self.out_norm = nn.BatchNorm()\n",
    "        self.out_conv = nn.Conv(self.vocab_dim, kernel_size=(1, 1),\n",
    "                                kernel_init=resnet_kernel_init,\n",
    "                                use_bias=True)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.block_modules:\n",
    "            x = layer(x, train=train)\n",
    "        x = self.out_norm(x, use_running_average=not train)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.out_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 12, 12, 512)\n"
     ]
    }
   ],
   "source": [
    "module = Encoder(num_blocks=[3, 3, 2],\n",
    "                 c_hidden=[32, 64, 128],\n",
    "                 vocab_dim=512)\n",
    "inp, _ = next(iter(train_loader))\n",
    "variables = module.init(random.PRNGKey(0), inp)\n",
    "params, batch_stats = variables['params'], variables['batch_stats']\n",
    "out = module.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                   inp,\n",
    "                   train=True,\n",
    "                   mutable=['batch_stats'])\n",
    "vecs, _ = out\n",
    "print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    scale_factor : float = 2.0\n",
    "    method : str = 'nearest'\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        B, H, W, C = x.shape\n",
    "        x = jax.image.resize(x, \n",
    "                             shape=(B, int(H * self.scale_factor), int(W * self.scale_factor), C),\n",
    "                             method=self.method)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    num_blocks : Sequence[int]\n",
    "    c_hidden : Sequence[int]\n",
    "    c_out : int\n",
    "    act_fn : callable = nn.swish\n",
    "        \n",
    "    def setup(self):\n",
    "        self.in_conv = nn.Conv(self.c_hidden[0], kernel_size=(1, 1),\n",
    "                               kernel_init=resnet_kernel_init,\n",
    "                               use_bias=False)\n",
    "        \n",
    "        block_modules = []\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            if block_idx > 0:\n",
    "                block_modules.append(Upsample())\n",
    "            for bc in range(block_count):\n",
    "                channel_change = (bc == 0 and block_idx > 0)\n",
    "                block_modules.append(\n",
    "                    DecoderResNetBlock(act_fn=self.act_fn,\n",
    "                                       c_out=self.c_hidden[block_idx])\n",
    "                )\n",
    "                \n",
    "        self.block_modules = block_modules\n",
    "        self.out_norm = nn.BatchNorm()\n",
    "        self.out_conv = nn.ConvTranspose(self.c_out, kernel_size=(3, 3),\n",
    "                                         strides=(2, 2),\n",
    "                                         kernel_init=resnet_kernel_init)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.block_modules:\n",
    "            x = layer(x, train=train)\n",
    "        x = self.out_norm(x, use_running_average=not train)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "module = Decoder(num_blocks=[2, 3, 3],\n",
    "                 c_hidden=[128, 64, 32],\n",
    "                 c_out=3)\n",
    "inp = random.normal(random.PRNGKey(0), (64, 12, 12, 256))\n",
    "variables = module.init(random.PRNGKey(0), inp)\n",
    "params, batch_stats = variables['params'], variables['batch_stats']\n",
    "out = module.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                   inp,\n",
    "                   train=True,\n",
    "                   mutable=['batch_stats'])\n",
    "vecs, _ = out\n",
    "print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbedding(nn.Module):\n",
    "    vocab_size : int\n",
    "    vocab_dim : int\n",
    "    mask_prob : float = 0.4\n",
    "    \n",
    "    def setup(self):\n",
    "        self.codebook = nn.Embed(self.vocab_size, self.vocab_dim)\n",
    "        \n",
    "    def __call__(self, z, train=True):\n",
    "        z_flatten = z.reshape(-1, z.shape[-1])\n",
    "        \n",
    "        # dummy op to init the weights, so we can access them below\n",
    "        self.codebook(jnp.ones((1, 1), dtype=\"i4\"))\n",
    "\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "        emb_weights = self.variables[\"params\"][\"codebook\"][\"embedding\"]\n",
    "        distance = (\n",
    "            jnp.sum(z_flatten ** 2, axis=1, keepdims=True)\n",
    "            + jnp.sum(emb_weights ** 2, axis=1)\n",
    "            - 2 * jnp.dot(z_flatten, emb_weights.T)\n",
    "        )\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        min_encoding_indices = jnp.argmin(distance, axis=1)\n",
    "        z_embeds = self.codebook(min_encoding_indices).reshape(z.shape)\n",
    "        z_q = z_embeds\n",
    "        if train:\n",
    "            z_q = z + jax.lax.stop_gradient(z_q - z)\n",
    "            if self.mask_prob > 0.0:\n",
    "                num_masked = int(z_q.shape[0] * self.mask_prob)\n",
    "                z_q = jnp.concatenate([z[:num_masked], z_q[num_masked:]], axis=0)\n",
    "        \n",
    "        return z_q, z_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 12, 12, 256)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = VQEmbedding(vocab_size=512, vocab_dim=256)\n",
    "inp = random.normal(random.PRNGKey(0), (64, 12, 12, 256))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "z_q, z_embeds = module.apply(params, inp, train=False)\n",
    "z_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(pred, target):\n",
    "    return ((pred - jax.lax.stop_gradient(target))**2).mean()\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    vocab_size : int\n",
    "    vocab_dim : int\n",
    "    beta : float\n",
    "    resnet_blocks : Sequence[int] = (3, 3, 2)\n",
    "    resnet_hidden : Sequence[int] = (32, 64, 128)\n",
    "    \n",
    "    def setup(self):\n",
    "        # Encoder network (ResNet)\n",
    "        self.encoder = Encoder(num_blocks=self.resnet_blocks,\n",
    "                               c_hidden=self.resnet_hidden,\n",
    "                               vocab_dim=self.vocab_dim,\n",
    "                               act_fn=nn.swish)\n",
    "        # Vector quantized bottleneck layer\n",
    "        self.vector_quantization = VQEmbedding(vocab_size=self.vocab_size,\n",
    "                                               vocab_dim=self.vocab_dim)\n",
    "        # Decoder network (ResNet, mirrored version of Encoder)\n",
    "        self.decoder = Decoder(num_blocks=self.resnet_blocks[::-1],\n",
    "                               c_hidden=self.resnet_hidden[::-1],\n",
    "                               c_out=3,\n",
    "                               act_fn=nn.swish)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        z = self.encoder(x, train=train)\n",
    "        z_quantized, z_embeddings = self.vector_quantization(z, train=train)\n",
    "        x_rec = self.decoder(z_quantized, train=train)\n",
    "        \n",
    "        rec_loss = mse_loss(x_rec, x)\n",
    "        embed_loss = mse_loss(z_embeddings, z)\n",
    "        quant_loss = mse_loss(z, z_embeddings)\n",
    "        \n",
    "        loss = rec_loss + embed_loss + self.beta * quant_loss\n",
    "        \n",
    "        metrics = {'loss': loss,\n",
    "                   'rec_loss': rec_loss,\n",
    "                   'l1_dist': jnp.abs(x_rec - x).mean(),\n",
    "                   'embed_loss': embed_loss,\n",
    "                   'quant_loss': self.beta * quant_loss}\n",
    "        \n",
    "        return loss, metrics\n",
    "        \n",
    "    def reconstruct(self, x, train=False):\n",
    "        z = self.encoder(x, train=train)\n",
    "        z, _ = self.vector_quantization(z, train=train)\n",
    "        x_rec = self.decoder(z, train=train)\n",
    "        return x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': DeviceArray(0.5234843, dtype=float32),\n",
       " 'rec_loss': DeviceArray(0.50710994, dtype=float32),\n",
       " 'l1_dist': DeviceArray(0.60258454, dtype=float32),\n",
       " 'embed_loss': DeviceArray(0.01309951, dtype=float32),\n",
       " 'quant_loss': DeviceArray(0.00327488, dtype=float32)}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = VQVAE(vocab_size=512, vocab_dim=256, beta=0.25)\n",
    "inp = nn.tanh(random.normal(random.PRNGKey(0), (64, 96, 96, 3)))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "_, metrics = module.apply(params, inp, train=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "del module, metrics, params, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    batch_stats: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_grid(imgs, pads=4, num_rows=4):\n",
    "    imgs = np.pad(imgs, ((0,0), (pads,pads), (pads,pads), (0,0)))\n",
    "    imgs = np.reshape(imgs, (num_rows, -1, *imgs.shape[1:]))\n",
    "    imgs = imgs.swapaxes(1, 2)\n",
    "    imgs = imgs.reshape(num_rows*imgs.shape[1], -1, imgs.shape[-1])\n",
    "    imgs = np.pad(imgs, ((pads,pads), (pads,pads), (0,0)))\n",
    "    imgs = (imgs + 1.) / 2.\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, exmp_imgs, lr=5e-4, seed=42, **model_hparams):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            lr - Learning rate of the optimizer to use\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.exmp_imgs = exmp_imgs\n",
    "        self.model_name = 'VQVAE2'\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = VQVAE(**model_hparams)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, f'{self.model_name}/')\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.logged_orig_imgs = False\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_imgs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def calculate_loss(params, batch_stats, batch, train):\n",
    "            outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                                    batch,\n",
    "                                    train=train,\n",
    "                                    mutable=['batch_stats'] if train else False)\n",
    "            (loss, metrics), new_model_state = outs if train else (outs, None)\n",
    "            return loss, (metrics, new_model_state)\n",
    "        # Training function\n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, state.batch_stats, batch, train=True)\n",
    "            # Get loss, gradients for loss, and other outputs of loss function\n",
    "            (_, (metrics, new_model_state)), grads = jax.value_and_grad(loss_fn, \n",
    "                                                                        has_aux=True)(state.params)\n",
    "            # Update parameters and batch statistics\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])\n",
    "            return state, metrics\n",
    "        # Eval function\n",
    "        def eval_step(state, batch):\n",
    "            # Return the accuracy for a single batch\n",
    "            _, (metrics, _) = calculate_loss(state.params, state.batch_stats, batch, train=False)\n",
    "            return metrics\n",
    "        # Reconstruction function\n",
    "        def reconst_func(state):\n",
    "            x_rec = self.model.bind({'params': state.params, \n",
    "                                     'batch_stats': state.batch_stats}).reconstruct(self.exmp_imgs[:16])\n",
    "            return jax.device_get(x_rec)\n",
    "        # jit for efficiency\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "        self.reconst_func = jax.jit(reconst_func)\n",
    "\n",
    "    def init_model(self, exmp_imgs):\n",
    "        # Initialize model\n",
    "        init_rng = jax.random.PRNGKey(self.seed)\n",
    "        variables = self.model.init(init_rng, exmp_imgs, train=True)\n",
    "        self.state = TrainState(step=0,\n",
    "                                apply_fn=self.model.apply,\n",
    "                                params=variables['params'],\n",
    "                                batch_stats=variables['batch_stats'],\n",
    "                                tx=None, opt_state=None)\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "        # We decrease the learning rate by a factor of 0.1 after 60% and 85% of the training\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=50,\n",
    "            decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "            end_value=1e-2*self.lr\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = TrainState.create(apply_fn=self.state.apply_fn,\n",
    "                                       params=self.state.params,\n",
    "                                       batch_stats=self.state.batch_stats,\n",
    "                                       tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200):\n",
    "        # Train model for defined number of epochs\n",
    "        # We first need to create optimizer and the scheduler for the given number of epochs\n",
    "        self.init_optimizer(num_epochs, len(train_loader))\n",
    "        # Track best eval reconstruction\n",
    "        best_eval = 1e6\n",
    "        self.log_reconstructions(0)\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(epoch=epoch_idx)\n",
    "            if epoch_idx % 1 == 0:\n",
    "                eval_metrics = self.eval_model(val_loader)\n",
    "                for key in eval_metrics:\n",
    "                    self.logger.add_scalar(f'val/{key}', eval_metrics[key], global_step=epoch_idx)\n",
    "                if eval_metrics['rec_loss'] <= best_eval:\n",
    "                    best_eval = eval_metrics['rec_loss']\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                    self.log_reconstructions(epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        metrics = defaultdict(float)\n",
    "        for imgs, _ in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, batch_metrics = self.train_step(self.state, imgs)\n",
    "            for key in batch_metrics:\n",
    "                metrics[key] += batch_metrics[key]\n",
    "        num_train_steps = len(train_loader)\n",
    "        for key in metrics:\n",
    "            avg_val = metrics[key].item() / num_train_steps\n",
    "            self.logger.add_scalar(f'train/{key}', avg_val, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all images of a data loader and return avg loss\n",
    "        count = 0\n",
    "        metrics = defaultdict(float)\n",
    "        for imgs, _ in data_loader:\n",
    "            batch_metrics = self.eval_step(self.state, imgs)\n",
    "            count += imgs.shape[0]\n",
    "            for key in batch_metrics:\n",
    "                metrics[key] += batch_metrics[key] * imgs.shape[0]\n",
    "        metrics = {key: metrics[key].item() / count for key in metrics}\n",
    "        return metrics\n",
    "    \n",
    "    def log_reconstructions(self, epoch_idx):\n",
    "        x_rec = self.reconst_func(self.state)\n",
    "        self.logger.add_image('reconstructions', imgs_to_grid(x_rec), global_step=epoch_idx, dataformats='HWC')\n",
    "        \n",
    "        if not self.logged_orig_imgs:\n",
    "            x_ori = imgs_to_grid(self.exmp_imgs[:x_rec.shape[0]])\n",
    "            self.logger.add_image('originals', x_ori, global_step=epoch_idx, dataformats='HWC')\n",
    "            self.logged_orig_imgs = True\n",
    "        \n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target={'params': self.state.params,\n",
    "                                            'batch_stats': self.state.batch_stats},\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for pretrained models\n",
    "        if not pretrained:\n",
    "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        else:\n",
    "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=None)\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=state_dict['params'],\n",
    "                                       batch_stats=state_dict['batch_stats'],\n",
    "                                       tx=self.state.tx if self.state.tx else optax.sgd(0.1)   # Default optimizer\n",
    "                                      )\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this autoencoder\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(*args, num_epochs=200, **kwargs):\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = TrainerModule(*args, **kwargs)\n",
    "    if not trainer.checkpoint_exists():  # Skip training if pretrained model exists\n",
    "        trainer.train_model(train_loader, val_loader, num_epochs=num_epochs)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        trainer.load_model(pretrained=True)\n",
    "    # Test trained model\n",
    "    val_acc = trainer.eval_model(val_loader)\n",
    "    test_acc = trainer.eval_model(test_loader)\n",
    "    return trainer, {'val': val_acc, 'test': test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e918efaf8c0f4bcdb55d0278ebec40dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42e34795bf49aa86df071f184ae448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-7a0587fcb0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                \u001b[0mvocab_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                exmp_imgs=next(iter(train_loader))[0])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-57affdc49c92>\u001b[0m in \u001b[0;36mtrain_vqvae\u001b[0;34m(num_epochs, *args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip training if pretrained model exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-cdd3ba434dc6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_reconstructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-cdd3ba434dc6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Train model for one epoch, and log avg loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torchvision/datasets/stl10.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0e007527abba>\u001b[0m in \u001b[0;36mimage_to_numpy\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimage_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer, results = train_vqvae(vocab_size=512,\n",
    "                               vocab_dim=128,\n",
    "                               beta=0.25,\n",
    "                               exmp_imgs=next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
