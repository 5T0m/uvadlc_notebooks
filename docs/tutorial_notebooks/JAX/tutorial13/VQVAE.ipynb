{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13 (JAX): Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Sequence, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch Data Loading\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tensorboard\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (i.e. STL10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial13_jax\"\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial13/\"\n",
    "# Files to download\n",
    "pretrained_files = []\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255. - 0.5) * 2.0\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                 transforms.Resize((64, 64)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 image_to_numpy\n",
    "                                ])\n",
    "test_transforms = transforms.Compose([\n",
    "                                 transforms.Resize((64, 64)),\n",
    "                                 image_to_numpy\n",
    "                                ])\n",
    "\n",
    "train_data = STL10(root=DATASET_PATH, split='unlabeled', download=True,\n",
    "                   transform=train_transforms)\n",
    "test_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
    "                  transform=test_transforms)\n",
    "test_data, val_data = data.random_split(test_data, [3000, 2000], \n",
    "                                        generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = data.DataLoader(train_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )\n",
    "val_loader   = data.DataLoader(val_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )\n",
    "test_loader  = data.DataLoader(test_data,\n",
    "                               batch_size=64,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=0,\n",
    "                               # persistent_workers=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # return nn.BatchNorm()(x, use_running_average=not train)\n",
    "        return nn.GroupNorm(num_groups=None, group_size=16, epsilon=1e-6)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        _, H, W, C = x.shape\n",
    "        grid = jnp.stack(jnp.meshgrid(jnp.linspace(-1, 1, H),\n",
    "                                      jnp.linspace(-1, 1, W)), \n",
    "                         axis=-1)[None]\n",
    "        feats = nn.Conv(C, kernel_size=(1, 1))(grid)\n",
    "        x = x + feats\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[[ 0.73624116, -0.33292973, -0.24212673, ...,\n",
       "                -0.32700923,  1.3191122 ,  0.90083843],\n",
       "               [ 0.57841843, -0.1958822 , -0.18887016, ...,\n",
       "                -0.26044402,  1.2928921 ,  0.9437376 ],\n",
       "               [ 0.4205956 , -0.05883467, -0.13561362, ...,\n",
       "                -0.1938788 ,  1.266672  ,  0.9866368 ],\n",
       "               ...,\n",
       "               [-1.3154552 ,  1.4486885 ,  0.45020863, ...,\n",
       "                 0.53833866,  0.97825134,  1.4585282 ],\n",
       "               [-1.473278  ,  1.585736  ,  0.5034652 , ...,\n",
       "                 0.6049038 ,  0.95203125,  1.5014273 ],\n",
       "               [-1.6311007 ,  1.7227834 ,  0.5567217 , ...,\n",
       "                 0.671469  ,  0.9258112 ,  1.5443265 ]],\n",
       "\n",
       "              [[ 0.79589844, -0.42558664, -0.26309973, ...,\n",
       "                -0.3499732 ,  1.1694506 ,  0.7378274 ],\n",
       "               [ 0.6380757 , -0.2885391 , -0.20984316, ...,\n",
       "                -0.283408  ,  1.1432306 ,  0.7807266 ],\n",
       "               [ 0.4802529 , -0.15149158, -0.15658662, ...,\n",
       "                -0.21684279,  1.1170105 ,  0.82362586],\n",
       "               ...,\n",
       "               [-1.2557979 ,  1.3560317 ,  0.42923564, ...,\n",
       "                 0.51537466,  0.82858974,  1.2955171 ],\n",
       "               [-1.4136207 ,  1.4930792 ,  0.4824922 , ...,\n",
       "                 0.5819399 ,  0.8023697 ,  1.3384163 ],\n",
       "               [-1.5714433 ,  1.6301266 ,  0.5357487 , ...,\n",
       "                 0.64850503,  0.77614963,  1.3813155 ]],\n",
       "\n",
       "              [[ 0.8555558 , -0.51824355, -0.28407273, ...,\n",
       "                -0.3729372 ,  1.0197891 ,  0.57481647],\n",
       "               [ 0.69773304, -0.38119602, -0.23081616, ...,\n",
       "                -0.306372  ,  0.993569  ,  0.61771566],\n",
       "               [ 0.5399102 , -0.2441485 , -0.17755961, ...,\n",
       "                -0.23980677,  0.96734893,  0.66061485],\n",
       "               ...,\n",
       "               [-1.1961406 ,  1.2633747 ,  0.40826264, ...,\n",
       "                 0.49241066,  0.6789282 ,  1.1325061 ],\n",
       "               [-1.3539634 ,  1.4004222 ,  0.4615192 , ...,\n",
       "                 0.5589759 ,  0.6527082 ,  1.1754054 ],\n",
       "               [-1.511786  ,  1.5374696 ,  0.5147757 , ...,\n",
       "                 0.62554103,  0.6264881 ,  1.2183045 ]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[ 1.5117861 , -1.5374697 , -0.51477575, ...,\n",
       "                -0.62554103, -0.62648827, -1.2183046 ],\n",
       "               [ 1.3539634 , -1.4004222 , -0.46151918, ...,\n",
       "                -0.5589758 , -0.6527083 , -1.1754055 ],\n",
       "               [ 1.1961405 , -1.2633747 , -0.40826264, ...,\n",
       "                -0.49241063, -0.6789284 , -1.1325063 ],\n",
       "               ...,\n",
       "               [-0.5399103 ,  0.24414852,  0.17755963, ...,\n",
       "                 0.2398068 , -0.9673491 , -0.66061497],\n",
       "               [-0.6977331 ,  0.38119605,  0.2308162 , ...,\n",
       "                 0.30637202, -0.99356914, -0.6177158 ],\n",
       "               [-0.8555557 ,  0.5182435 ,  0.2840727 , ...,\n",
       "                 0.37293717, -1.0197892 , -0.5748166 ]],\n",
       "\n",
       "              [[ 1.5714433 , -1.6301266 , -0.5357487 , ...,\n",
       "                -0.64850503, -0.7761498 , -1.3813156 ],\n",
       "               [ 1.4136206 , -1.4930791 , -0.48249218, ...,\n",
       "                -0.5819398 , -0.80236983, -1.3384165 ],\n",
       "               [ 1.2557977 , -1.3560315 , -0.42923564, ...,\n",
       "                -0.5153746 , -0.8285899 , -1.2955172 ],\n",
       "               ...,\n",
       "               [-0.480253  ,  0.15149161,  0.15658663, ...,\n",
       "                 0.21684282, -1.1170106 , -0.823626  ],\n",
       "               [-0.63807577,  0.28853914,  0.2098432 , ...,\n",
       "                 0.28340805, -1.1432307 , -0.78072673],\n",
       "               [-0.7958984 ,  0.42558655,  0.2630997 , ...,\n",
       "                 0.3499732 , -1.1694508 , -0.7378276 ]],\n",
       "\n",
       "              [[ 1.6311007 , -1.7227834 , -0.5567217 , ...,\n",
       "                -0.671469  , -0.9258112 , -1.5443265 ],\n",
       "               [ 1.4732778 , -1.5857359 , -0.5034652 , ...,\n",
       "                -0.6049038 , -0.95203125, -1.5014273 ],\n",
       "               [ 1.315455  , -1.4486884 , -0.4502086 , ...,\n",
       "                -0.53833854, -0.97825134, -1.458528  ],\n",
       "               ...,\n",
       "               [-0.42059577,  0.05883479,  0.13561365, ...,\n",
       "                 0.19387886, -1.266672  , -0.9866368 ],\n",
       "               [-0.57841855,  0.19588232,  0.18887022, ...,\n",
       "                 0.26044407, -1.2928921 , -0.9437376 ],\n",
       "               [-0.73624116,  0.33292973,  0.24212673, ...,\n",
       "                 0.32700923, -1.3191122 , -0.90083843]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = PositionEmbedding()\n",
    "inp = jnp.zeros((1, 16, 16, 8))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "module.apply(params, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv initialized with kaiming int, but uses fan-out instead of fan-in mode\n",
    "# Fan-out focuses on the gradient distribution, and is commonly used in ResNets\n",
    "resnet_kernel_init = nn.initializers.variance_scaling(2.0, mode='fan_out', distribution='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "    subsample : bool = False  # If True, we apply a stride inside F\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = Norm()(x, train=train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    strides=(1, 1) if not self.subsample else (2, 2),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = PositionEmbedding()(z)\n",
    "        z = Norm()(z, train=train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "\n",
    "        if self.subsample:\n",
    "            x = Norm()(x, train=train)\n",
    "            x = self.act_fn(x)\n",
    "            x = nn.Conv(self.c_out,\n",
    "                        kernel_size=(1, 1),\n",
    "                        strides=(2, 2),\n",
    "                        kernel_init=resnet_kernel_init,\n",
    "                        use_bias=False)(x)\n",
    "\n",
    "        x_out = z + x\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderResNetBlock(nn.Module):\n",
    "    act_fn : callable  # Activation function\n",
    "    c_out : int   # Output feature size\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        # Network representing F\n",
    "        z = Norm()(x, train=train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        z = PositionEmbedding()(z)\n",
    "        z = Norm()(z, train=train)\n",
    "        z = self.act_fn(z)\n",
    "        z = nn.Conv(self.c_out, kernel_size=(3, 3),\n",
    "                    kernel_init=resnet_kernel_init,\n",
    "                    use_bias=False)(z)\n",
    "        \n",
    "        if x.shape[-1] != z.shape[-1]:\n",
    "            x = Norm()(x, train=train)\n",
    "            x = self.act_fn(x)\n",
    "            x = nn.Conv(self.c_out, kernel_size=(1, 1),\n",
    "                        kernel_init=resnet_kernel_init,\n",
    "                        use_bias=False)(z)\n",
    "        \n",
    "        return x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        residual = x\n",
    "        x = Norm()(x, train=train)\n",
    "        \n",
    "        qkv = nn.Conv(3*x.shape[-1], kernel_size=(1, 1))(x)\n",
    "        query, key, value = qkv.split(3, axis=-1)\n",
    "\n",
    "        # compute attentions\n",
    "        B, H, W, C = query.shape\n",
    "        query = query.reshape((B, H * W, C))\n",
    "        key = key.reshape((B, H * W, C))\n",
    "        attn_weights = jnp.einsum('...qc,...kc->...qk', query, key)\n",
    "        attn_weights = attn_weights * (int(C) ** -0.5)\n",
    "        attn_weights = nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        ## attend to values\n",
    "        value = value.reshape((B, H * W, C))\n",
    "        x = jnp.einsum('...kc,...qk->...qc', value, attn_weights)\n",
    "        x = x.reshape((B, H, W, C))\n",
    "\n",
    "        x = nn.Conv(x.shape[-1], kernel_size=(1, 1))(x)\n",
    "        x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 12, 12, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = AttnBlock()\n",
    "inp = random.normal(random.PRNGKey(0), (64, 12, 12, 256))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "out = module.apply(params, inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    num_blocks : Sequence[int]\n",
    "    c_hidden : Sequence[int]\n",
    "    vocab_dim : int\n",
    "    act_fn : callable = nn.swish\n",
    "        \n",
    "    def setup(self):\n",
    "        self.in_conv = nn.Conv(self.c_hidden[0], kernel_size=(3, 3),\n",
    "                               kernel_init=resnet_kernel_init,\n",
    "                               use_bias=False)\n",
    "        block_modules = []\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                subsample = (bc == 0 and block_idx > 0)  # Subsample the first block of each group, except the very first one.\n",
    "                block_modules.append(\n",
    "                    EncoderResNetBlock(act_fn=self.act_fn,\n",
    "                                       subsample=subsample,\n",
    "                                       c_out=self.c_hidden[block_idx])\n",
    "                )\n",
    "                if block_idx > 1 and bc < block_count - 1:\n",
    "                    block_modules.append(AttnBlock())\n",
    "        self.block_modules = block_modules\n",
    "                \n",
    "        self.out_norm = Norm()\n",
    "        self.out_conv = nn.Conv(self.vocab_dim, kernel_size=(3, 3),\n",
    "                                kernel_init=resnet_kernel_init,\n",
    "                                use_bias=True)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.block_modules:\n",
    "            x = layer(x, train=train)\n",
    "        x = self.out_norm(x, train=train)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.out_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16, 16, 256)\n"
     ]
    }
   ],
   "source": [
    "module = Encoder(num_blocks=[2, 2, 3],\n",
    "                 c_hidden=[64, 128, 256],\n",
    "                 vocab_dim=256)\n",
    "inp, _ = next(iter(train_loader))\n",
    "variables = module.init(random.PRNGKey(0), inp)\n",
    "# params, batch_stats = variables['params'], variables['batch_stats']\n",
    "out = module.apply(variables, # {'params': params, 'batch_stats': batch_stats},\n",
    "                   inp,\n",
    "                   train=True)\n",
    "#                    mutable=['batch_stats'])\n",
    "vecs = out\n",
    "print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    scale_factor : float = 2.0\n",
    "    method : str = 'nearest'\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        B, H, W, C = x.shape\n",
    "        x = jax.image.resize(x, \n",
    "                             shape=(B, int(H * self.scale_factor), int(W * self.scale_factor), C),\n",
    "                             method=self.method)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    num_blocks : Sequence[int]\n",
    "    c_hidden : Sequence[int]\n",
    "    c_out : int\n",
    "    act_fn : callable = nn.swish\n",
    "        \n",
    "    def setup(self):\n",
    "        self.in_conv = nn.Conv(self.c_hidden[0], kernel_size=(1, 1),\n",
    "                               kernel_init=resnet_kernel_init,\n",
    "                               use_bias=False)\n",
    "        \n",
    "        block_modules = []\n",
    "        for block_idx, block_count in enumerate(self.num_blocks):\n",
    "            if block_idx > 0:\n",
    "                block_modules.append(Upsample())\n",
    "            for bc in range(block_count):\n",
    "                channel_change = (bc == 0 and block_idx > 0)\n",
    "                block_modules.append(\n",
    "                    DecoderResNetBlock(act_fn=self.act_fn,\n",
    "                                       c_out=self.c_hidden[block_idx])\n",
    "                )\n",
    "                if block_idx == 0 and bc < block_count - 1:\n",
    "                    block_modules.append(AttnBlock())\n",
    "                \n",
    "        self.block_modules = block_modules\n",
    "        self.out_norm = Norm()\n",
    "#         self.out_conv = nn.ConvTranspose(self.c_out, kernel_size=(3, 3),\n",
    "#                                          strides=(2, 2),\n",
    "#                                          kernel_init=resnet_kernel_init)\n",
    "        self.out_conv = nn.Conv(self.c_out, kernel_size=(3, 3))\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.block_modules:\n",
    "            x = layer(x, train=train)\n",
    "        x = self.out_norm(x, train=train)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "module = Decoder(num_blocks=[3, 2, 2],\n",
    "                 c_hidden=[256, 128, 64],\n",
    "                 c_out=3)\n",
    "inp = random.normal(random.PRNGKey(0), (64, 16, 16, 256))\n",
    "variables = module.init(random.PRNGKey(0), inp)\n",
    "# params, batch_stats = variables['params'], variables['batch_stats']\n",
    "out = module.apply(variables,\n",
    "                   inp,\n",
    "                   train=True)\n",
    "#                    mutable=['batch_stats'])\n",
    "# vecs, _ = out\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbedding(nn.Module):\n",
    "    vocab_size : int\n",
    "    vocab_dim : int\n",
    "    mask_prob : float = 0.0\n",
    "    \n",
    "    def setup(self):\n",
    "        self.codebook = nn.Embed(self.vocab_size, self.vocab_dim)\n",
    "        \n",
    "    def __call__(self, z, train=True):\n",
    "        z_flatten = z.reshape(-1, z.shape[-1])\n",
    "        \n",
    "        # dummy op to init the weights, so we can access them below\n",
    "        self.codebook(jnp.ones((1, 1), dtype=\"i4\"))\n",
    "\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "        emb_weights = self.variables[\"params\"][\"codebook\"][\"embedding\"]\n",
    "        distance = (\n",
    "            jnp.sum(z_flatten ** 2, axis=1, keepdims=True)\n",
    "            + jnp.sum(emb_weights ** 2, axis=1)\n",
    "            - 2 * jnp.dot(z_flatten, emb_weights.T)\n",
    "        )\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        min_encoding_indices = jnp.argmin(distance, axis=1)\n",
    "        z_embeds = self.codebook(min_encoding_indices).reshape(z.shape)\n",
    "        z_q = z_embeds\n",
    "        if train:\n",
    "            z_q = z + jax.lax.stop_gradient(z_q - z)\n",
    "            if self.mask_prob > 0.0:\n",
    "                num_masked = int(z_q.shape[0] * self.mask_prob)\n",
    "                z_q = jnp.concatenate([z[:num_masked], z_q[num_masked:]], axis=0)\n",
    "        \n",
    "        return z_q, z_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 16, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = VQEmbedding(vocab_size=512, vocab_dim=256)\n",
    "inp = random.normal(random.PRNGKey(0), (64, 16, 16, 256))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "z_q, z_embeds = module.apply(params, inp, train=False)\n",
    "z_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(pred, target):\n",
    "    return ((pred - jax.lax.stop_gradient(target))**2).mean()\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    vocab_size : int\n",
    "    vocab_dim : int\n",
    "    beta : float\n",
    "    resnet_blocks : Sequence[int] = (2, 2, 3)\n",
    "    resnet_hidden : Sequence[int] = (64, 128, 128)\n",
    "    \n",
    "    def setup(self):\n",
    "        # Encoder network (ResNet)\n",
    "        self.encoder = Encoder(num_blocks=self.resnet_blocks,\n",
    "                               c_hidden=self.resnet_hidden,\n",
    "                               vocab_dim=self.vocab_dim,\n",
    "                               act_fn=nn.swish)\n",
    "        # Vector quantized bottleneck layer\n",
    "        self.vector_quantization = VQEmbedding(vocab_size=self.vocab_size,\n",
    "                                               vocab_dim=self.vocab_dim)\n",
    "        # Decoder network (ResNet, mirrored version of Encoder)\n",
    "        self.decoder = Decoder(num_blocks=self.resnet_blocks[::-1],\n",
    "                               c_hidden=self.resnet_hidden[::-1],\n",
    "                               c_out=3,\n",
    "                               act_fn=nn.swish)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        z = self.encoder(x, train=train)\n",
    "        z_quantized, z_embeddings = self.vector_quantization(z, train=train)\n",
    "        x_rec = self.decoder(z_quantized, train=train)\n",
    "        \n",
    "        rec_loss = mse_loss(x_rec, x)\n",
    "        embed_loss = mse_loss(z_embeddings, z)\n",
    "        quant_loss = mse_loss(z, z_embeddings)\n",
    "        \n",
    "        loss = rec_loss + embed_loss + self.beta * quant_loss\n",
    "        \n",
    "        metrics = {'loss': loss,\n",
    "                   'rec_loss': rec_loss,\n",
    "                   'l1_dist': jnp.abs(x_rec - x).mean(),\n",
    "                   'embed_loss': embed_loss,\n",
    "                   'quant_loss': self.beta * quant_loss}\n",
    "        \n",
    "        return loss, metrics\n",
    "        \n",
    "    def reconstruct(self, x, train=False):\n",
    "        z = self.encoder(x, train=train)\n",
    "        z, _ = self.vector_quantization(z, train=train)\n",
    "        x_rec = self.decoder(z, train=train)\n",
    "        return x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': DeviceArray(0.95695674, dtype=float32),\n",
       " 'rec_loss': DeviceArray(0.5601914, dtype=float32),\n",
       " 'l1_dist': DeviceArray(0.6247081, dtype=float32),\n",
       " 'embed_loss': DeviceArray(0.31741226, dtype=float32),\n",
       " 'quant_loss': DeviceArray(0.07935306, dtype=float32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = VQVAE(vocab_size=512, vocab_dim=256, beta=0.25)\n",
    "inp = nn.tanh(random.normal(random.PRNGKey(0), (64, 96, 96, 3)))\n",
    "params = module.init(random.PRNGKey(0), inp)\n",
    "_, metrics = module.apply(params, inp, train=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del module, metrics, params, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    batch_stats: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_grid(imgs, pads=4, num_rows=4):\n",
    "    imgs = np.pad(imgs, ((0,0), (pads,pads), (pads,pads), (0,0)))\n",
    "    imgs = np.reshape(imgs, (num_rows, -1, *imgs.shape[1:]))\n",
    "    imgs = imgs.swapaxes(1, 2)\n",
    "    imgs = imgs.reshape(num_rows*imgs.shape[1], -1, imgs.shape[-1])\n",
    "    imgs = np.pad(imgs, ((pads,pads), (pads,pads), (0,0)))\n",
    "    imgs = (imgs + 1.) / 2.\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, exmp_imgs, lr=5e-4, seed=42, **model_hparams):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            lr - Learning rate of the optimizer to use\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.exmp_imgs = exmp_imgs\n",
    "        self.model_name = 'VQVAE_masked04'\n",
    "        print('Model', self.model_name)\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = VQVAE(**model_hparams)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, f'{self.model_name}/')\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.logged_orig_imgs = False\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_imgs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def calculate_loss(params, batch_stats, batch, train):\n",
    "#             outs = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "#                                     batch,\n",
    "#                                     train=train,\n",
    "#                                     mutable=['batch_stats'] if train else False)\n",
    "#             (loss, metrics), new_model_state = outs if train else (outs, None)\n",
    "            loss, metrics = self.model.apply({'params': params}, batch, train=train)\n",
    "            new_model_state = {'batch_stats': None}\n",
    "            return loss, (metrics, new_model_state)\n",
    "        # Training function\n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, state.batch_stats, batch, train=True)\n",
    "            # Get loss, gradients for loss, and other outputs of loss function\n",
    "            (_, (metrics, new_model_state)), grads = jax.value_and_grad(loss_fn, \n",
    "                                                                        has_aux=True)(state.params)\n",
    "            # Update parameters and batch statistics\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])\n",
    "            return state, metrics\n",
    "        # Eval function\n",
    "        def eval_step(state, batch):\n",
    "            # Return the accuracy for a single batch\n",
    "            _, (metrics, _) = calculate_loss(state.params, state.batch_stats, batch, train=False)\n",
    "            return metrics\n",
    "        # Reconstruction function\n",
    "        def reconst_func(state):\n",
    "            x_rec = self.model.bind({'params': state.params, \n",
    "                                     # 'batch_stats': state.batch_stats\n",
    "                                    }).reconstruct(self.exmp_imgs[:16])\n",
    "            return jax.device_get(x_rec)\n",
    "        # jit for efficiency\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "        self.reconst_func = jax.jit(reconst_func)\n",
    "\n",
    "    def init_model(self, exmp_imgs):\n",
    "        # Initialize model\n",
    "        init_rng = jax.random.PRNGKey(self.seed)\n",
    "        variables = self.model.init(init_rng, exmp_imgs, train=True)\n",
    "        self.state = TrainState(step=0,\n",
    "                                apply_fn=self.model.apply,\n",
    "                                params=variables['params'],\n",
    "                                batch_stats=variables.get('batch_stats', None),\n",
    "                                tx=None, opt_state=None)\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "        # We decrease the learning rate by a factor of 0.1 after 60% and 85% of the training\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=50,\n",
    "            decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "            end_value=1e-2*self.lr\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = TrainState.create(apply_fn=self.state.apply_fn,\n",
    "                                       params=self.state.params,\n",
    "                                       batch_stats=self.state.batch_stats,\n",
    "                                       tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200):\n",
    "        # Train model for defined number of epochs\n",
    "        # We first need to create optimizer and the scheduler for the given number of epochs\n",
    "        self.init_optimizer(num_epochs, len(train_loader))\n",
    "        # Track best eval reconstruction\n",
    "        best_eval = 1e6\n",
    "        self.log_reconstructions(0)\n",
    "        print('Training starts')\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(epoch=epoch_idx)\n",
    "            if epoch_idx % 1 == 0:\n",
    "                eval_metrics = self.eval_model(val_loader)\n",
    "                print(f'[Epoch {epoch_idx}]', ', '.join([f'{key}: {eval_metrics[key]:5.4f}' for key in eval_metrics]))\n",
    "                for key in eval_metrics:\n",
    "                    self.logger.add_scalar(f'val/{key}', eval_metrics[key], global_step=epoch_idx)\n",
    "                if eval_metrics['rec_loss'] <= best_eval:\n",
    "                    best_eval = eval_metrics['rec_loss']\n",
    "                    self.save_model(step=epoch_idx)\n",
    "            if epoch_idx % 10 == 0:\n",
    "                self.log_reconstructions(epoch_idx)\n",
    "            self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        metrics = defaultdict(float)\n",
    "        for imgs, _ in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, batch_metrics = self.train_step(self.state, imgs)\n",
    "            for key in batch_metrics:\n",
    "                metrics[key] += batch_metrics[key]\n",
    "        num_train_steps = len(train_loader)\n",
    "        for key in metrics:\n",
    "            avg_val = metrics[key].item() / num_train_steps\n",
    "            self.logger.add_scalar(f'train/{key}', avg_val, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all images of a data loader and return avg loss\n",
    "        count = 0\n",
    "        metrics = defaultdict(float)\n",
    "        for imgs, _ in data_loader:\n",
    "            batch_metrics = self.eval_step(self.state, imgs)\n",
    "            count += imgs.shape[0]\n",
    "            for key in batch_metrics:\n",
    "                metrics[key] += batch_metrics[key] * imgs.shape[0]\n",
    "        metrics = {key: metrics[key].item() / count for key in metrics}\n",
    "        return metrics\n",
    "    \n",
    "    def log_reconstructions(self, epoch_idx):\n",
    "        x_rec = self.reconst_func(self.state)\n",
    "        self.logger.add_image('reconstructions', imgs_to_grid(x_rec), global_step=epoch_idx, dataformats='HWC')\n",
    "        \n",
    "        if not self.logged_orig_imgs:\n",
    "            x_ori = imgs_to_grid(self.exmp_imgs[:x_rec.shape[0]])\n",
    "            self.logger.add_image('originals', x_ori, global_step=epoch_idx, dataformats='HWC')\n",
    "            self.logged_orig_imgs = True\n",
    "        \n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target={'params': self.state.params,\n",
    "                                            'batch_stats': self.state.batch_stats},\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for pretrained models\n",
    "        if not pretrained:\n",
    "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        else:\n",
    "            state_dict = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=None)\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=state_dict['params'],\n",
    "                                       batch_stats=state_dict['batch_stats'],\n",
    "                                       tx=self.state.tx if self.state.tx else optax.sgd(0.1)   # Default optimizer\n",
    "                                      )\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this autoencoder\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(*args, num_epochs=200, **kwargs):\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = TrainerModule(*args, **kwargs)\n",
    "    if not trainer.checkpoint_exists():  # Skip training if pretrained model exists\n",
    "        trainer.train_model(train_loader, val_loader, num_epochs=num_epochs)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        trainer.load_model(pretrained=True)\n",
    "    # Test trained model\n",
    "    val_acc = trainer.eval_model(val_loader)\n",
    "    test_acc = trainer.eval_model(test_loader)\n",
    "    return trainer, {'val': val_acc, 'test': test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model VQVAE_masked04\n",
      "Training starts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45814a968d76466f821ccb6cdb2ff70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbba01f314a54d28ad1bfd17539abab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-217cb189a8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                \u001b[0mvocab_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                exmp_imgs=next(iter(train_loader))[0])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-57affdc49c92>\u001b[0m in \u001b[0;36mtrain_vqvae\u001b[0;34m(num_epochs, *args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip training if pretrained model exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1b159bec9a19>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training starts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1b159bec9a19>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl2020/lib/python3.7/site-packages/flax/core/frozen_dict.py\u001b[0m in \u001b[0;36mtree_unflatten\u001b[0;34m(cls, _, data)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# data is already deep copied due to tree map mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer, results = train_vqvae(vocab_size=512,\n",
    "                               vocab_dim=256,\n",
    "                               beta=0.25,\n",
    "                               exmp_imgs=next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
