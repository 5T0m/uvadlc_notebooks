{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8.1: Deep Autoencoders \n",
    "\n",
    "In this tutorial, we will take a closer look at autoencoders. In contrast to variational autoencoders (VAE), autoencoders are not considered as a generative model because they do not model a distribution from which we can easily sample. The latent space does not have any constraint/incentive to follow a specific distribution. However, autoencoders are still useful, in particular to represent data in lower dimensional space and compressing data. \n",
    "\n",
    "First of all, we again import most of our standard libraries. We will use [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) to reduce the training code overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_NOTEBOOK = False\n",
    "TRAIN_CIFAR = True\n",
    "TRAIN_STL = False\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "if USE_NOTEBOOK:\n",
    "    %matplotlib inline \n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('svg', 'pdf') # For export\n",
    "    from matplotlib.colors import to_rgb\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "if USE_NOTEBOOK:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial8\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we work with the CIFAR10 dataset. In CIFAR10, each image has 3 color channels and is 32x32 pixels large. As autoencoders do not have the constrain of modeling images probabilistic, we can work on more complex image data (i.e. 3 color channels instead of black-and-white) much easier than for VAEs. \n",
    "\n",
    "In case you have downloaded CIFAR10 already in a different directory, make sure to set DATASET_PATH accordingly to prevent another download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the autoencoder\n",
    "\n",
    "In general, an autoencoder consists of an **encoder** that maps the input $x$ to a lower dimensional feature vector $z$, and a **decoder** that reconstructs the input $\\hat{x}$ from $z$. We train the model by comparing $x$ to $\\hat{x}$, and optimizing the parameters to increase similarity between $x$ and $\\hat{x}$. See below for a small illustration of the autoencoder framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\"> **TODO**: Add illustration of autoencoder </span>\n",
    "\n",
    "<span style=\"color:red;\"> **TODO**: Check whether weight norm is needed at all, and if not, remove it (less to explain) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by implementing the encoder. The encoder effectively consists of a deep convolutional network, where we scale down the image layer-by-layer using strided convolutions. After downscaling the image three times, we flatten the features and apply linear layers. The latent representation $z$ is therefore a vector of size *d* which can be flexible selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_conv(*args, **kwargs):\n",
    "    # Convolution with weight norm applied\n",
    "    return nn.utils.weight_norm(nn.Conv2d(*args, **kwargs))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_input_channels : int, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is a mirrored, flipped version of the encoder. The only difference is that we replace strided convolutions by transposed convolutions (i.e. deconvolutions) to upscale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_conv_trans(*args, **kwargs):\n",
    "    return nn.utils.weight_norm(nn.ConvTranspose2d(*args, **kwargs))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_input_channels : int, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs: \n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv_trans(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv_trans(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv_trans(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size: int, \n",
    "                 latent_dim: int, \n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3, \n",
    "                 width: int = 32, \n",
    "                 height: int = 32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        x, _ = batch # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         factor=0.2, \n",
    "                                                         patience=50, \n",
    "                                                         min_lr=5e-5)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)                             \n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        return {'val_loss': loss, 'checkpoint_on': loss, 'log': {'val_loss': loss}}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        result = pl.EvalResult()\n",
    "        result.log(\"test_loss\", loss)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0904 15:35:31.035763 139824593631040 distributed.py:41] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0904 15:35:31.038624 139824593631040 distributed.py:41] TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "I0904 15:35:31.040117 139824593631040 distributed.py:41] CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 496 K  | [2, 3, 32, 32] | [2, 384]      \n",
      "1 | decoder | Decoder | 496 K  | [2, 384]       | [2, 3, 32, 32]\n",
      "I0904 15:35:31.214221 139824593631040 lightning.py:1449] \n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 496 K  | [2, 3, 32, 32] | [2, 384]      \n",
      "1 | decoder | Decoder | 496 K  | [2, 384]       | [2, 3, 32, 32]\n",
      "Saving latest checkpoint..\n",
      "I0904 15:35:41.368177 139824593631040 training_loop.py:1136] Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(240.6534, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I0904 15:35:42.489176 139824593631040 distributed.py:41] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0904 15:35:42.491806 139824593631040 distributed.py:41] TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "I0904 15:35:42.493494 139824593631040 distributed.py:41] CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 365 K  | [2, 3, 32, 32] | [2, 256]      \n",
      "1 | decoder | Decoder | 365 K  | [2, 256]       | [2, 3, 32, 32]\n",
      "I0904 15:35:42.643676 139824593631040 lightning.py:1449] \n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 365 K  | [2, 3, 32, 32] | [2, 256]      \n",
      "1 | decoder | Decoder | 365 K  | [2, 256]       | [2, 3, 32, 32]\n",
      "Saving latest checkpoint..\n",
      "I0904 15:35:52.111944 139824593631040 training_loop.py:1136] Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(256.4281, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I0904 15:35:53.252152 139824593631040 distributed.py:41] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0904 15:35:53.254111 139824593631040 distributed.py:41] TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "I0904 15:35:53.255248 139824593631040 distributed.py:41] CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 233 K  | [2, 3, 32, 32] | [2, 128]      \n",
      "1 | decoder | Decoder | 234 K  | [2, 128]       | [2, 3, 32, 32]\n",
      "I0904 15:35:53.401491 139824593631040 lightning.py:1449] \n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 233 K  | [2, 3, 32, 32] | [2, 128]      \n",
      "1 | decoder | Decoder | 234 K  | [2, 128]       | [2, 3, 32, 32]\n",
      "Saving latest checkpoint..\n",
      "I0904 15:36:03.694238 139824593631040 training_loop.py:1136] Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(240.8853, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "I0904 15:36:04.797479 139824593631040 distributed.py:41] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0904 15:36:04.798998 139824593631040 distributed.py:41] TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "I0904 15:36:04.799760 139824593631040 distributed.py:41] CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 168 K  | [2, 3, 32, 32] | [2, 64]       \n",
      "1 | decoder | Decoder | 169 K  | [2, 64]        | [2, 3, 32, 32]\n",
      "I0904 15:36:04.952379 139824593631040 lightning.py:1449] \n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 168 K  | [2, 3, 32, 32] | [2, 64]       \n",
      "1 | decoder | Decoder | 169 K  | [2, 64]        | [2, 3, 32, 32]\n",
      "Saving latest checkpoint..\n",
      "I0904 15:36:14.807443 139824593631040 training_loop.py:1136] Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(246.1945, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phillip/anaconda3/envs/nlp1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def train_cifar(latent_dim):\n",
    "    exmp_imgs, _ = next(iter(train_loader))\n",
    "    exmp_imgs = exmp_imgs[:8]\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"cifar10_%i\" % latent_dim), \n",
    "                         gpus=1, \n",
    "                         max_epochs=1, \n",
    "                         callbacks=[GenerateCallback(exmp_imgs, every_n_epochs=10),\n",
    "                                    LearningRateLogger(\"epoch\")],\n",
    "                         benchmark=True,\n",
    "                         progress_bar_refresh_rate=1 if USE_NOTEBOOK else 0)\n",
    "    \n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"cifar10_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_dataloaders=test_loader)\n",
    "    return model\n",
    "\n",
    "if TRAIN_CIFAR:\n",
    "    model_384 = train_cifar(384)\n",
    "    model_256 = train_cifar(256)\n",
    "    model_128 = train_cifar(128)\n",
    "    model_64 = train_cifar(64)\n",
    "    \n",
    "if not USE_NOTEBOOK:\n",
    "    import sys\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_imgs(data_loader):\n",
    "    img_embeddings = ([],[])\n",
    "    model.eval()\n",
    "    for imgs, _ in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            z = model.encoder(imgs.to(model.device))\n",
    "        img_embeddings[0].append(imgs)\n",
    "        img_embeddings[1].append(z)\n",
    "    return (torch.cat(img_embeddings[0], dim=0), torch.cat(img_embeddings[1], dim=0))\n",
    "\n",
    "train_img_embeds = embed_imgs(train_loader)\n",
    "test_img_embeds = embed_imgs(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(query_img, query_z, key_embeds, num_imgs=8):\n",
    "    dist = torch.cdist(query_z[None,:], key_embeds[1], p=2)\n",
    "    dist = dist.squeeze(dim=0)\n",
    "    dist, indices = torch.sort(dist)\n",
    "    \n",
    "    imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:num_imgs]]], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_display, nrow=num_imgs+1, normalize=True, range=(-1,1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
