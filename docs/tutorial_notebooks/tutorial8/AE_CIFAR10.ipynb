{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8.1: Deep Autoencoders \n",
    "\n",
    "**Goal**: Show the working of a deep convolutional autoencoder, and visualize embeddings in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_NOTEBOOK = True\n",
    "TRAIN_CIFAR = False\n",
    "TRAIN_STL = False\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "if USE_NOTEBOOK:\n",
    "    %matplotlib inline \n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('svg', 'pdf') # For export\n",
    "    from matplotlib.colors import to_rgb\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "if USE_NOTEBOOK:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial8\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "set_seed(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass CIFAR10DataModule(pl.LightningDataModule):\\n    \\n    def __init__(self, batch_size=256):\\n        super().__init__()\\n        self.batch_size = batch_size\\n    \\n    def setup(self, stage):\\n        transform = transforms.Compose([transforms.ToTensor()])\\n        if stage == 'fit':\\n            # Loading the training set\\n            train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\\n            set_seed(42)\\n            self.train_set, self.val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\\n        if stage == 'test':\\n            # Loading the test set\\n            self.test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\\n    \\n    def train_dataloader(self):\\n        train_loader = data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, drop_last=True, pin_memory=True)\\n        return train_loader\\n    \\n    def val_dataloader(self):\\n        val_loader = data.DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\\n        return train_loader\\n    \\n    def test_dataloader(self):\\n        test_loader = data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\\n        return test_loader\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size=256):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        if stage == 'fit':\n",
    "            # Loading the training set\n",
    "            train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "            set_seed(42)\n",
    "            self.train_set, self.val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "        if stage == 'test':\n",
    "            # Loading the test set\n",
    "            self.test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = data.DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        return train_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_loader = data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        return test_loader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_conv(*args, **kwargs):\n",
    "    return nn.utils.weight_norm(nn.Conv2d(*args, **kwargs))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_channels, base_channel_size, latent_dim, act_fn=nn.ReLU):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv(num_input_channels, c_hid, kernel_size=5, padding=2, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # This is the CIFAR10 encoder\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_conv_trans(*args, **kwargs):\n",
    "    return nn.utils.weight_norm(nn.ConvTranspose2d(*args, **kwargs))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_channels, base_channel_size, latent_dim, act_fn=nn.ReLU):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv_trans(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv_trans(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv_trans(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_channel_size: int, \n",
    "                 latent_dim: int, \n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3, \n",
    "                 width: int = 32, \n",
    "                 height: int = 32):\n",
    "        super().__init__()\n",
    "        print(\"Encoder class\", encoder_class)\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "    def _get_reconstruction_loss(self, x):\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch # We do not need the labels\n",
    "        loss = self._get_reconstruction_loss(imgs)\n",
    "        \n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, prog_bar=USE_NOTEBOOK)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, _ = batch\n",
    "        loss = self._get_reconstruction_loss(imgs)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, input_imgs):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs\n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        input_imgs = self.input_imgs.to(pl_module.device)\n",
    "        with torch.no_grad():\n",
    "            pl_module.eval()\n",
    "            reconst_imgs = pl_module(input_imgs)\n",
    "            pl_module.train()\n",
    "            \n",
    "        imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
    "        grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))\n",
    "        trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder class <class '__main__.Encoder'>\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(base_channel_size=32, latent_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_CIFAR:\n",
    "    exmp_imgs, _ = next(iter(train_loader))\n",
    "    exmp_imgs = exmp_imgs[:8]\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"autoencoder\"), gpus=1, max_epochs=1000, callbacks=[GenerateCallback(exmp_imgs)])\n",
    "    trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL10 Dataset\n",
    "In contrast to CIFAR10, this dataset has 96x96 images (and a lot of unlabeled ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = torchvision.datasets.STL10(root=DATASET_PATH, split='train+unlabeled', transform=transform, download=True)\n",
    "set_seed(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [100000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = torchvision.datasets.STL10(root=DATASET_PATH, split='test', transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=8)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=8)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_hidden, act_fn):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            act_fn(),\n",
    "            wn_conv(c_hidden, c_hidden, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(c_hidden, 2*c_hidden, kernel_size=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        val, gate = self.net(x).chunk(2, dim=1)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "class ResNetDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_hidden, act_fn):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv(c_hidden, 2*c_hidden, kernel_size=1, padding=0),\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hidden, 2*c_hidden, kernel_size=5, padding=2, groups=2*c_hidden),\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hidden, 2*c_hidden, kernel_size=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        val, gate = self.net(x).chunk(2, dim=1)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "    \n",
    "class DeepEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_channels, base_channel_size, latent_dim, act_fn=nn.GELU):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv(num_input_channels, c_hid, kernel_size=5, padding=2, stride=2), # 96x96 => 48x48\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 48x48 => 24x24\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 24x24 => 12x12\n",
    "            ResNetEncoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetEncoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            wn_conv(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 12x12 => 6x6\n",
    "            ResNetEncoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetEncoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            wn_conv(2*c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(36*c_hid, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "class DeepDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_channels, base_channel_size, latent_dim, act_fn=nn.GELU):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 36*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            wn_conv(c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            wn_conv_trans(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 6x6 => 12x12\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            wn_conv_trans(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 12x12 => 24x24\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=2*c_hid, act_fn=act_fn),\n",
    "            wn_conv_trans(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 24x24 => 48x48\n",
    "            ResNetDecoder(c_hidden=c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=c_hid, act_fn=act_fn),\n",
    "            ResNetDecoder(c_hidden=c_hid, act_fn=act_fn),\n",
    "            wn_conv_trans(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 48x48 => 96x96\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 6, 6)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder class <class '__main__.DeepEncoder'>\n"
     ]
    }
   ],
   "source": [
    "deep_model = Autoencoder(base_channel_size=32, latent_dim=512, encoder_class=DeepEncoder, decoder_class=DeepDecoder, width=96, height=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_STL:\n",
    "    exmp_imgs, _ = next(iter(train_loader))\n",
    "    exmp_imgs = exmp_imgs[:8]\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"deep_autoencoder\"), gpus=1, max_epochs=1000, callbacks=[GenerateCallback(exmp_imgs)])\n",
    "    trainer.fit(deep_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
