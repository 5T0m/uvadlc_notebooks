*************************************************
Tutorial 6: Transformers and Multi-head Attention
*************************************************

**Note**

Currently in development. Will discuss the basic modules of the Transformer architecture, and discusses the wide ranging applications of the architecture beyond the commonly known recurrent tasks like language modeling.

.. toctree::
   :caption: Jupyter notebooks
   :maxdepth: 2

   tutorial_notebooks/tutorial6/Transformers_and_MHAttention